{
  "task": "github_issues",
  "description": "GitHub Issues Retrieval",
  "total": 500,
  "queries": [
    {
      "id": "0",
      "title": "ADD COVID-QA dataset",
      "body": "This PR adds the COVID-QA dataset, a question answering dataset consisting of 2,019 question/answer pairs annotated by volunteer biomedical experts on scientific articles related to COVID-19\r\n\r\nLink to the paper: https://openreview.net/forum?id=JENSKEEzsoU\r\nLink to the dataset/repo: https://github.com/deepset-ai/COVID-QA"
    },
    {
      "id": "1",
      "title": "Loading of FAISS index fails for index_name = 'exact'",
      "body": "Hi,\r\n\r\nIt looks like loading of FAISS index now fails when using index_name = 'exact'.\r\n\r\nFor example, from the RAG [model card](https://huggingface.co/facebook/rag-token-nq?fbclid=IwAR3bTfhls5U_t9DqsX2Vzb7NhtRHxJxfQ-uwFT7VuCPMZUM2AdAlKF_qkI8#usage).\r\n\r\nRunning `transformers==4.3.2` and datasets installed from source on latest `master` branch.\r\n\r\n```bash\r\n(venv) sergey_mkrtchyan datasets (master) $ python\r\nPython 3.8.6 (v3.8.6:db455296be, Sep 23 2020, 13:31:39)\r\n[Clang 6.0 (clang-600.0.57)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration\r\n>>> tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-nq\")\r\n>>> retriever = RagRetriever.from_pretrained(\"facebook/rag-token-nq\", index_name=\"exact\", use_dummy_dataset=True)\r\nUsing custom data configuration dummy.psgs_w100.nq.no_index-dummy=True,with_index=False\r\nReusing dataset wiki_dpr (/Users/sergey_mkrtchyan/.cache/huggingfa"
    },
    {
      "id": "2",
      "title": "1",
      "body": "## Adding a Dataset\n- **Name:** *name of the dataset*\n- **Description:** *short description of the dataset (or link to social media or blog post)*\n- **Paper:** *link to the dataset paper if available*\n- **Data:** *link to the Github repository or current dataset location*\n- **Motivation:** *what are some good reasons to have this dataset*\n\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md)."
    },
    {
      "id": "3",
      "title": "Specify utf-8 encoding for GLUE",
      "body": "#242 \r\nThis makes the GLUE-MNLI dataset readable on my machine, not sure if it's a Windows-only bug."
    },
    {
      "id": "4",
      "title": "add m_lama (multilingual lama) dataset",
      "body": "Add a multilingual (machine translated and automatically generated) version of the LAMA benchmark. For details see the paper https://arxiv.org/pdf/2102.00894.pdf "
    },
    {
      "id": "5",
      "title": "Add OPUS EMEA Dataset",
      "body": ""
    },
    {
      "id": "6",
      "title": "Use a config id in the cache directory names for custom configs",
      "body": "As noticed by @JetRunner there was some issues when trying to generate a dataset using a custom config that is based on an existing config.\r\n\r\nFor example in the following code the `mnli_custom` would reuse the cache used to create `mnli` instead of generating a new dataset with the new label classes:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nmnli = load_dataset(\"glue\", \"mnli\")\r\nmnli_custom = load_dataset(\"glue\", \"mnli\", label_classes=[\"contradiction\", \"entailment\", \"neutral\"])\r\n```\r\n\r\nI fixed that by extending the cache directory definition of a dataset that is being generated.\r\nInstead of using the config name in the cache directory name, I switched to using a `config_id`.\r\n\r\nBy default it is equal to the config name.\r\nHowever the name of a config is not sufficent to have a unique identifier for the dataset being generated since it doesn't take into account:\r\n- the config kwargs that can be used to overwrite attributes\r\n- the custom features used to write the dataset\r\n- the da"
    },
    {
      "id": "7",
      "title": "fix: ðŸ› remove URL's query string only if it's ?dl=1",
      "body": "A lot of URL use the query strings, for example\r\nhttp://opus.nlpl.eu/download.php?f=Bianet/v1/moses/en-ku.txt.zip, we\r\nmust not remove it when trying to detect the protocol. We thus remove it\r\nonly in the case of the query string being ?dl=1 which occurs on dropbox\r\nand dl.orangedox.com. Also: add unit tests.\r\n\r\nSee https://github.com/huggingface/datasets/pull/2843 for the original\r\ndiscussion."
    },
    {
      "id": "8",
      "title": "Add IC, SI, ER tasks to SUPERB",
      "body": "This PR adds 3 additional classification tasks to SUPERB\r\n\r\n#### Intent Classification\r\nDataset URL seems to be down at the moment :( See the note below.\r\nS3PRL source: https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/fluent_commands/dataset.py\r\nInstructions: https://github.com/s3prl/s3prl/tree/master/s3prl/downstream#ic-intent-classification---fluent-speech-commands\r\n\r\n#### Speaker Identification\r\nManual download script:\r\n```\r\nmkdir VoxCeleb1\r\ncd VoxCeleb1\r\n            \r\nwget https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partaa\r\nwget https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partab\r\nwget https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partac\r\nwget https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partad\r\ncat vox1_dev* > vox1_dev_wav.zip\r\nunzip vox1_dev_wav.zip\r\n            \r\nwget https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip\r\nunzip vox1_test_wav.zip\r\n            \r\n# d"
    },
    {
      "id": "9",
      "title": "Fix KeyboardInterrupt in map and bad indices in select",
      "body": "If you interrupted a map function while it was writing, the cached file was not discarded.\r\nTherefore the next time you called map, it was loading an incomplete arrow file.\r\n\r\nWe had the same issue with select if there was a bad indice at one point.\r\n\r\nTo fix that I used temporary files that are renamed once everything is finished."
    },
    {
      "id": "10",
      "title": "Add MWSC",
      "body": "Adding the [Modified Winograd Schema Challenge](https://github.com/salesforce/decaNLP/blob/master/local_data/schema.txt) dataset which formed part of the [decaNLP](http://decanlp.com/) benchmark. Not sure how much use people would find for it it outside of the benchmark, but it is general purpose.\r\n\r\nCode is heavily borrowed from the [decaNLP repo](https://github.com/salesforce/decaNLP/blob/1e9605f246b9e05199b28bde2a2093bc49feeeaa/text/torchtext/datasets/generic.py#L773-L877).\r\n\r\nThere's a few (possibly overly opinionated) design choices I made:\r\n\r\n- I used the train/test/dev split [buried in the decaNLP code](https://github.com/salesforce/decaNLP/blob/1e9605f246b9e05199b28bde2a2093bc49feeeaa/text/torchtext/datasets/generic.py#L852-L855)\r\n- I split out each example into the 2 alternatives. Originally the data uses the format:\r\n    ```\r\n    The city councilmen refused the demonstrators a permit because they [feared/advocated] violence. \r\n    Who [feared/advocated] violence? \r\n    counci"
    },
    {
      "id": "11",
      "title": "Not able to access the XNLI dataset",
      "body": "When I try to access the XNLI dataset, I get the following error. The option of plain_text get selected automatically and then I get the following error.\r\n\r\n```\r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/sasha/.cache/huggingface/datasets/xnli/plain_text/1.0.0/dataset_info.json'\r\nTraceback:\r\nFile \"/home/sasha/.local/lib/python3.7/site-packages/streamlit/ScriptRunner.py\", line 322, in _run_script\r\n    exec(code, module.__dict__)\r\nFile \"/home/sasha/nlp_viewer/run.py\", line 86, in <module>\r\n    dts, fail = get(str(option.id), str(conf_option.name) if conf_option else None)\r\nFile \"/home/sasha/.local/lib/python3.7/site-packages/streamlit/caching.py\", line 591, in wrapped_func\r\n    return get_or_create_cached_value()\r\nFile \"/home/sasha/.local/lib/python3.7/site-packages/streamlit/caching.py\", line 575, in get_or_create_cached_value\r\n    return_value = func(*args, **kwargs)\r\nFile \"/home/sasha/nlp_viewer/run.py\", line 72, in get\r\n    builder_instance = builder_cls(name=conf"
    },
    {
      "id": "12",
      "title": "Add FiNER dataset",
      "body": "Hi,\r\n\r\nthis PR adds \"A Finnish News Corpus for Named Entity Recognition\" as new `finer` dataset.\r\n\r\nThe dataset is described in [this paper](https://arxiv.org/abs/1908.04212). The data is publicly available in [this GitHub](https://github.com/mpsilfve/finer-data).\r\n\r\nNotice: they provide two testsets. The additional test dataset taken from Wikipedia is named as \"test_wikipedia\" split."
    },
    {
      "id": "13",
      "title": "Add tedlium",
      "body": "## Adding a Dataset\r\n- **Name:** *tedlium*\r\n- **Description:** *The TED-LIUM 1-3 corpus is English-language TED talks, with transcriptions, sampled at 16kHz. It contains about 118 hours of speech.*\r\n- **Paper:** Homepage: http://www.openslr.org/7/, https://lium.univ-lemans.fr/en/ted-lium2/ &, https://www.openslr.org/51/\r\n- **Data:** http://www.openslr.org/7/\r\n- **Motivation:** Important speech dataset\r\n- **TFDatasets Implementation**: https://www.tensorflow.org/datasets/catalog/tedlium\r\nIf interested in tackling this issue, feel free to tag @patrickvonplaten\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n"
    },
    {
      "id": "14",
      "title": "Bad message when config name is missing",
      "body": "When loading a dataset that have several configurations, we expect to see an error message if the user doesn't specify a config name.\r\n\r\nHowever in `datasets` 1.10.0 and 1.10.1 it doesn't show the right message:\r\n\r\n```python\r\nimport datasets\r\n\r\ndatasets.load_dataset(\"glue\")\r\n```\r\nraises\r\n```python\r\nAttributeError: 'BuilderConfig' object has no attribute 'text_features'\r\n```\r\ninstead of\r\n```python\r\nValueError: Config name is missing.\r\nPlease pick one among the available configs: ['cola', 'sst2', 'mrpc', 'qqp', 'stsb', 'mnli', 'mnli_mismatched', 'mnli_matched', 'qnli', 'rte', 'wnli', 'ax']\r\nExample of usage:\r\n        `load_dataset('glue', 'cola')`\r\n```"
    },
    {
      "id": "15",
      "title": "Update ADD NEW DATASET",
      "body": "This PR adds a couple of detail on cloning/rebasing the repo."
    },
    {
      "id": "16",
      "title": "Update README.md",
      "body": "small typo"
    },
    {
      "id": "17",
      "title": "Adds APPS dataset to the hub [WIP]",
      "body": "A loading script for [APPS dataset](https://github.com/hendrycks/apps) "
    },
    {
      "id": "18",
      "title": "Fix manual download instructions",
      "body": "This PR replaces the static `DatasetBulider` variable `MANUAL_DOWNLOAD_INSTRUCTIONS` by a property function `manual_download_instructions()`. \r\n\r\nSome datasets like XTREME and all WMT need the manual data dir only for a small fraction of the possible configs.\r\n\r\nAfter some brainstorming with @mariamabarham and @lhoestq, we came to the conclusion that having a property function `manual_download_instructions()` gives us more flexibility to decide on a per config basis in the dataset builder if manual download instructions are needed.\r\n\r\nAlso this PR should unblock solves a bug with `wmt16 - ro-en` \r\n@sshleifer from this branch you should be able to succesfully run\r\n\r\n```python \r\nimport nlp \r\nds = nlp.load_dataset('./datasets/wmt16', 'ro-en')\r\n```\r\n\r\nand once this PR is merged S3 should be synched so that \r\n\r\n```python\r\nimport nlp\r\nds = nlp.load_dataset(\"wmt16\", \"ro-en\")\r\n```\r\n\r\nworks as well.\r\n\r\n**Important**: Since `MANUAL_DOWNLOAD_INSTRUCTIONS` was not really exposed to the user, this "
    },
    {
      "id": "19",
      "title": "add AJGT dataset",
      "body": "Arabic Jordanian General Tweets."
    },
    {
      "id": "20",
      "title": "Datasets cli improvements",
      "body": "This PR:\r\n* replaces the code from the `bug_report.md` that was used to get relevant system info with a dedicated command (a more elegant approach than copy-pasting the code IMO)\r\n* removes the `download` command (copied from the transformers repo?)\r\n* adds missing help messages to the cli commands\r\n\r\n\r\n\r\n"
    },
    {
      "id": "21",
      "title": "Fix Blog Authorship Corpus dataset",
      "body": "This PR:\r\n- Update the JSON metadata file, which previously was raising a `NonMatchingSplitsSizesError`\r\n- Fix the codec of the data files (`latin_1` instead of `utf-8`), which previously was raising ` UnicodeDecodeError` for some files\r\n\r\nClose #2679."
    },
    {
      "id": "22",
      "title": "Update legacy Python image for CI tests in Linux",
      "body": "Instead of legacy, use next-generation convenience images, built from the ground up with CI, efficiency, and determinism in mind. Here are some of the highlights:\r\n\r\n- Faster spin-up time - In Docker terminology, these next-gen images will generally have fewer and smaller layers. Using these new images will lead to faster image downloads when a build starts, and a higher likelihood that the image is already cached on the host.\r\n\r\n- Improved reliability and stability - The existing legacy convenience images are rebuilt practically every day with potential changes from upstream that we cannot always test fast enough. This leads to frequent breaking changes, which is not the best environment for stable, deterministic builds. Next-gen images will only be rebuilt for security and critical-bugs, leading to more stable and deterministic images.\r\n\r\nMore info: https://circleci.com/docs/2.0/circleci-images"
    },
    {
      "id": "23",
      "title": "Conda support",
      "body": "Will push a new version on anaconda cloud every time a tag starting with `v` is pushed (like `v1.2.2`).\r\n\r\nWill appear here: https://anaconda.org/huggingface/datasets\r\n\r\nDepends on `conda-forge` for now, so the following is required for installation:\r\n\r\n```\r\nconda install -c huggingface -c conda-forge datasets\r\n```"
    },
    {
      "id": "24",
      "title": "handle data alteration when trying type",
      "body": "Fix #649 \r\n\r\nThe bug came from the type inference that didn't handle a weird case in Pyarrow.\r\nIndeed this code runs without error but alters the data in arrow:\r\n```python\r\nimport pyarrow as pa\r\n\r\ntype = pa.struct({\"a\": pa.struct({\"b\": pa.string()})})\r\narray_with_altered_data = pa.array([{\"a\": {\"b\": \"foo\", \"c\": \"bar\"}}] * 10, type=type)\r\nprint(array_with_altered_data[0].as_py())\r\n# {'a': {'b': 'foo'}} -> the sub-field \"c\" is missing\r\n```\r\n(I don't know if this is intended in pyarrow tbh)\r\n\r\nWe didn't take this case into account during type inference. By default it was keeping old features and maybe alter data.\r\nTo fix that I added a line that checks that the first element of the array is not altered."
    },
    {
      "id": "25",
      "title": "Update format columns in Dataset.rename_columns",
      "body": "Fixes #2026 "
    },
    {
      "id": "26",
      "title": "Adding wiki lingua dataset as new branch",
      "body": "Adding the dataset as new branch as advised here: #1470\r\n"
    },
    {
      "id": "27",
      "title": "Update dataset loading and features - Add TREC dataset",
      "body": "This PR:\r\n- add a template for a new dataset script\r\n- update the caching structure so that the path to the cached data files is also a function of the dataset loading script hash. This way when you update a loading script the data will be automatically updated instead of falling back to the previous version (which is usually a outdated). This makes it in particular easier to iterate when writing a new dataset loading script.\r\n- fix a bug in the `ClassLabel` feature and make it more flexible so that its methods `str2int` and `int2str` can also accept list, numpy arrays and PyTorch/TensorFlow tensors.\r\n- add the TREC-6 dataset"
    },
    {
      "id": "28",
      "title": "Implementation of class_encode_column",
      "body": "Addresses #2176 \r\n\r\nI'm happy to discuss the API and internals!"
    },
    {
      "id": "29",
      "title": "Mapping in the distributed setting",
      "body": "The barrier trick for distributed mapping as discussed on Thursday with @lhoestq"
    },
    {
      "id": "30",
      "title": "Add BERTScore to metrics",
      "body": "This PR adds [BERTScore](https://arxiv.org/abs/1904.09675) to metrics.\r\nHere is an example of how to use it.\r\n\r\n```sh\r\nimport nlp\r\nbertscore = nlp.load_metric('metrics/bertscore') # or simply nlp.load_metric('bertscore') after this is added to huggingface's s3 bucket\r\npredictions = ['example', 'fruit']\r\nreferences = [['this is an example.', 'this is one example.'], ['apple']]\r\nresults = bertscore.compute(predictions, references, lang='en')\r\nprint(results)\r\n```"
    },
    {
      "id": "31",
      "title": "add wikisql",
      "body": "Adding the [WikiSQL](https://github.com/salesforce/WikiSQL) dataset.\r\n\r\nInteresting things to note:\r\n- Have copied the function (`_convert_to_human_readable`) which converts the SQL query to a human-readable (string) format as this is what most people will want when actually using this dataset for NLP applications.\r\n- `conds` was originally a tuple but is converted to a dictionary to support differing types.\r\n\r\nWould be nice to add the logical_form metrics too at some point."
    },
    {
      "id": "32",
      "title": "Add metrics usage examples and tests",
      "body": "All metrics finally have usage examples and proper fast + slow tests :)\r\n\r\nI added examples of usage for every metric, and I use doctest to make sure they all work as expected.\r\n\r\nFor \"slow\" metrics such as bert_score or bleurt which require to download + run a transformer model, the download + forward pass are only done in the slow test.\r\nIn the fast test on the other hand, the download + forward pass are monkey patched.\r\n\r\nMetrics that need to be installed from github are not added to setup.py because it prevents uploading the `datasets` package to pypi.\r\nAn additional-test-requirements.txt file is used instead. This file also include `comet` in order to not have to resolve its *impossible* dependencies.\r\n\r\nAlso `comet` is not tested on windows because one of its dependencies (fairseq) can't be installed in the CI for some reason."
    },
    {
      "id": "33",
      "title": "wmt19 is broken",
      "body": "1. Check which lang pairs we have: `--dataset_name wmt19`:\r\n\r\nPlease pick one among the available configs: ['cs-en', 'de-en', 'fi-en', 'gu-en', 'kk-en', 'lt-en', 'ru-en', 'zh-en', 'fr-de']\r\n\r\n \r\n2. OK, let's pick `ru-en`:\r\n\r\n`--dataset_name wmt19 --dataset_config \"ru-en\"`\r\n\r\nno cookies:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"./run_seq2seq.py\", line 661, in <module>\r\n    main()\r\n  File \"./run_seq2seq.py\", line 317, in main\r\n    datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name)\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/load.py\", line 740, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/builder.py\", line 572, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/builder.py\", line 628, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_gene"
    },
    {
      "id": "34",
      "title": "Caching doesn't work for map (non-deterministic)",
      "body": "The caching functionality doesn't work reliably when tokenizing a dataset. Here's a small example to reproduce it. \r\n\r\n```python\r\nimport nlp\r\nimport transformers\r\n\r\ndef main():\r\n    ds = nlp.load_dataset(\"reddit\", split=\"train[:500]\")\r\n\r\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\r\n\r\n    def convert_to_features(example_batch):\r\n        input_str = example_batch[\"body\"]\r\n        encodings = tokenizer(input_str, add_special_tokens=True, truncation=True)\r\n        return encodings\r\n\r\n    ds = ds.map(convert_to_features, batched=True)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nRoughly 3/10 times, this example recomputes the tokenization.\r\n\r\nIs this expected behaviour?"
    },
    {
      "id": "35",
      "title": "offset overflow when multiprocessing batched map on large datasets.",
      "body": "It only happened when \"multiprocessing\" + \"batched\" + \"large dataset\" at the same time.\r\n\r\n```\r\ndef bprocess(examples):\r\n  examples['len'] = []\r\n  for text in examples['text']:\r\n    examples['len'].append(len(text))\r\n  return examples\r\nwiki.map(brpocess, batched=True, num_proc=8)\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nRemoteTraceback                           Traceback (most recent call last)\r\nRemoteTraceback: \r\n\"\"\"\r\nTraceback (most recent call last):\r\n  File \"/home/yisiang/miniconda3/envs/ml/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"/home/yisiang/datasets/src/datasets/arrow_dataset.py\", line 153, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/yisiang/datasets/src/datasets/fingerprint.py\", line 163, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/home/yisiang/datasets/src/datasets/arrow_dataset.py\", line 148"
    },
    {
      "id": "36",
      "title": "fixed download link for web_science",
      "body": "Fixes #2337. Should work with:\r\n`dataset = load_dataset(\"web_of_science\", \"WOS11967\", ignore_verifications=True)`"
    },
    {
      "id": "37",
      "title": "Add support for other languages for rouge",
      "body": "I calculate rouge with\r\n```\r\nfrom datasets import load_metric\r\nrouge = load_metric(\"rouge\")\r\nrouge_output = rouge.compute(predictions=['Ñ‚ÐµÑÑ‚ Ñ‚ÐµÑÑ‚ Ð¿Ñ€Ð¸Ð²ÐµÑ‚'], references=['Ñ‚ÐµÑÑ‚ Ñ‚ÐµÑÑ‚ Ð¿Ð¾ÐºÐ°'], rouge_types=[\r\n    \"rouge2\"])[\"rouge2\"].mid\r\nprint(rouge_output)\r\n```\r\nthe result is\r\n`Score(precision=0.0, recall=0.0, fmeasure=0.0)`\r\nIt seems like the `rouge_score` library that this metric uses filters all non-alphanueric latin characters \r\nin `rouge_scorer/tokenize.py` with `text = re.sub(r\"[^a-z0-9]+\", \" \", six.ensure_str(text))`.\r\nPlease add support for other languages. "
    },
    {
      "id": "38",
      "title": "add Indonesian newspapers (id_newspapers_2018)",
      "body": "The dataset contains around 500K articles (136M of words) from 7 Indonesian newspapers. The size of uncompressed 500K json files (newspapers-json.tgz) is around 2.2GB."
    },
    {
      "id": "39",
      "title": "Adding XQUAD-R Dataset",
      "body": ""
    },
    {
      "id": "40",
      "title": "Remove unused py_utils objects",
      "body": "Remove unused/unnecessary py_utils functions/classes."
    },
    {
      "id": "41",
      "title": "Add Spanish Billion Words Corpus",
      "body": "Add an unannotated Spanish corpus of nearly 1.5 billion words, compiled from different resources from the web."
    },
    {
      "id": "42",
      "title": "(Load dataset failure) ConnectionError: Couldnâ€™t reach https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/cnn_dailymail/cnn_dailymail.py",
      "body": "Hey, I want to load the cnn-dailymail dataset for fine-tune.\r\nI write the code like this\r\nfrom datasets import load_dataset\r\n\r\ntest_dataset = load_dataset(â€œcnn_dailymailâ€, â€œ3.0.0â€, split=â€œtrainâ€)\r\n\r\nAnd I got the following errors.\r\n\r\nTraceback (most recent call last):\r\nFile â€œtest.pyâ€, line 7, in\r\ntest_dataset = load_dataset(â€œcnn_dailymailâ€, â€œ3.0.0â€, split=â€œtestâ€)\r\nFile â€œC:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\load.pyâ€, line 589, in load_dataset\r\nmodule_path, hash = prepare_module(\r\nFile â€œC:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\load.pyâ€, line 268, in prepare_module\r\nlocal_path = cached_path(file_path, download_config=download_config)\r\nFile â€œC:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\utils\\file_utils.pyâ€, line 300, in cached_path\r\noutput_path = get_from_cache(\r\nFile â€œC:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\utils\\file_utils.py"
    },
    {
      "id": "43",
      "title": "Benchmarks",
      "body": "Adding some benchmarks with DVC/CML\r\n\r\nTo add a new tracked benchmark:\r\n- create a new python benchmarking script in `./benchmarks/`. The script can use the utilities in `./benchmarks/utils.py` and should output a JSON file with results in `./benchmarks/results/`.\r\n- add a new pipeline stage in [dvc.yaml](./dvc.yaml) with the name of your new benchmark.\r\n\r\nThat's it"
    },
    {
      "id": "44",
      "title": "'cp950' codec error from load_dataset('xtreme', 'tydiqa')",
      "body": "![image](https://user-images.githubusercontent.com/50871412/86744744-67481680-c06c-11ea-8612-b77eba92a392.png)\r\n\r\nI guess the error is related to python source encoding issue that my PC is trying to decode the source code with wrong encoding-decoding tools, perhaps :\r\nhttps://www.python.org/dev/peps/pep-0263/\r\n\r\nI guess the error was triggered by the code \" module = importlib.import_module(module_path)\" at line 57 in the source code:  nlp/src/nlp/load.py / (https://github.com/huggingface/nlp/blob/911d5596f9b500e39af8642fe3d1b891758999c7/src/nlp/load.py#L51)\r\n\r\nAny ideas?\r\n\r\np.s. tried the same code on colab, that runs perfectly\r\n"
    },
    {
      "id": "45",
      "title": "Render docstring return type as inline",
      "body": "This documentation setting will avoid having the return type in a separate line under `Return type`. \r\n\r\nSee e.g. current docs for `Dataset.to_csv`."
    },
    {
      "id": "46",
      "title": "add stack exchange",
      "body": "stack exchange is part of EleutherAI/The Pile, but AFAIK, The Pile dataset blend all sub datasets together thus we are not able to use just one of its sub dataset from The Pile data. So I create an independent dataset using The Pile preliminary components.\r\n\r\nI also change default `timeout` to 100 seconds instead of 10 seconds, otherwise I keep getting read time out when downloading source data of stack exchange and cc100 dataset.\r\n\r\nWhen I was creating dataset card. I found there is room for creating / editing dataset card. I've made it an issue. #2797\r\n\r\nAlso I am wondering whether the import of The Pile dataset is actively undertaken (because I may need it recently)? #1675"
    },
    {
      "id": "47",
      "title": "`Can not decode content-encoding: gzip` when loading `scitldr` dataset with streaming",
      "body": "## Describe the bug\r\n\r\nTrying to load the `\"FullText\"` config of the `\"scitldr\"` dataset with `streaming=True` raises an error from `aiohttp`:\r\n```python\r\nClientPayloadError: 400, message='Can not decode content-encoding: gzip'\r\n```\r\n\r\ncc @lhoestq \r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\niter_dset = iter(\r\n    load_dataset(\"scitldr\", name=\"FullText\", split=\"test\", streaming=True)\r\n)\r\n\r\nnext(iter_dset)\r\n```\r\n\r\n## Expected results\r\nReturns the first sample of the dataset\r\n\r\n## Actual results\r\nCalling `__next__` crashes with the following Traceback:\r\n\r\n```python\r\n----> 1 next(dset_iter)\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in __iter__(self)\r\n    339\r\n    340     def __iter__(self):\r\n--> 341         for key, example in self._iter():\r\n    342             if self.features:\r\n    343                 # we encode the example for ClassLabel feature types for example\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\da"
    },
    {
      "id": "48",
      "title": "[Testing] Improved testing structure",
      "body": "This PR refactors the test design a bit and puts the mock download manager in the `utils` files as it is just a test helper class.\r\n\r\nas @mariamabarham pointed out, creating a dummy folder structure can be quite hard to grasp.\r\nThis PR tries to change that to some extent.\r\n\r\nIt follows the following logic for the `dummy` folder structure now:\r\n1.) The data bulider has no config -> the  `dummy` folder structure is:\r\n`dummy/<version>/dummy_data.zip`\r\n2) The data builder has >= 1 configs -> the `dummy` folder structure is: \r\n`dummy/<config_name_1>/<version>/dummy_data.zip`\r\n`dummy/<config_name_2>/<version>/dummy_data.zip`\r\n\r\nNow, the difficult part is how to create the `dummy_data.zip` file. There are two cases:\r\nA) The `data_urs` parameter inserted into the `download_and_extract` fn is a **string**:\r\n-> the `dummy_data.zip` file zips the folder: \r\n`dummy_data/<relative_path_of_folder_structure_of_url>`\r\nB) The `data_urs` parameter inserted into the `download_and_extract` fn is a **dict**"
    },
    {
      "id": "49",
      "title": "lener_br dataset: add instances and data splits info",
      "body": ""
    },
    {
      "id": "50",
      "title": "gitignore .python-version",
      "body": "ignore `.python-version` added by `pyenv`"
    },
    {
      "id": "51",
      "title": "How to use similarity settings  other then \"BM25\" in Elasticsearch index ?",
      "body": "**QUESTION : How should we use other similarity algorithms supported by Elasticsearch other than \"BM25\"  ?**\r\n**ES Reference**\r\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-similarity.html\r\n**HF doc reference:**\r\nhttps://huggingface.co/docs/datasets/faiss_and_ea.html\r\n\r\n**context :**\r\n========\r\n\r\nI used the latest Elasticsearch server  version 7.9.2\r\nWhen I set DFR  which is one of the other similarity algorithms supported by elasticsearch  in the mapping, I get an error\r\n\r\nFor example DFR that I had tried in the first instance in mappings as below.,\r\n`\"mappings\": {\"properties\": {\"text\": {\"type\": \"text\", \"analyzer\": \"standard\", \"similarity\": \"DFR\"}}},`\r\n\r\nI get the following error \r\nRequestError: RequestError(400, 'mapper_parsing_exception', 'Unknown Similarity type [DFR] for field [text]')\r\n\r\nThe other thing as another option I had tried was to declare \"similarity\": \"my_similarity\" within settings and then assigning \"my_similarity\" inside the mappings "
    },
    {
      "id": "52",
      "title": "Add CodeSearchNet corpus dataset",
      "body": "This PR adds the CodeSearchNet corpus proxy dataset for semantic code search: https://github.com/github/CodeSearchNet\r\nI have had a few issues, mentioned below. Would appreciate some help on how to solve them.\r\n\r\n## Issues generating dataset card\r\nIs there something wrong with my declaration of the dataset features ?\r\n```\r\nfeatures=datasets.Features(\r\n    {\r\n        \"repository_name\": datasets.Value(\"string\"),\r\n        \"func_path_in_repository\": datasets.Value(\"string\"),\r\n        \"func_name\": datasets.Value(\"string\"),\r\n        \"whole_func_string\": datasets.Value(\"string\"),\r\n        \"language\": datasets.Value(\"string\"),\r\n        \"func_code_string\": datasets.Value(\"string\"),\r\n        \"func_code_tokens\": datasets.Sequence(datasets.Value(\"string\")),\r\n        \"func_documentation_string\": datasets.Value(\"string\"),\r\n        \"func_documentation_tokens\": datasets.Sequence(datasets.Value(\"string\")),\r\n        \"split_name\": datasets.Value(\"string\"),\r\n        \"func_code_url\": datasets.Value(\"string"
    },
    {
      "id": "53",
      "title": "Unable to format dataset to CUDA Tensors",
      "body": "Hi,\r\n\r\nI came across this [link](https://huggingface.co/docs/datasets/torch_tensorflow.html) where the docs show show to convert a dataset to a particular format. I see that there is an option to convert it to tensors, but I don't see any option to convert it to CUDA tensors.\r\n\r\nI tried this, but Dataset doesn't support assignment:\r\n```\r\n  columns=['input_ids', 'token_type_ids', 'attention_mask', 'start_positions','end_positions']\r\n\r\n        samples.set_format(type='torch', columns = columns)\r\n        for column in columns:\r\n            samples[column].to(torch.device(self.config.device))\r\n```\r\nThere should be an option to do so, or if there is already a way to do this, please let me know.\r\n\r\nThanks,\r\nGunjan"
    },
    {
      "id": "54",
      "title": "fix README typos/ consistency",
      "body": ""
    },
    {
      "id": "55",
      "title": "Add xquad-r dataset",
      "body": ""
    },
    {
      "id": "56",
      "title": "adding swedish_medical_ner",
      "body": "Adding the Swedish Medical NER dataset, listed in \"Biomedical Datasets - BigScience Workshop 2021\""
    },
    {
      "id": "57",
      "title": "News_commentary Dataset Translation Pairs are of Incorrect Language Specified Pairs",
      "body": "I used load_dataset to load the news_commentary dataset for \"ar-en\" translation pairs but found translations from Arabic to Hindi.  \r\n\r\n```\r\ntrain_ds = load_dataset(\"news_commentary\", \"ar-en\", split='train[:98%]')\r\nval_ds = load_dataset(\"news_commentary\", \"ar-en\", split='train[98%:]')\r\n\r\n# filtering out examples that are not ar-en translations but ar-hi\r\nval_ds = val_ds.filter(lambda example, indice: indice not in chain(range(1312,1327) ,range(1384,1399), range(1030,1042)), with_indices=True)\r\n```\r\n\r\n* I'm fairly new to using datasets so I might be doing something wrong"
    },
    {
      "id": "58",
      "title": "Add KorQPair dataset",
      "body": "This PR adds a [Korean paired question dataset](https://github.com/songys/Question_pair) containing labels indicating whether two questions in a given pair are semantically identical. This dataset was used to evaluate the performance of [KoGPT2](https://github.com/SKT-AI/KoGPT2#subtask-evaluations) on a phrase detection downstream task. "
    },
    {
      "id": "59",
      "title": "WMT19 Dataset for Kazakh-English is not formatted correctly",
      "body": "In addition to the bug of languages being switched from Issue @415, there are incorrect translations in the dataset because the English-Kazakh translations have a one off formatting error.\r\n\r\nThe News Commentary v14 parallel data set for kk-en from http://www.statmt.org/wmt19/translation-task.html has a bug here:\r\n\r\n> Line 94. The Swiss National Bank, for its part, has been battling with the deflationary effects of the francâ€™s dramatic appreciation over the past few years.\tÐ¨Ð²ÐµÐ¹Ñ†Ð°Ñ€Ð¸ÑÐ½Ñ‹Ò£ Ò°Ð»Ñ‚Ñ‚Ñ‹Ò› Ð±Ð°Ð½ÐºÑ– Ó©Ð· Ñ‚Ð°Ñ€Ð°Ð¿Ñ‹Ð½Ð°Ð½, ÑÐ¾Ò£Ò“Ñ‹ Ð±Ñ–Ñ€Ð½ÐµÑˆÐµ Ð¶Ñ‹Ð» Ñ–ÑˆÑ–Ð½Ð´Ðµ Ñ„Ñ€Ð°Ð½Ðº Ò›Ò±Ð½Ñ‹Ð½Ñ‹Ò£ Ò›Ð°Ñ‚Ñ‚Ñ‹ Ó©ÑÑƒÑ–Ð½Ñ–Ò£ Ð´ÐµÑ„Ð»ÑÑ†Ð¸ÑÐ»Ñ‹Ò› Ó™ÑÐµÑ€Ñ–Ð¼ÐµÐ½ ÐºÒ¯Ñ€ÐµÑÑ–Ð¿ ÐºÐµÐ»ÐµÐ´Ñ–.\r\n> \r\n> Line 95. Ð”ÐµÑ„Ð»ÑÑ†Ð¸ÑÐ»Ñ‹Ò› ÐºÒ¯ÑˆÑ‚ÐµÑ€ 2008 Ð¶Ñ‹Ð»Ñ‹ Ñ‚ÐµÑ€ÐµÒ£ Ð¶Ó™Ð½Ðµ Ò±Ð·Ð°Ò›Ò›Ð° ÑÐ¾Ð·Ñ‹Ð»Ò“Ð°Ð½ Ð¶Ð°Ò»Ð°Ð½Ð´Ñ‹Ò› Ð´Ð°Ò“Ð´Ð°Ñ€Ñ‹ÑÒ›Ð° Ð±Ð°Ð¹Ð»Ð°Ð½Ñ‹ÑÑ‚Ñ‹ Ð¾Ñ€Ñ‹Ð½ Ð°Ð»Ò“Ð°Ð½ Ñ–Ñ€Ñ– ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸ÐºÐ°Ð»Ñ‹Ò› Ð¶Ó™Ð½Ðµ Ò›Ð°Ñ€Ð¶Ñ‹Ð»Ñ‹Ò› Ð¾Ñ€Ñ‹Ð½ Ð°Ð»Ð¼Ð°ÑÑƒÐ»Ð°Ñ€Ð´Ñ‹Ò£ Ð°Ñ€Ò›Ð°ÑÑ‹Ð½Ð´Ð° Ð±Ð¾ÑÐ°Ñ‚Ñ‹Ð»Ð´Ñ‹.  Ð–ÐµÐºÐµ Ò›Ð°Ñ€Ñ‹Ð· Ò›Ð°Ñ€Ð°Ð¶Ð°Ñ‚Ñ‹ Ò¯Ð»ÐµÑÑ–Ð½Ñ–Ò£ Ò›Ñ‹ÑÒ›Ð°Ñ€ÑƒÑ‹ Ð¾Ñ€Ñ‚Ð°Ð»Ñ‹Ò› Ð±Ð°Ð½ÐºÑ‚Ñ–Ò£ Ñ€ÐµÑ„Ð»ÑÑ†Ð¸ÑÒ“Ð° Ð¶Ò±Ð¼ÑÐ°Ð»Ò“Ð°Ð½ ÐºÒ¯Ñˆ-Ð¶Ñ–Ð³ÐµÑ€Ñ–Ð½Ðµ Ñ‚Ò±Ñ€Ð°Ò›Ñ‚Ñ‹ ÑÐ¾Ò›Ò›Ð°Ð½ Ò›Ð°Ñ€ÑÑ‹ Ð¶ÐµÐ»Ð´ÐµÐ¹ Ð±Ð¾Ð»Ð´Ñ‹.\r\n> \r\n> Line 96. The deflationary forces were unleashed by the major economic and fin"
    },
    {
      "id": "60",
      "title": "to_tf_dataset keeps a reference to the open data somewhere, causing issues on windows",
      "body": "To reproduce:\r\n```python\r\nimport datasets as ds\r\nimport weakref\r\nimport gc\r\n\r\nd = ds.load_dataset(\"mnist\", split=\"train\")\r\nref = weakref.ref(d._data.table)\r\ntfd = d.to_tf_dataset(\"image\", batch_size=1, shuffle=False, label_cols=\"label\")\r\ndel tfd, d\r\ngc.collect()\r\nassert ref() is None, \"Error: there is at least one reference left\"\r\n```\r\n\r\nThis causes issues because the table holds a reference to an open arrow file that should be closed. So on windows it's not possible to delete or move the arrow file afterwards.\r\n\r\nMoreover the CI test of the `to_tf_dataset` method isn't able to clean up the temporary arrow files because of this.\r\n\r\ncc @Rocketknight1 "
    },
    {
      "id": "61",
      "title": "Added downloading to Hyperpartisan news detection",
      "body": "Following the discussion on Slack and #349, I've updated the hyperpartisan dataset to pull directly from Zenodo rather than manual install, which should make this dataset much more accessible. Many thanks to @johanneskiesel !\r\n\r\nCurrently doesn't pass `test_load_real_dataset` - I'm using `self.config.name` which is `default` in this test. Might be related to #474"
    },
    {
      "id": "62",
      "title": "Remove outdated commands in favor of huggingface-cli",
      "body": "Removing the old user commands since `huggingface_hub` is going to be used instead.\r\ncc @julien-c "
    },
    {
      "id": "63",
      "title": "Support .xz file format",
      "body": "Add support to extract/uncompress files in .xz format."
    },
    {
      "id": "64",
      "title": "Fix COUNTER dataset",
      "body": "Fix filename generating `FileNotFoundError`.\r\n\r\nRelated to #2866.\r\nCC: @severo."
    },
    {
      "id": "65",
      "title": "Rename anli dataset",
      "body": "What we have now as the `anli` dataset is actually the Î±NLI dataset from the ART challenge dataset. This name is confusing because `anli` is also the name of adversarial NLI (see [https://github.com/facebookresearch/anli](https://github.com/facebookresearch/anli)).\r\n\r\nI renamed the current `anli` dataset by `art`."
    },
    {
      "id": "66",
      "title": "CRD3 dataset card",
      "body": "This PR adds additional information to the CRD3 dataset card. "
    },
    {
      "id": "67",
      "title": "Better error message when using the wrong load_from_disk",
      "body": "As mentioned in #2424, the error message when one tries to use `Dataset.load_from_disk` to load a DatasetDict object (or _vice versa_) can be improved. I added a suggestion in the error message to let users know that they should use the other one."
    },
    {
      "id": "68",
      "title": "Always update metadata in arrow schema",
      "body": "We store a redundant copy of the features in the metadata of the schema of the arrow table. This is used to recover the features when doing `Dataset.from_file`. These metadata are updated after each transfor, that changes the feature types.\r\n\r\nFor each function that transforms the feature types of the dataset, I added a step in the tests to make sure the metadata in the arrow schema are up to date.\r\n\r\nI also added a line to update the metadata directly in the Dataset.__init__ method.\r\nThis way even a dataset instantiated with __init__ will have a table with the right metadata.\r\n\r\ncc @mariosasko "
    },
    {
      "id": "69",
      "title": "wmt16 does not download ",
      "body": "Hi, I appreciate your help with the following error, thanks \r\n\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\"wmt16\", \"ro-en\", split=\"train\")\r\nDownloading and preparing dataset wmt16/ro-en (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/7b2c4443a7d34c2e13df267eaa8cab4c62dd82f6b62b0d9ecc2e3a673ce17308...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/load.py\", line 611, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/builder.py\", line 476, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/builder.py\", line 531, in _download_and_prepare\r\n "
    },
    {
      "id": "70",
      "title": "Add Turkish News Category Dataset (270K).Updates were made for reviewâ€¦",
      "body": "This PR adds the **Turkish News Categories Dataset (270K)** dataset which is a text classification dataset by me and @yavuzKomecoglu. Turkish news dataset consisting of **273601 news in 17 categories**, compiled from printed media and news websites between 2010 and 2017 by the [Interpress](https://www.interpress.com/) media monitoring company.\r\n\r\n**Note**: Resubmitted as a clean version of the previous Pull Request(#1419).  @SBrandeis @lhoestq "
    },
    {
      "id": "71",
      "title": "How to sample every file in a list of files making up a split in a dataset when loading?",
      "body": "I am loading a dataset with multiple train, test, and validation files like this:\r\n\r\n```\r\ndata_files_dict = {\r\n    \"train\": [train_file1, train_file2],\r\n    \"test\": [test_file1, test_file2],\r\n    \"val\": [val_file1, val_file2]\r\n}\r\ndataset = datasets.load_dataset(\r\n    \"csv\",\r\n    data_files=data_files_dict,\r\n    split=['train[:8]', 'test[:8]', 'val[:8]']\r\n)\r\n\r\n```\r\n\r\nHowever, this only selects the first 8 rows from train_file1, test_file1, val_file1, since they are the first files in the lists.\r\n\r\nI'm trying to formulate a split argument that can sample from each file specified in my list of files that make up each split.\r\n\r\nIs this type of splitting supported? If so, how can I do it?"
    },
    {
      "id": "72",
      "title": "wmt datasets fail to load",
      "body": "~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\wmt14\\43e717d978d2261502b0194999583acb874ba73b0f4aed0ada2889d1bb00f36e\\wmt_utils.py in _split_generators(self, dl_manager)\r\n    758         # Extract manually downloaded files.\r\n    759         manual_files = dl_manager.extract(manual_paths_dict)\r\n--> 760         extraction_map = dict(downloaded_files, **manual_files)\r\n    761 \r\n    762         for language in self.config.language_pair:\r\n\r\nTypeError: type object argument after ** must be a mapping, not list"
    },
    {
      "id": "73",
      "title": "Add more issue templates and customize issue template chooser",
      "body": "When opening an issue, it is not evident for the users how to choose a blank issue template. There is a link at the bottom of all the other issue templates (`Donâ€™t see your issue here? Open a blank issue.`), but this is not very visible for users. This is the reason why many users finally chose the `add-dataset` template instead (this is more visible) for issues that indeed are not requesting the addition of a new dataset.\r\n\r\n~~With this PR, the default blank issue template would be as visible as the other templates (as the `add-dataset` template), thus making easier for the users to choose it.~~\r\n\r\nWith this PR:\r\n- more issue templates, besides `add-dataset`, are added: `bug-report` and `feature-request`\r\n- the issue template chooser is customized, so that it now includes a link to `Discussions` for questions"
    },
    {
      "id": "74",
      "title": "ðŸ› [Dataset] Cannot download wmt14, wmt15 and wmt17",
      "body": "1. I try downloading `wmt14`, `wmt15`, `wmt17`, `wmt19` with the following code:\r\n```\r\nnlp.load_dataset('wmt14','de-en')\r\nnlp.load_dataset('wmt15','de-en')\r\nnlp.load_dataset('wmt17','de-en')\r\nnlp.load_dataset('wmt19','de-en')\r\n```\r\nThe code runs but the download speed is **extremely slow**, the same behaviour is not observed on `wmt16` and `wmt18`\r\n\r\n2. When trying to download `wmt17 zh-en`, I got the following error:\r\n> ConnectionError: Couldn't reach https://storage.googleapis.com/tfdataset-data/downloadataset/uncorpus/UNv1.0.en-zh.tar.gz"
    },
    {
      "id": "75",
      "title": "Start filling GLUE dataset card",
      "body": "The dataset card was pretty much empty.\r\n\r\nI added the descriptions (mainly from TFDS since the script is the same), and I also added the tasks tags as well as examples for a subset of the tasks.\r\n\r\ncc @sgugger "
    },
    {
      "id": "76",
      "title": "fix overflow check",
      "body": "I did some tests and unfortunately the test\r\n```\r\npa_array.nbytes > MAX_BATCH_BYTES\r\n```\r\ndoesn't work. Indeed for a StructArray, `nbytes` can be less 2GB even if there is an overflow (it loops...).\r\n\r\nI don't think we can do a proper overflow test for the limit of 2GB...\r\n\r\nFor now I replaced it with a sanity check on the first element."
    },
    {
      "id": "77",
      "title": "Add DER metric for SUPERB speaker diarization task",
      "body": ""
    },
    {
      "id": "78",
      "title": "adding opus_openoffice",
      "body": "Adding Opus OpenOffice: http://opus.nlpl.eu/OpenOffice.php\r\n8 languages, 28 bitexts"
    },
    {
      "id": "79",
      "title": "add yelp_review_full",
      "body": "This corresponds to the Yelp-5 requested in https://github.com/huggingface/datasets/issues/353\r\nI included the dataset card. "
    },
    {
      "id": "80",
      "title": "Replace script_version with revision",
      "body": "As discussed in https://github.com/huggingface/datasets/pull/2718#discussion_r707013278, the parameter name `script_version` is no longer applicable to datasets without loading script (i.e., datasets only with raw data files).\r\n\r\nThis PR replaces the parameter name `script_version` with `revision`.\r\n\r\nThis way, we are also aligned with:\r\n- Transformers: `AutoTokenizer.from_pretrained(..., revision=...)`\r\n- Hub: `HfApi.dataset_info(..., revision=...)`, `HfApi.upload_file(..., revision=...)`"
    },
    {
      "id": "81",
      "title": "NewsPH NLI dataset script fails to access test data.",
      "body": "In Newsph-NLI Dataset (#1192), it fails to access test data.\r\n\r\nAccording to the script below, the download manager will download the train data when trying to download the test data. \r\n\r\nhttps://github.com/huggingface/datasets/blob/2a2dd6316af2cc7fdf24e4779312e8ee0c7ed98b/datasets/newsph_nli/newsph_nli.py#L71\r\n\r\nIf you download it according to the script above, you can see that train and test receive the same data as shown below.\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> newsph_nli = load_dataset(path=\"./datasets/newsph_nli.py\")\r\n>>> newsph_nli\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['premise', 'hypothesis', 'label'],\r\n        num_rows: 420000\r\n    })\r\n    test: Dataset({\r\n        features: ['premise', 'hypothesis', 'label'],\r\n        num_rows: 420000\r\n    })\r\n    validation: Dataset({\r\n        features: ['premise', 'hypothesis', 'label'],\r\n        num_rows: 90000\r\n    })\r\n})\r\n>>> newsph_nli[\"train\"][0]\r\n{'hypothesis': 'Ito ang dineklara ni Atty. Romulo M"
    },
    {
      "id": "82",
      "title": "adding yahoo_answers_qa",
      "body": "Adding Yahoo Answers QA dataset.\r\n\r\nMore info:\r\nhttps://ciir.cs.umass.edu/downloads/nfL6/"
    },
    {
      "id": "83",
      "title": "Don't use old, incompatible cache for the new `filter`",
      "body": "#2836 changed `Dataset.filter` and the resulting data that are stored in the cache are different and incompatible with the ones of the previous `filter` implementation.\r\n\r\nHowever the caching mechanism wasn't able to differentiate between the old and the new implementation of filter (only the method name was taken into account). \r\n\r\nThis is an issue because anyone that update `datasets` and re-runs some code that uses `filter` would see an error, because the cache would try to load an incompatible `filter` result.\r\n\r\nTo fix this I added the notion of versioning for dataset transform in the caching mechanism, and bumped the version of the `filter` implementation to 2.0.0\r\n\r\nThis way the new `filter` outputs are now considered different from the old ones from the caching point of view.\r\n\r\nThis should fix #2943\r\n\r\ncc @anton-l"
    },
    {
      "id": "84",
      "title": "Add align_labels_with_mapping to DatasetDict",
      "body": "https://github.com/huggingface/datasets/pull/2457 added the `Dataset.align_labels_with_mapping` method.\r\nIn this PR I also added `DatasetDict.align_labels_with_mapping`"
    },
    {
      "id": "85",
      "title": "update multiwozv22 checksums",
      "body": "a file was updated on the GitHub repo for the dataset"
    },
    {
      "id": "86",
      "title": "Index outside of table length",
      "body": "The offset input box warns of numbers larger than a limit (like 2000) but then the errors start at a smaller value than that limit (like 1955).\r\n\r\n> ValueError: Index (2000) outside of table length (2000).\r\n> Traceback:\r\n> File \"/home/sasha/.local/lib/python3.7/site-packages/streamlit/ScriptRunner.py\", line 322, in _run_script\r\n>     exec(code, module.__dict__)\r\n> File \"/home/sasha/nlp_viewer/run.py\", line 116, in <module>\r\n>     v = d[item][k]\r\n> File \"/home/sasha/.local/lib/python3.7/site-packages/nlp/arrow_dataset.py\", line 338, in __getitem__\r\n>     output_all_columns=self._output_all_columns,\r\n> File \"/home/sasha/.local/lib/python3.7/site-packages/nlp/arrow_dataset.py\", line 290, in _getitem\r\n>     raise ValueError(f\"Index ({key}) outside of table length ({self._data.num_rows}).\")"
    },
    {
      "id": "87",
      "title": "Error in Demo for Specific Datasets",
      "body": "Selecting `natural_questions` or `newsroom` dataset in the online demo results in an error similar to the following.\r\n\r\n![image](https://user-images.githubusercontent.com/60150701/85347842-ac861900-b4ae-11ea-98c4-a53a00934783.png)\r\n"
    },
    {
      "id": "88",
      "title": "Qa4mre - add dataset",
      "body": "Added dummy data test only for the first config. Will do the rest later.\r\nI had to do add some minor hacks to an important function to make it work. \r\nThere might be a cleaner way to handle it - can you take a look @thomwolf ?"
    },
    {
      "id": "89",
      "title": "Add loading from the Datasets Hub + add relative paths in download manager",
      "body": "With the new Datasets Hub on huggingface.co it's now possible to have a dataset repo with your own script and data.\r\nFor example: https://huggingface.co/datasets/lhoestq/custom_squad/tree/main contains one script and two json files.\r\n\r\nYou can load it using\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nd = load_dataset(\"lhoestq/custom_squad\")\r\n```\r\n\r\nTo be able to use the data files that live right next to the dataset script on the repo in the hub, I added relative paths support for the DownloadManager. For example in the repo mentioned above, there are two json files that can be downloaded via\r\n```python\r\n_URLS = {\r\n    \"train\": \"train-v1.1.json\",\r\n    \"dev\": \"dev-v1.1.json\",\r\n}\r\ndownloaded_files = dl_manager.download_and_extract(_URLS)\r\n```\r\n\r\nTo make it work, I set the `base_path` of the DownloadManager to be the parent path of the dataset script (which comes from either a local path or a remote url).\r\n\r\nI also had to add the auth header of the requests to huggingface.co for priv"
    },
    {
      "id": "90",
      "title": "adding hind_encorp dataset",
      "body": "adding Hindi_Encorp05 dataset"
    },
    {
      "id": "91",
      "title": "GPT2 MNLI training using run_glue.py",
      "body": "Edit: I'm closing this because I actually meant to post this in `transformers `not `datasets`\r\n\r\nRunning this on Google Colab,\r\n\r\n```\r\n!python run_glue.py \\\r\n  --model_name_or_path gpt2 \\\r\n  --task_name mnli \\\r\n  --do_train \\\r\n  --do_eval \\\r\n  --max_seq_length 128 \\\r\n  --per_gpu_train_batch_size 10 \\\r\n  --gradient_accumulation_steps 32\\\r\n  --learning_rate 2e-5 \\\r\n  --num_train_epochs 3.0 \\\r\n  --output_dir models/gpt2/mnli/\r\n```\r\n\r\nI get the following error,\r\n\r\n```\r\n \"Asking to pad but the tokenizer does not have a padding token. \"\r\nValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\r\n```\r\n\r\nDo I need to modify the trainer to work with GPT2 ?"
    },
    {
      "id": "92",
      "title": "dataloading slow when using HUGE dataset",
      "body": "Hi,\r\n\r\nWhen I use datasets with 600GB data, the dataloading speed increases significantly. \r\nI am experimenting with two datasets, and one is about 60GB and the other 600GB.\r\nSimply speaking, my code uses `datasets.set_format(\"torch\")` function and let pytorch-lightning handle ddp training.\r\nWhen looking at the pytorch-lightning supported profile of two different runs, I see that fetching a batch(`get_train_batch`) consumes an unreasonable amount of time when data is large. What could be the cause?\r\n\r\n* 60GB data\r\n```\r\nAction                             \t|  Mean duration (s)\t|Num calls      \t|  Total time (s) \t|  Percentage %   \t|\r\n------------------------------------------------------------------------------------------------------------------------------------\r\nTotal                              \t|  -              \t|_              \t|  200.33         \t|  100 %          \t|\r\n-----------------------------------------------------------------------------------------------------------------"
    },
    {
      "id": "93",
      "title": "add W&I + LOCNESS dataset (BEA-2019 workshop shared task on GEC) [PROPER]",
      "body": "- **Name:** W&I + LOCNESS dataset (from the BEA-2019 workshop shared task on GEC)\r\n- **Description:** https://www.cl.cam.ac.uk/research/nl/bea2019st/#data\r\n- **Paper:** https://www.aclweb.org/anthology/W19-4406/\r\n- **Motivation:** This is a recent dataset (actually two in one) for grammatical error correction and is used for benchmarking in this field of NLP.\r\n\r\n### Checkbox\r\n\r\n- [x] Create the dataset script `/datasets/my_dataset/my_dataset.py` using the template\r\n- [x] Fill the `_DESCRIPTION` and `_CITATION` variables\r\n- [x] Implement `_infos()`, `_split_generators()` and `_generate_examples()`\r\n- [x] Make sure that the `BUILDER_CONFIGS` class attribute is filled with the different configurations of the dataset and that the `BUILDER_CONFIG_CLASS` is specified if there is a custom config class.\r\n- [x] Generate the metadata file `dataset_infos.json` for all configurations\r\n- [x] Generate the dummy data `dummy_data.zip` files to have the dataset script tested and that they don't weigh t"
    },
    {
      "id": "94",
      "title": "Add CaSiNo dataset",
      "body": "Hi. I request you to add our dataset to the repository. \r\n\r\nThis data was recently published at NAACL 2021: https://aclanthology.org/2021.naacl-main.254.pdf"
    },
    {
      "id": "95",
      "title": "Added dataset clickbait_news_bg",
      "body": ""
    },
    {
      "id": "96",
      "title": "Add ASR task for SUPERB",
      "body": "This PR starts building up the SUPERB benchmark by including the ASR task as described in the [SUPERB paper](https://arxiv.org/abs/2105.01051) and `s3prl` [instructions](https://github.com/s3prl/s3prl/tree/v0.2.0/downstream#asr-automatic-speech-recognition).\r\n\r\nUsage:\r\n\r\n```python\r\nfrom datasets import load_dataset \r\n\r\nasr = load_dataset(\"superb\", \"asr\")\r\n# DatasetDict({\r\n#     train: Dataset({\r\n#         features: ['file', 'text', 'speaker_id', 'chapter_id', 'id'],\r\n#         num_rows: 28539\r\n#     })\r\n#     validation: Dataset({\r\n#         features: ['file', 'text', 'speaker_id', 'chapter_id', 'id'],\r\n#         num_rows: 2703\r\n#     })\r\n#     test: Dataset({\r\n#         features: ['file', 'text', 'speaker_id', 'chapter_id', 'id'],\r\n#         num_rows: 2620\r\n#     })\r\n# })\r\n```\r\n\r\nI've used the GLUE benchmark as a guide for filling out the README.\r\n\r\nTo move fast during the evaluation PoC I propose to merge one task at a time, so we can continue building the training / evaluation frame"
    },
    {
      "id": "97",
      "title": "Unwanted progress bars when accessing examples",
      "body": "When accessing examples from a dataset formatted for pytorch, some progress bars appear when accessing examples:\r\n```python\r\nIn [1]: import datasets as ds                                        \r\n\r\nIn [2]: d = ds.Dataset.from_dict({\"a\": [0, 1, 2]}).with_format(\"torch\")                                                           \r\n\r\nIn [3]: d[0]                                                         \r\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 3172.70it/s]\r\nOut[3]: {'a': tensor(0)}\r\n```\r\n\r\nThis is because the pytorch formatter calls `map_nested` that uses progress bars\r\n\r\ncc @sgugger "
    },
    {
      "id": "98",
      "title": "Fix: Wikipedia - save memory by replacing root.clear with elem.clear",
      "body": "see: https://github.com/huggingface/datasets/issues/2031\r\n\r\nWhat I did:\r\n- replace root.clear with elem.clear\r\n- remove lines to get root element\r\n- $ make style\r\n- $ make test\r\n  - some tests required some pip packages, I installed them.\r\n\r\ntest results on origin/master and my branch are same. I think it's not related on my modification, isn't it?\r\n```\r\n==================================================================================== short test summary info ====================================================================================\r\nFAILED tests/test_arrow_writer.py::TypedSequenceTest::test_catch_overflow - AssertionError: OverflowError not raised\r\n============================================================= 1 failed, 2332 passed, 5138 skipped, 70 warnings in 91.75s (0:01:31) ==============================================================\r\nmake: *** [Makefile:19: test] Error 1\r\n\r\n```\r\n\r\nIs there anything else I should do?"
    },
    {
      "id": "99",
      "title": "Add skip and take",
      "body": "As discussed in https://github.com/huggingface/datasets/pull/2375#discussion_r657084544 I added the `IterableDataset.skip` and `IterableDataset.take` methods that allows to do basic splitting of iterable datasets.\r\n\r\nYou can create new dataset with the first `n` examples using `IterableDataset.take()`, or you can get a dataset with the rest of the examples by skipping the first `n` examples with `IterableDataset.skip()`\r\n\r\nOne implementation detail:\r\n\r\nUsing `take` (or `skip`) prevents future dataset shuffling from shuffling the dataset shards, otherwise the taken examples could come from other shards. In this case it only uses the shuffle buffer.\r\nI would have loved to allow the shards of the taken examples to be shuffled anyway, but since we don't know in advance the length of each shard we don't know what shards to take or skip.\r\nI think this is ok though since users can shuffle before doing take or skip. I mentioned this in the documentation\r\n\r\ncc @vblagoje @lewtun "
    },
    {
      "id": "100",
      "title": "tmp_file referenced before assignment",
      "body": "Just learning about this library - so might've not set up all the flags correctly, but was getting this error about \"tmp_file\"."
    },
    {
      "id": "101",
      "title": "[Convert] add new pattern",
      "body": ""
    },
    {
      "id": "102",
      "title": "Fix typo in huggingface hub",
      "body": "pip knows how to resolve to `huggingface_hub`, but conda doesn't!\r\n\r\nThe `packaging` dependency is also required for the build to complete."
    },
    {
      "id": "103",
      "title": "Add QED Amara Dataset",
      "body": ""
    },
    {
      "id": "104",
      "title": "Wino bias",
      "body": "The PR will fail circleCi tests because of the requirement of manual loading of data. Fresh PR because of messed up history of the previous one. "
    },
    {
      "id": "105",
      "title": "Added conv ai 2 (Again)",
      "body": "The original PR -> https://github.com/huggingface/datasets/pull/1383\r\n\r\nReason for creating again - \r\n\r\nThe reason I had to create the PR again was due to the master rebasing issue. After rebasing the changes, all the previous commits got added to the branch. "
    },
    {
      "id": "106",
      "title": "Updated TTC4900 Dataset",
      "body": "- The source address of the TTC4900 dataset of [@savasy](https://github.com/savasy) has been updated for direct download.\r\n- Updated readme."
    },
    {
      "id": "107",
      "title": "fix anli splits",
      "body": "I can't run the tests for dummy data, facing this error \r\n\r\n`ImportError while loading conftest '/home/zaid/tmp/fix_anli_splits/datasets/tests/conftest.py'.\r\ntests/conftest.py:10: in <module>\r\n    from datasets import config\r\nE   ImportError: cannot import name 'config' from 'datasets' (unknown location)`"
    },
    {
      "id": "108",
      "title": "Add OpenSLR dataset",
      "body": "OpenSLR (https://openslr.org/) is a site devoted to hosting speech and language resources, such as training corpora for speech recognition, and software related to speech recognition. There are around 80 speech datasets listed in OpenSLR, currently this PR includes only 9 speech datasets SLR41, SLR42, SLR43, SLR44, SLR63, SLR64, SLR65, SLR66 and SLR69 (Javanese, Khmer, Nepali and Sundanese, Malayalam, Marathi, Tamil, Telugu and Catalan). I can add other speech datasets gradually next time."
    },
    {
      "id": "109",
      "title": "lm1b",
      "body": "Add lm1b dataset."
    },
    {
      "id": "110",
      "title": "Added kannada news headlines classification dataset. ",
      "body": "Manual Download of a kaggle dataset. Mostly followed process as ms_terms."
    },
    {
      "id": "111",
      "title": "add metrec: arabic poetry dataset",
      "body": ""
    },
    {
      "id": "112",
      "title": "[dateset subset missing]  xtreme paws-x",
      "body": "I tried nlp.load_dataset('xtreme', 'PAWS-X.es') but get the value error\r\nIt turns out that the subset for Spanish is missing\r\nhttps://github.com/google-research-datasets/paws/tree/master/pawsx"
    },
    {
      "id": "113",
      "title": "ru_reviews dataset adding",
      "body": "RuReviews: An Automatically Annotated Sentiment Analysis Dataset for Product Reviews in Russian"
    },
    {
      "id": "114",
      "title": "Is there support for Deep learning datasets?",
      "body": "I looked around this repository and looking the datasets I think that there's no support for images-datasets. Or am I missing something? For example to add a repo like this https://github.com/DZPeru/fish-datasets"
    },
    {
      "id": "115",
      "title": "train_test_split returns empty dataset item",
      "body": "I try to split my dataset by `train_test_split`, but after that the item in `train` and `test` `Dataset` is empty.\r\nThe codes:\r\n```\r\nyelp_data = datasets.load_from_disk('/home/ssd4/huanglianzhe/test_yelp')\r\n    print(yelp_data[0])\r\n    yelp_data = yelp_data.train_test_split(test_size=0.1)\r\n    print(yelp_data)\r\n    print(yelp_data['test'])\r\n    print(yelp_data['test'][0])\r\n```\r\nThe outputs:\r\n```\r\n{'stars': 2.0, 'text': 'xxxx'}\r\nLoading cached split indices for dataset at /home/ssd4/huanglianzhe/test_yelp/cache-f9b22d8b9d5a7346.arrow and /home/ssd4/huanglianzhe/test_yelp/cache-4aa26fa4005059d1.arrow\r\nDatasetDict({'train': Dataset(features: {'stars': Value(dtype='float64', id=None), 'text': Value(dtype='string', id=None)}, num_rows: 7219009), 'test': Dataset(features: {'stars': Value(dtype='float64', id=None), 'text': Value(dtype='string', id=None)}, num_rows: 802113)})\r\nDataset(features: {'stars': Value(dtype='float64', id=None), 'text': Value(dtype='string', id=None)}, num_rows: 802113"
    },
    {
      "id": "116",
      "title": "BLUE file not found",
      "body": "Hi, I'm having the following issue when I try to load the `blue` metric.\r\n\r\n```shell\r\nimport datasets\r\nmetric = datasets.load_metric('blue')\r\nTraceback (most recent call last):\r\n  File \"/home/irfan/environments/Perplexity_Transformers/lib/python3.6/site-packages/datasets/load.py\", line 320, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File \"/home/irfan/environments/Perplexity_Transformers/lib/python3.6/site-packages/datasets/utils/file_utils.py\", line 291, in cached_path\r\n    use_auth_token=download_config.use_auth_token,\r\n  File \"/home/irfan/environments/Perplexity_Transformers/lib/python3.6/site-packages/datasets/utils/file_utils.py\", line 621, in get_from_cache\r\n    raise FileNotFoundError(\"Couldn't find file at {}\".format(url))\r\nFileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.7.0/metrics/blue/blue.py\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (mos"
    },
    {
      "id": "117",
      "title": "Improve ReadInstruction logic and update docs",
      "body": "Improve ReadInstruction logic and docs."
    },
    {
      "id": "118",
      "title": "Add reasoning_bg",
      "body": "Adding reading comprehension dataset for Bulgarian language"
    },
    {
      "id": "119",
      "title": "Fix potential DuplicatedKeysError",
      "body": "Fix potential DiplicatedKeysError by ensuring keys are unique.\r\n\r\nWe should promote as a good practice, that the keys should be programmatically generated as unique, instead of read from data (which might be not unique)."
    },
    {
      "id": "120",
      "title": "Updating citation information on LinCE readme",
      "body": "Hi!\r\n\r\nI just updated the citation information in this PR. It had an additional bibtex from one of the datasets used in LinCE and then the LinCE bibtex. I removed the former and added a link that shows the full list of citations for each dataset. \r\n\r\nThanks!"
    },
    {
      "id": "121",
      "title": "Fix `xnli` dataset tuple key",
      "body": "Closes #2229 \r\nThe `xnli` dataset yields a tuple key in case of `ar` which is inconsistant with the acceptable key types (str/int).\r\nThe key was thus ported to `str` keeping the original information intact."
    },
    {
      "id": "122",
      "title": "Narrativeqa (with full text)",
      "body": "Following the uploading of the full text data in #309, I've added the full text to the narrativeqa dataset.\r\n\r\nFew notes:\r\n- Had some encoding issues using the default `open` so am using `open(encoding=\"latin-1\"...` which seems to fix it. Looks fine.\r\n-  Can't get the dummy data to work. Currently putting stuff at: \r\n    ```\r\n    dummy\r\n    |---- 0.0.0\r\n            |- dummy_data.zip\r\n                |-master.zip\r\n                |   |- narrativeqa-master\r\n                |       |- documents.csv\r\n                |       |- qaps.csv\r\n                |       |- third_party ......\r\n                | \r\n                | - narrativeqa_full_text.zip\r\n                |       | - 001.content\r\n                |       | - ....\r\n    ```\r\n    Not sure what I'm messing up here (probably something obvious)."
    },
    {
      "id": "123",
      "title": "adding eitb_parcc",
      "body": "Adding EiTB-ParCC: Parallel Corpus of Comparable News\r\nhttp://opus.nlpl.eu/EiTB-ParCC.php"
    },
    {
      "id": "124",
      "title": "Jsonlines export error",
      "body": "## Describe the bug\r\nWhen exporting large datasets in jsonlines (c4 in my case) the created file has an error every 9999 lines: the 9999th and 10000th are concatenated, thus breaking the jsonlines format. This sounds like it is related to batching, which is by 10000 by default\r\n\r\n## Steps to reproduce the bug\r\nThis what I'm running:\r\n\r\nin python:\r\n\r\n```\r\nfrom datasets import load_dataset\r\nptb = load_dataset(\"ptb_text_only\")\r\nptb[\"train\"].to_json(\"ptb.jsonl\")\r\n```\r\n\r\nthen out of python:\r\n\r\n```\r\nhead -10000 ptb.jsonl\r\n```\r\n\r\n## Expected results\r\nProperly separated lines\r\n\r\n## Actual results\r\nThe last line is a concatenation of two lines\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n\r\n- `datasets` version: 1.9.1.dev0\r\n- Platform: Linux-5.4.0-1046-gcp-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.6.9\r\n- PyArrow version: 4.0.1"
    },
    {
      "id": "125",
      "title": "Improve docs on Enhancing performance",
      "body": "In the [\"Enhancing performance\"](https://huggingface.co/docs/datasets/loading_datasets.html#enhancing-performance) section of docs, add specific use cases:\r\n- How to make datasets the fastest\r\n- How to make datasets take the less RAM\r\n- How to make datasets take the less hard drive mem\r\n\r\ncc: @thomwolf \r\n"
    },
    {
      "id": "126",
      "title": "0x290B112ED1280537B24Ee6C268a004994a16e6CE",
      "body": "## Adding a Dataset\n- **Name:** *name of the dataset*\n- **Description:** *short description of the dataset (or link to social media or blog post)*\n- **Paper:** *link to the dataset paper if available*\n- **Data:** *link to the Github repository or current dataset location*\n- **Motivation:** *what are some good reasons to have this dataset*\n\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md)."
    },
    {
      "id": "127",
      "title": "Add HoVer Dataset",
      "body": "HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification\r\nhttps://arxiv.org/abs/2011.03088 "
    },
    {
      "id": "128",
      "title": "Overview.ipynb throws exceptions with nlp 0.4.0",
      "body": "with nlp 0.4.0, the TensorFlow example in Overview.ipynb throws the following exceptions:\r\n\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-5-48907f2ad433> in <module>\r\n----> 1 features = {x: train_tf_dataset[x].to_tensor(default_value=0, shape=[None, tokenizer.max_len]) for x in columns[:3]}\r\n      2 labels = {\"output_1\": train_tf_dataset[\"start_positions\"].to_tensor(default_value=0, shape=[None, 1])}\r\n      3 labels[\"output_2\"] = train_tf_dataset[\"end_positions\"].to_tensor(default_value=0, shape=[None, 1])\r\n      4 tfdataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(8)\r\n\r\n<ipython-input-5-48907f2ad433> in <dictcomp>(.0)\r\n----> 1 features = {x: train_tf_dataset[x].to_tensor(default_value=0, shape=[None, tokenizer.max_len]) for x in columns[:3]}\r\n      2 labels = {\"output_1\": train_tf_dataset[\"start_positions\"].to_tensor(default_value=0, shape"
    },
    {
      "id": "129",
      "title": "How to choose proper download_mode in function load_dataset?",
      "body": "Hi, I am a beginner to datasets and I try to use datasets to load my csv file.\r\nmy csv file looks like this\r\n\r\n``` \r\ntext,label\r\n\"Effective but too-tepid biopic\",3\r\n\"If you sometimes like to go to the movies to have fun , Wasabi is a good place to start .\",4\r\n\"Emerges as something rare , an issue movie that 's so honest and keenly observed that it does n't feel like one .\",5\r\n```\r\n\r\nFirst I try to use this command to load my csv file .  \r\n\r\n``` python\r\ndataset=load_dataset('csv', data_files=['sst_test.csv'])\r\n```\r\n\r\nIt seems good, but when i try to overwrite the convert_options to convert  'label' columns from int64 to float32 like this.\r\n\r\n``` python\r\nimport pyarrow as pa\r\nfrom pyarrow import csv\r\nread_options = csv.ReadOptions(block_size=1024*1024)\r\nparse_options = csv.ParseOptions()\r\nconvert_options = csv.ConvertOptions(column_types={'text': pa.string(), 'label': pa.float32()})\r\ndataset = load_dataset('csv', data_files=['sst_test.csv'], read_options=read_options,\r\n                  "
    },
    {
      "id": "130",
      "title": "Add Indosum dataset",
      "body": ""
    },
    {
      "id": "131",
      "title": "Add data dir test command",
      "body": ""
    },
    {
      "id": "132",
      "title": "Add CAIL 2018 dataset",
      "body": ""
    },
    {
      "id": "133",
      "title": "Add Naver sentiment movie corpus",
      "body": "Supersedes #1168 \r\n\r\n> This PR adds the [Naver sentiment movie corpus](https://github.com/e9t/nsmc), a dataset containing Korean movie reviews from Naver, the most commonly used search engine in Korea. This dataset is often used to benchmark models on Korean NLP tasks, as seen in [this paper](https://www.aclweb.org/anthology/2020.lrec-1.199.pdf). "
    },
    {
      "id": "134",
      "title": "datasets.config.PYARROW_VERSION has no attribute 'major'",
      "body": "In the test_dataset_common.py script, line 288-289\r\n\r\n```\r\nif datasets.config.PYARROW_VERSION.major < 3:\r\n   packaged_datasets = [pd for pd in packaged_datasets if pd[\"dataset_name\"] != \"parquet\"]\r\n```\r\n\r\nwhich throws the error below. `datasets.config.PYARROW_VERSION` itself return the string '4.0.1'. I have tested this on both datasets.__version_=='1.11.0' and '1.9.0'. I am using Mac OS.\r\n\r\n```\r\nimport datasets\r\ndatasets.config.PYARROW_VERSION.major\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/var/folders/1f/0wqmlgp90qjd5mpj53fnjq440000gn/T/ipykernel_73361/2547517336.py in <module>\r\n      1 import datasets\r\n----> 2 datasets.config.PYARROW_VERSION.major\r\n\r\nAttributeError: 'str' object has no attribute 'major'\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.11.0\r\n- Platform: Darwin-20.6.0-x86_64-i386-64bit\r\n- Python version: 3.7.11\r\n- PyArrow version: 4.0.1\r\n"
    },
    {
      "id": "135",
      "title": "ArrowTypeError in squad metrics",
      "body": "`squad_metric.compute` is giving following error\r\n```\r\nArrowTypeError: Could not convert [{'text': 'Denver Broncos'}, {'text': 'Denver Broncos'}, {'text': 'Denver Broncos'}] with type list: was not a dict, tuple, or recognized null value for conversion to struct type\r\n```\r\n\r\nThis is how my predictions and references look like\r\n```\r\npredictions[0]\r\n# {'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}\r\n```\r\n\r\n```\r\nreferences[0]\r\n# {'answers': [{'text': 'Denver Broncos'},\r\n  {'text': 'Denver Broncos'},\r\n  {'text': 'Denver Broncos'}],\r\n 'id': '56be4db0acb8001400a502ec'}\r\n```\r\n\r\nThese are structured as per the `squad_metric.compute` help string."
    },
    {
      "id": "136",
      "title": "Fix extracted files directory for the DownloadManager",
      "body": "The cache dir was often cluttered by extracted files because of the download manager.\r\n\r\nFor downloaded files, we are using the `downloads` directory to make things easier to navigate, but extracted files were still placed at the root of the cache directory. To fix that I changed the directory for extracted files to cache_dir/downloads/extracted."
    },
    {
      "id": "137",
      "title": "Add narrativeQA",
      "body": "Redo of #1368 #309 #499\r\n\r\nIn redoing the dummy data a few times, I ended up adding a load of files to git. Hopefully this should work."
    },
    {
      "id": "138",
      "title": "py3.7: TypeError: can't pickle _LazyModule objects",
      "body": "While this works fine with py3.8, under py3.7, with a totally new conda env and transformers install:\r\n\r\n```\r\ngit clone https://github.com/huggingface/transformers\r\ncd transformers\r\npip install -e .[testing]\r\n\r\nexport BS=1; rm -rf /tmp/test-clm; PYTHONPATH=src USE_TF=0 CUDA_VISIBLE_DEVICES=0 python \\\r\nexamples/language-modeling/run_clm.py --model_name_or_path distilgpt2 --dataset_name wikitext \\\r\n--dataset_config_name wikitext-2-raw-v1 --do_train --max_train_samples 1 \\\r\n--per_device_train_batch_size $BS --output_dir /tmp/test-clm --block_size 128 --logging_steps 1  \\\r\n--fp16\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"examples/language-modeling/run_clm.py\", line 453, in <module>\r\n    main()\r\n  File \"examples/language-modeling/run_clm.py\", line 336, in main\r\n    load_from_cache_file=not data_args.overwrite_cache,\r\n  File \"/home/stas/anaconda3/lib/python3.7/site-packages/datasets/dataset_dict.py\", line 303, in map\r\n    for k, dataset in self.items()\r\n  File \"/home/stas/anac"
    },
    {
      "id": "139",
      "title": "Add Knowledge-Enhanced Language Model Pre-training (KELM)",
      "body": "Adds the KELM dataset.\r\n\r\n- Webpage/repo: https://github.com/google-research-datasets/KELM-corpus\r\n- Paper: https://arxiv.org/pdf/2010.12688.pdf"
    },
    {
      "id": "140",
      "title": "dtype of tensors should be preserved",
      "body": "After switching to `datasets` my model just broke. After a weekend of debugging, the issue was that my model could not handle the double that the Dataset provided, as it expected a float (but didn't give a warning, which seems a [PyTorch issue](https://discuss.pytorch.org/t/is-it-required-that-input-and-hidden-for-gru-have-the-same-dtype-float32/96221)). \r\n\r\nAs a user I did not expect this bug. I have a `map` function that I call on the Dataset that looks like this:\r\n\r\n```python\r\ndef preprocess(sentences: List[str]):\r\n    token_ids = [[vocab.to_index(t) for t in s.split()] for s in sentences]\r\n\r\n    sembeddings = stransformer.encode(sentences)\r\n    print(sembeddings.dtype)\r\n    return {\"input_ids\": token_ids, \"sembedding\": sembeddings}\r\n```\r\n\r\nGiven a list of `sentences` (`List[str]`), it converts those into token_ids on the one hand (list of lists of ints; `List[List[int]]`) and into sentence embeddings on the other (Tensor of dtype `torch.float32`). That means that I actually set the"
    },
    {
      "id": "141",
      "title": "Updating the DART file checksums in GEM",
      "body": "The DART files were just updated on the source GitHub\r\n\r\nhttps://github.com/Yale-LILY/dart/commit/34b3c872da4811523e334f1631e54ca8105dffab"
    },
    {
      "id": "142",
      "title": "Cannot upload my own dataset",
      "body": "I look into `nlp-cli` and `user.py` to learn how to upload my own data.\r\n\r\nIt is supposed to work like this\r\n- Register to get username, password at huggingface.co\r\n- `nlp-cli login` and type username, passworld\r\n- I have a single file to upload at `./ttc/ttc_freq_extra.csv`\r\n- `nlp-cli upload ttc/ttc_freq_extra.csv`\r\n\r\nBut I got this error.\r\n\r\n```\r\n2020-05-21 16:33:52.722464: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\nAbout to upload file /content/ttc/ttc_freq_extra.csv to S3 under filename ttc/ttc_freq_extra.csv and namespace korakot\r\nProceed? [Y/n] y\r\nUploading... This might take a while if files are large\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/nlp-cli\", line 33, in <module>\r\n    service.run()\r\n  File \"/usr/local/lib/python3.6/dist-packages/nlp/commands/user.py\", line 234, in run\r\n    token=token, filename=filename, filepath=filepath, organization=self.args.organization\r\n  File \"/usr/loc"
    },
    {
      "id": "143",
      "title": "Can datasets remove duplicated rows?",
      "body": "**Is your feature request related to a problem? Please describe.**\r\ni find myself more and more relying on datasets just to do all the preprocessing. One thing however, for removing duplicated rows, I couldn't find out how and am always converting datasets to pandas to do that..\r\n\r\n\r\n**Describe the solution you'd like**\r\nhave a functionality of \" remove duplicated rows\"\r\n\r\n**Describe alternatives you've considered**\r\nconvert dataset to pandas, remove duplicate, and convert back...\r\n\r\n\r\n**Additional context**\r\nno"
    },
    {
      "id": "144",
      "title": "[XGLUE] Adding new dataset",
      "body": "XGLUE is a multilingual GLUE like dataset propesed in this [paper](https://arxiv.org/pdf/2004.01401.pdf).\r\n\r\nI'm planning on adding the dataset to the library myself in a couple of weeks.\r\nAlso tagging @JetRunner @qiweizhen in case I need some guidance "
    },
    {
      "id": "145",
      "title": "Make shuffle compatible with temp_seed",
      "body": "This code used to return different dataset at each run\r\n```python\r\nimport dataset as ds\r\n\r\ndataset = ...\r\n\r\nwith ds.temp_seed(42):\r\n    shuffled = dataset.shuffle()\r\n```\r\n\r\nNow it returns the same one since the seed is set"
    },
    {
      "id": "146",
      "title": "Add SICK dataset",
      "body": "Adds the SICK dataset (http://marcobaroni.org/composes/sick.html).\r\n\r\nCloses #1772.\r\n\r\nEdit: also closes #1632, which is the original issue requesting the dataset. The newer one is a duplicate."
    },
    {
      "id": "147",
      "title": "Have Trouble importing `datasets`",
      "body": "I'm failing to import transformers (v4.0.0-dev), and tracing the cause seems to be failing to import datasets.\r\n\r\nI cloned the newest version of datasets (master branch), and do `pip install -e .`.\r\n\r\nThen, `import datasets` causes the error below.\r\n\r\n```\r\n~/workspace/Clone/datasets/src/datasets/utils/file_utils.py in <module>\r\n    116 sys.path.append(str(HF_MODULES_CACHE))\r\n    117 \r\n--> 118 os.makedirs(HF_MODULES_CACHE, exist_ok=True)\r\n    119 if not os.path.exists(os.path.join(HF_MODULES_CACHE, \"__init__.py\")):\r\n    120     with open(os.path.join(HF_MODULES_CACHE, \"__init__.py\"), \"w\"):\r\n\r\n~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/os.py in makedirs(name, mode, exist_ok)\r\n    221             return\r\n    222     try:\r\n--> 223         mkdir(name, mode)\r\n    224     except OSError:\r\n    225         # Cannot rely on checking for EEXIST, since the operating system \r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: '<MY_HOME_DIRECTORY>/.cache/huggingface/modules'\r\n```\r\n\r\nTh"
    },
    {
      "id": "148",
      "title": "Add ConLL-2000 dataset",
      "body": "Adds ConLL-2000 dataset used for text chunking. See https://www.clips.uantwerpen.be/conll2000/chunking/ for details and [motivation](https://github.com/huggingface/transformers/pull/7041#issuecomment-692710948) behind this PR"
    },
    {
      "id": "149",
      "title": "Adding PolEval2019 Machine Translation Task dataset",
      "body": "Facing an error with pytest in training. Dummy data is passing.\r\nREADME has to be updated."
    },
    {
      "id": "150",
      "title": "Re-adding narrativeqa dataset",
      "body": "An update of #309. "
    },
    {
      "id": "151",
      "title": "adding dataset for diplomacy detection-2",
      "body": ""
    },
    {
      "id": "152",
      "title": "Why is dataset after tokenization far more larger than the orginal one ?",
      "body": "I tokenize wiki dataset by `map` and cache the results.\r\n```\r\ndef tokenize_tfm(example):\r\n    example['input_ids'] = hf_fast_tokenizer.convert_tokens_to_ids(hf_fast_tokenizer.tokenize(example['text']))\r\n    return example\r\nwiki = nlp.load_dataset('wikipedia', '20200501.en', cache_dir=cache_dir)['train']\r\nwiki.map(tokenize_tfm, cache_file_name=cache_dir/\"wikipedia/20200501.en/1.0.0/tokenized_wiki.arrow\")\r\n```\r\nand when I see their size\r\n```\r\nls -l --block-size=M\r\n17460M  wikipedia-train.arrow\r\n47511M  tokenized_wiki.arrow\r\n```\r\nThe tokenized one is over 2x size of original one.\r\nIs there something I did wrong ?"
    },
    {
      "id": "153",
      "title": "added TurkishMovieSentiment dataset",
      "body": "This PR adds the **TurkishMovieSentiment: This dataset contains turkish movie reviews.**\r\n\r\n- **Homepage:** [https://www.kaggle.com/mustfkeskin/turkish-movie-sentiment-analysis-dataset/tasks](https://www.kaggle.com/mustfkeskin/turkish-movie-sentiment-analysis-dataset/tasks)\r\n- **Point of Contact:** [Mustafa Keskin](https://www.linkedin.com/in/mustfkeskin/)"
    },
    {
      "id": "154",
      "title": "terminate called after throwing an instance of 'google::protobuf::FatalException'",
      "body": "Hi\r\nI am using the dataset \"iwslt2017-en-nl\", and after downloading it I am getting this error when trying to evaluate it on T5-base with seq2seq_trainer.py in the huggingface repo could you assist me please? thanks \r\n\r\n\r\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [02:47<00:00,  2.18s/it][libprotobuf FATAL /sentencepiece/src/../third_party/protobuf-lite/google/protobuf/repeated_field.h:1505] CHECK failed: (index) >= (0): \r\nterminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  CHECK failed: (index) >= (0): \r\nrun_t5_base_eval.sh: line 19:  5795 Aborted "
    },
    {
      "id": "155",
      "title": "Fix JSON tests.",
      "body": ""
    },
    {
      "id": "156",
      "title": "add more precise information for size",
      "body": "For the import into ELG, we would like a more precise description of the size of the dataset, instead of the current size categories. The size can be expressed in bytes, or any other preferred size unit. As suggested in the slack channel, perhaps this could be computed with a regex for existing datasets."
    },
    {
      "id": "157",
      "title": "Update XSUM Factuality DatasetCard",
      "body": "Update XSUM Factuality DatasetCard"
    },
    {
      "id": "158",
      "title": "`filelock.py` Error",
      "body": "## Describe the bug\r\n\r\nIt seems that the `filelock.py` went error. \r\n\r\n```\r\n>>> ds=load_dataset('xsum')\r\n\r\n^CTraceback (most recent call last):\r\n  File \"/user/HS502/yl02706/.conda/envs/lyc/lib/python3.6/site-packages/datasets/utils/filelock.py\", line 402, in _acquire\r\n    fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)\r\nOSError: [Errno 37] No locks available\r\n```\r\n\r\nAccording to error log, it is OSError, but there is an `except` in the `_acquire` function.\r\n\r\n```\r\n    def _acquire(self):\r\n        open_mode = os.O_WRONLY | os.O_CREAT | os.O_EXCL | os.O_TRUNC\r\n        try:\r\n            fd = os.open(self._lock_file, open_mode)\r\n        except (IOError, OSError):\r\n            pass\r\n        else:\r\n            self._lock_file_fd = fd\r\n        return None\r\n```\r\n\r\nI don't know why it stucked rather than `pass` directly.\r\n\r\nI am not quite familiar with filelock operation, so any help is highly appriciated.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n\r\nds = load_dataset('xsum')\r\n```\r\n\r\n## Expect"
    },
    {
      "id": "159",
      "title": "add tensorflow-macos support",
      "body": "ref - https://github.com/huggingface/datasets/issues/2068"
    },
    {
      "id": "160",
      "title": "Add Urdu fake news dataset",
      "body": "@lhoestq opened a clean PR containing only relevant files.\r\n\r\nold PR #1125 "
    },
    {
      "id": "161",
      "title": "Add CLINC150 Dataset",
      "body": "Added CLINC150 Dataset. The link to the dataset can be found [here](https://github.com/clinc/oos-eval) and the paper can be found [here](https://www.aclweb.org/anthology/D19-1131.pdf)\r\n\r\n- [x] Followed the instructions in CONTRIBUTING.md\r\n- [x] Ran the tests successfully\r\n- [x] Created the dummy data"
    },
    {
      "id": "162",
      "title": "New documentation structure",
      "body": "Organize Datasets documentation into four documentation types to improve clarity and discoverability of content.\r\n\r\n**Content to add in the very short term (feel free to add anything I'm missing):**\r\n- A discussion on why Datasets uses Arrow that includes some context and background about why we use Arrow. Would also be great to talk about Datasets speed and performance here, and if you can share any benchmarking/tests you did, that would be awesome! Finally, a discussion about how memory-mapping frees the user from RAM constraints would be very helpful.\r\n- Explain why you would want to disable or override verifications when loading a dataset.\r\n- If possible, include a code sample of when the number of elements in the field of an output dictionary arenâ€™t the same as the other fields in the output dictionary (taken from the [note](https://huggingface.co/docs/datasets/processing.html#augmenting-the-dataset) here)."
    },
    {
      "id": "163",
      "title": "Loading Data From S3 Path in Sagemaker",
      "body": "In Sagemaker Im tring to load the data set from S3 path as follows\r\n\r\n`train_path = 's3://xxxxxxxxxx/xxxxxxxxxx/train.csv'\r\n    valid_path = 's3://xxxxxxxxxx/xxxxxxxxxx/validation.csv'\r\n    test_path = 's3://xxxxxxxxxx/xxxxxxxxxx/test.csv'\r\n    \r\n    data_files = {}\r\n    data_files[\"train\"] = train_path\r\n    data_files[\"validation\"] = valid_path\r\n    data_files[\"test\"] = test_path\r\n    extension = train_path.split(\".\")[-1]\r\n    datasets = load_dataset(extension, data_files=data_files, s3_enabled=True)\r\n    print(datasets)`\r\n\r\n\r\nI getting an error of\r\n\r\n`algo-1-7plil_1  |   File \"main.py\", line 21, in <module>\r\nalgo-1-7plil_1  |     datasets = load_dataset(extension, data_files=data_files)\r\nalgo-1-7plil_1  |   File \"/opt/conda/lib/python3.6/site-packages/datasets/load.py\", line 603, in load_dataset\r\nalgo-1-7plil_1  |     **config_kwargs,\r\nalgo-1-7plil_1  |   File \"/opt/conda/lib/python3.6/site-packages/datasets/builder.py\", line 155, in __init__\r\nalgo-1-7plil_1  |     **config_kwargs,\r\n"
    },
    {
      "id": "164",
      "title": "fix dataset.map for function without outputs",
      "body": "As noticed in #505 , giving a function that doesn't return anything in `.map` raises an error because of an unreferenced variable.\r\nI fixed that and added tests.\r\n\r\nThanks @avloss for reporting"
    },
    {
      "id": "165",
      "title": "IWSLT-17 Link Broken",
      "body": "```\r\nFileNotFoundError: Couldn't find file at https://wit3.fbk.eu/archive/2017-01-trnmted//texts/DeEnItNlRo/DeEnItNlRo/DeEnItNlRo-DeEnItNlRo.tgz\r\n```"
    },
    {
      "id": "166",
      "title": "Update oscar sizes",
      "body": "This commit https://github.com/huggingface/datasets/commit/837a152e4724adc5308e2c4481908c00a8d93383 removed empty lines from the oscar deduplicated datasets. This PR updates the size of each deduplicated dataset to fix possible `NonMatchingSplitsSizesError` errors. cc @cahya-wirawan"
    },
    {
      "id": "167",
      "title": "More consistent naming",
      "body": "As per @stas00's suggestion in #2500, this PR inserts a space between the logo and the lib name (`ðŸ¤—Datasets` -> `ðŸ¤— Datasets`) for consistency with the Transformers lib. Additionally, more consistent names are used for Datasets Hub, etc."
    },
    {
      "id": "168",
      "title": "[Question & Bug Report] Can we preprocess a dataset on the fly?",
      "body": "I know we can use `Datasets.map` to preprocess a dataset, but I'm using it with very large corpus which generates huge cache file (several TB cache from a 400 GB text file). I have no disk large enough to save it.  Can we preprocess a dataset on the fly without generating cache?\r\n\r\nBTW, I tried raising `writer_batch_size`. Seems that argument doesn't have any effect when it's larger than `batch_size`, because you are saving all the batch instantly after it's processed. Please check the following code:\r\n\r\nhttps://github.com/huggingface/datasets/blob/0281f9d881f3a55c89aeaa642f1ba23444b64083/src/datasets/arrow_dataset.py#L1532"
    },
    {
      "id": "169",
      "title": "Fix ted_talks_iwslt version error",
      "body": "This PR fixes the bug where the version argument would be passed twice if the dataset configuration was created on the fly.\r\n\r\nFixes #2059 "
    },
    {
      "id": "170",
      "title": "Caching processed dataset at wrong folder",
      "body": "Hi guys, I run this on my Colab (PRO):\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('text', data_files='/content/corpus.txt', cache_dir='/content/drive/My Drive', split='train')\r\n\r\ndef encode(examples):\r\n  return tokenizer(examples['text'], truncation=True, padding='max_length')\r\n\r\ndataset = dataset.map(encode, batched=True)\r\n```\r\nThe file is about 4 GB, so I cannot process it on the Colab HD because there is no enough space. So I decided to mount my Google Drive fs and do it on it.\r\nThe dataset is cached in the right place but by processing it (applying `encode` function) seems to use a different folder because Colab HD starts to grow and it crashes when it should be done in the Drive fs.\r\n\r\nWhat gets me crazy, it prints it is processing/encoding the dataset in the right folder:\r\n```\r\nTesting the mapped function outputs\r\nTesting finished, running the mapping function on the dataset\r\nCaching processed dataset at /content/drive/My Drive/text/default-ad3e69d624"
    },
    {
      "id": "171",
      "title": "fix anli splits",
      "body": "I can't run the tests for dummy data, facing this error \r\n\r\n`ImportError while loading conftest '/home/zaid/tmp/fix_anli_splits/datasets/tests/conftest.py'.\r\ntests/conftest.py:10: in <module>\r\n    from datasets import config\r\nE   ImportError: cannot import name 'config' from 'datasets' (unknown location)`"
    },
    {
      "id": "172",
      "title": "strange datasets from OSCAR corpus",
      "body": "![image](https://user-images.githubusercontent.com/50871412/119260850-4f876b80-bc07-11eb-8894-124302600643.png)\r\n![image](https://user-images.githubusercontent.com/50871412/119260875-675eef80-bc07-11eb-9da4-ee27567054ac.png)\r\nFrom the [official site ](https://oscar-corpus.com/), the Yue Chinese dataset should have 2.2KB data.\r\n7 training instances is obviously not a right number.\r\nAs I can read Yue Chinese, I call tell the last instance is definitely not something that would appear on Common Crawl.\r\nAnd even if you don't read Yue Chinese, you can tell the first six instance are problematic.\r\n(It is embarrassing, as the 7 training instances look exactly like something from a pornographic novel or flitting messages in a chat of a dating app)\r\nIt might not be the problem of the huggingface/datasets implementation, because when I tried to download the dataset from the official site, I found out that the zip file is corrupted.\r\nI will try to inform the host of OSCAR corpus later.\r\nAwy a rem"
    },
    {
      "id": "173",
      "title": "add multi-proc in `to_csv`",
      "body": "This PR extends the multi-proc method used in #2747 for`to_json` to `to_csv` as well. \r\n\r\nResults on my machine post benchmarking on `ascent_kb` dataset (giving ~45% improvement when compared to num_proc = 1):\r\n```\r\nTime taken on 1 num_proc, 10000 batch_size  674.2055702209473\r\nTime taken on 4 num_proc, 10000 batch_size  425.6553490161896\r\n\r\nTime taken on 1 num_proc, 50000 batch_size  623.5897650718689\r\nTime taken on 4 num_proc, 50000 batch_size  380.0402421951294\r\n\r\nTime taken on 4 num_proc, 100000 batch_size  361.7168130874634\r\n```\r\nThis is a WIP as writing tests is pending for this PR. \r\n\r\nI'm also exploring [this](https://arrow.apache.org/docs/python/csv.html#incremental-writing) approach for which I'm using `pyarrow-5.0.0`.\r\n"
    },
    {
      "id": "174",
      "title": "Set to_json default to JSON lines",
      "body": "With this PR, the method `Dataset.to_json`:\r\n- is added to the docs\r\n- defaults to JSON lines"
    },
    {
      "id": "175",
      "title": "[Tests] Local => aws",
      "body": "## Change default Test from local => aws\r\n\r\nAs a default we set` aws=True`, `Local=False`, `slow=False`\r\n\r\n### 1. RUN_AWS=1 (default)\r\nThis runs 4 tests per dataset script.\r\n\r\na) Does the dataset script have a valid etag / Can it be reached on AWS? \r\nb) Can we load its `builder_class`?\r\nc) Can we load **all** dataset configs?\r\nd) _Most importantly_: Can we load the dataset? \r\n\r\nImportant - we currently only test the first config of each dataset to reduce test time. Total test time is around 1min20s.\r\n\r\n### 2. RUN_LOCAL=1 RUN_AWS=0\r\n\r\n***This should be done when debugging dataset scripts of the ./datasets folder***\r\n\r\nThis only runs 1 test per dataset test, which is equivalent to aws d) - Can we load the dataset from the local `datasets` directory?\r\n\r\n### 3. RUN_SLOW=1\r\n\r\nWe should set up to run these tests maybe 1 time per week ? @thomwolf \r\n\r\nThe `slow` tests include two more important tests. \r\n\r\ne) Can we load the dataset with all possible configs? This test will probably fail at the"
    },
    {
      "id": "176",
      "title": "Issues: Adding a FAISS or Elastic Search index to a Dataset",
      "body": "It seems the DPRContextEncoder, DPRContextEncoderTokenizer cited[ in this documentation](https://huggingface.co/nlp/faiss_and_ea.html) is not implemented ? It didnot work with the standard nlp installation . Also, I couldn't find or use it with the latest nlp install from github in Colab.  Is  there any dependency on the latest PyArrow 1.0.0 ? Is it yet to be made generally available ?"
    },
    {
      "id": "177",
      "title": "Speed up Tokenization by optimizing cast_to_python_objects",
      "body": "I changed how `cast_to_python_objects` works to make it faster.\r\nIt is used to cast numpy/pytorch/tensorflow/pandas objects to python lists, and it works recursively.\r\n\r\nTo avoid iterating over possibly long lists, it first checks if the first element that is not None has to be casted.\r\nIf the first element needs to be casted, then all the elements of the list will be casted, otherwise they'll stay the same.\r\nThis trick allows to cast objects that contain tokenizers outputs without iterating over every single token for example.\r\n\r\nSpeed improvement:\r\n\r\n\r\n```python\r\nimport transformers\r\nimport nlp\r\n\r\ntok = transformers.BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\r\ntxt = [\"a \" * 512] * 1000\r\ndataset = nlp.Dataset.from_dict({\"txt\": txt})\r\n\r\n# Tokenization using .map is now faster. Previously it was taking 3.5s\r\n%time _ = dataset.map(lambda x: tok(x[\"txt\"]), batched=True, load_from_cache_file=False)\r\n# 450ms\r\n\r\n# for comparison\r\n%time _ = tok(txt)\r\n# 280ms\r\n\r\n```"
    },
    {
      "id": "178",
      "title": "Enable logging propagation and remove logging handler",
      "body": "We used to have logging propagation disabled because of this issue: https://github.com/tensorflow/tensorflow/issues/26691\r\nBut since it's now fixed we should re-enable it. This is important to keep the default logging behavior for users, and propagation is also needed for pytest fixtures as asked in #1826 \r\n\r\nI also removed the handler that was added since, according to the logging [documentation](https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library):\r\n> It is strongly advised that you do not add any handlers other than NullHandler to your libraryâ€™s loggers. This is because the configuration of handlers is the prerogative of the application developer who uses your library. The application developer knows their target audience and what handlers are most appropriate for their application: if you add handlers â€˜under the hoodâ€™, you might well interfere with their ability to carry out unit tests and deliver logs which suit their requirements.\r\n\r\nIt could have been "
    },
    {
      "id": "179",
      "title": "Update README vallidation rules",
      "body": "This PR allows unexpected subsections under third-level headings. All except `Contributions`.\r\n\r\n@lhoestq "
    },
    {
      "id": "180",
      "title": "[Feature Request] Add the OpenWebText dataset",
      "body": "The OpenWebText dataset is an open clone of OpenAI's WebText dataset. It can be used to train ELECTRA as is specified in the [README](https://www.github.com/google-research/electra).\r\n\r\nMore information and the download link are available [here](https://skylion007.github.io/OpenWebTextCorpus/)."
    },
    {
      "id": "181",
      "title": "Adding farsi_news dataset (https://github.com/sci2lab/Farsi-datasets)",
      "body": ""
    },
    {
      "id": "182",
      "title": "Fix caching when moving script",
      "body": "When caching the result of a `map` function, the hash that is computed depends on many properties of this function, such as all the python objects it uses, its code and also the location of this code.\r\n\r\nUsing the full path of the python script for the location of the code makes the hash change if a script like `run_mlm.py` is moved.\r\n\r\nI changed this by simply using the base name of the script instead of the full path.\r\n\r\nNote that this change also affects the hash of the code used from imported modules, but I think it's fine. Indeed it hashes the code of the imported modules anyway, so the location of the python files of the imported modules doesn't matter when computing the hash.\r\n\r\nClose https://github.com/huggingface/datasets/issues/2825"
    },
    {
      "id": "183",
      "title": "small updates to the \"add new dataset\" guide",
      "body": "small updates (corrections/typos) to the \"add new dataset\" guide"
    },
    {
      "id": "184",
      "title": "Faster Shuffling?",
      "body": "Consider shuffling bookcorpus:\r\n\r\n```\r\ndataset = nlp.load_dataset('bookcorpus', split='train')\r\ndataset.shuffle()\r\n```\r\nAccording to tqdm, this will take around 2.5 hours on my machine to complete (even with the faster version of select from #405). I've also tried with `keep_in_memory=True` and `writer_batch_size=1000`.\r\n\r\nBut I can also just write the lines to a text file:\r\n\r\n```\r\nbatch_size = 100000\r\nwith open('tmp.txt', 'w+') as out_f:\r\n    for i in tqdm(range(0, len(dataset), batch_size)):\r\n        batch = dataset[i:i+batch_size]['text']\r\n        print(\"\\n\".join(batch), file=out_f)\r\n```\r\n\r\nWhich completes in a couple minutes, followed by `shuf tmp.txt > tmp2.txt` which completes in under a minute. And finally,\r\n\r\n```\r\ndataset = nlp.load_dataset('text', data_files='tmp2.txt')\r\n```\r\n\r\nWhich completes in under 10 minutes. I read up on Apache Arrow this morning, and it seems like the columnar data format is not especially well-suited to shuffling rows, since moving items around require"
    },
    {
      "id": "185",
      "title": "Added generated READMEs for datasets that were missing one.",
      "body": "This is it: we worked on a generator with Yacine @yjernite , and we generated dataset cards for all missing ones (161), with all the information we could gather from datasets repository, and using dummy_data to generate examples when possible.\r\n\r\nCode is available here for the moment: https://github.com/madlag/datasets_readme_generator .\r\nWe will move it to a Hugging Face repository and to https://huggingface.co/datasets/card-creator/ later.\r\n"
    },
    {
      "id": "186",
      "title": "DatasetDict save load Failing test in 1.6 not in 1.5",
      "body": "## Describe the bug\r\n\r\nWe have a test that saves a DatasetDict to disk and then loads it from disk. In 1.6 there is an incompatibility in the schema.\r\n\r\n\r\n\r\n\r\nDowngrading to `>1.6` -- fixes the problem.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n\r\n### Load a dataset dict from jsonl \r\n\r\npath = '/test/foo'\r\n\r\nds_dict.save_to_disk(path)\r\n\r\nds_from_disk = DatasetDict.load_from_disk(path).  ## <-- this is where I see the error on 1.6\r\n```\r\n\r\n## Expected results\r\n\r\nUpgrading to 1.6 shouldn't break that test. We should be able to serialize to and from disk.\r\n\r\n## Actual results\r\n```\r\n        # Infer features if None\r\n        inferred_features = Features.from_arrow_schema(arrow_table.schema)\r\n        if self.info.features is None:\r\n            self.info.features = inferred_features\r\n    \r\n        # Infer fingerprint if None\r\n    \r\n        if self._fingerprint is None:\r\n            self._fingerprint = generate_fingerprint(self)\r\n    \r\n        # Sanity checks\r\n    \r\n        assert self.feature"
    },
    {
      "id": "187",
      "title": "`metric.compute` throws `ArrowInvalid` error",
      "body": "I get the following error with `rouge.compute`. It happens only with distributed training, and it occurs randomly I can't easily reproduce it. This is using `nlp==0.4.0`\r\n\r\n```\r\n  File \"/home/beltagy/trainer.py\", line 92, in validation_step\r\n    rouge_scores = rouge.compute(predictions=generated_str, references=gold_str, rouge_types=['rouge2', 'rouge1', 'rougeL'])\r\n  File \"/home/beltagy/miniconda3/envs/allennlp/lib/python3.7/site-packages/nlp/metric.py\", line 224, in compute\r\n    self.finalize(timeout=timeout)\r\n  File \"/home/beltagy/miniconda3/envs/allennlp/lib/python3.7/site-packages/nlp/metric.py\", line 213, in finalize\r\n    self.data = Dataset(**reader.read_files(node_files))\r\n  File \"/home/beltagy/miniconda3/envs/allennlp/lib/python3.7/site-packages/nlp/arrow_reader.py\", line 217, in read_files\r\n    dataset_kwargs = self._read_files(files=files, info=self._info, original_instructions=original_instructions)\r\n  File \"/home/beltagy/miniconda3/envs/allennlp/lib/python3.7/site-packages/"
    },
    {
      "id": "188",
      "title": "Create README.md",
      "body": ""
    },
    {
      "id": "189",
      "title": "added health_fact dataset ",
      "body": "Added dataset Explainable Fact-Checking for Public Health Claims (dataset_id: health_fact)"
    },
    {
      "id": "190",
      "title": "Fix batched map for formatted dataset",
      "body": "If you had a dataset formatted as numpy for example, and tried to do a batched map, then it would crash because one of the elements from the inputs was missing for unchanged columns (ex: batch of length 999 instead of 1000).\r\nThe happened during the creation of the `pa.Table`, since columns had different lengths."
    },
    {
      "id": "191",
      "title": "Multidimensional arrays in a Dataset",
      "body": "Hi,\r\n\r\nI'm trying to put together a `datasets.Dataset` to be used with LayoutLM which is available in `transformers`. This model requires as input the bounding boxes of each of the token of a sequence. This is when I realized that `Dataset` does not support multi-dimensional arrays as a value for a column in a row.\r\n\r\nThe following code results in conversion error in pyarrow (`pyarrow.lib.ArrowInvalid: ('Can only convert 1-dimensional array values', 'Conversion failed for column bbox with type object')`)\r\n\r\n```\r\nfrom datasets import Dataset\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndataset = pd.DataFrame({\r\n    'bbox': [\r\n        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]]),\r\n        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]]),\r\n        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]]),\r\n        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]])\r\n    ],\r\n    'input_ids': [1, 2, 3, 4]\r\n})\r\ndataset = Dataset.from_pandas(dataset)\r\n```\r\n\r\nSince I wanted to use pytorch for the downstream training task, I also "
    },
    {
      "id": "192",
      "title": "adding hate-speech-and-offensive-language",
      "body": ""
    },
    {
      "id": "193",
      "title": "Fix numpy stacking",
      "body": "When getting items using a column name as a key, numpy arrays were not stacked.\r\nI fixed that and added some tests.\r\n\r\nThere is another issue that still needs to be fixed though: when getting items using a column name as a key, pytorch tensors are not stacked (it outputs a list of tensors). This PR should help with the to fix this issue."
    },
    {
      "id": "194",
      "title": "Add wiki_dpr",
      "body": "Presented in the [Dense Passage Retrieval paper](https://arxiv.org/pdf/2004.04906.pdf), this dataset consists in 21M passages from the english wikipedia along with their 768-dim embeddings computed using DPR's context encoder.\r\n\r\nNote on the implementation:\r\n- There are two configs: with and without the embeddings (73GB vs 14GB)\r\n- I used a non-fixed-size sequence of floats to describe the feature format of the embeddings. I wanted to use fixed-size sequences but I had issues with reading the arrow file afterwards (for example `dataset[0]` was crashing)\r\n- I added the case for lists of urls as input of the download_manager"
    },
    {
      "id": "195",
      "title": "Add regression test for null Sequence",
      "body": "Relates to #2892 and #2900."
    },
    {
      "id": "196",
      "title": "Inject ASR template for lj_speech dataset",
      "body": "Related to: #2565, #2633.\r\n\r\ncc: @lewtun "
    },
    {
      "id": "197",
      "title": "wikipedia dataset incomplete",
      "body": "Hey guys,\r\n\r\nI am using the https://github.com/huggingface/datasets/tree/master/datasets/wikipedia dataset.\r\nUnfortunately, I found out that there is an incompleteness for the German dataset.\r\n For reasons unknown to me, the number of inhabitants has been removed from many pages:\r\nThorey-sur-Ouche has 128 inhabitants according to the webpage (https://de.wikipedia.org/wiki/Thorey-sur-Ouche).\r\nThe pickle file however shows: franzÃ¶sische Gemeinde mit  Einwohnern (Stand).\r\n Is it possible to fix this?\r\n\r\nBest regards \r\nChris\r\n"
    },
    {
      "id": "198",
      "title": "Added the Ascent KB",
      "body": "Added the Ascent Commonsense KB of 8.9M assertions.\r\n\r\n- Paper: [Advanced Semantics for Commonsense Knowledge Extraction (WWW'21)](https://arxiv.org/abs/2011.00905)\r\n- Website: https://ascent.mpi-inf.mpg.de/\r\n\r\n(I am the author of the dataset)"
    },
    {
      "id": "199",
      "title": "Print absolute local paths in load_dataset error messages",
      "body": "Use absolute local paths in the error messages of `load_dataset` as per @stas00's suggestion in https://github.com/huggingface/datasets/pull/2500#issuecomment-874891223 "
    },
    {
      "id": "200",
      "title": "Add not-in-place implementations for several dataset transforms",
      "body": "Should we deprecate in-place versions of such methods?"
    },
    {
      "id": "201",
      "title": "Adding PersiNLU reading-comprehension",
      "body": ""
    },
    {
      "id": "202",
      "title": "Add boolq",
      "body": "I just added the dummy data for this dataset.\r\nThis one was uses `tf.io.gfile.copy` to download the data but I added the support for custom download in the mock_download_manager. I also had to add a `tensorflow` dependency for tests."
    },
    {
      "id": "203",
      "title": "Fix metrics collision in separate multiprocessed experiments",
      "body": "As noticed in #1942 , there's a issue with locks if you run multiple separate evaluation experiments in a multiprocessed setup.\r\n\r\nIndeed there is a time span in Metric._finalize() where the process 0 loses its lock before re-acquiring it. This is bad since the lock of the process 0 tells the other process that the corresponding cache file is available for writing/reading/deleting: we end up having one metric cache that collides with another one. This can raise FileNotFound errors when a metric tries to read the cache file and if the second conflicting metric deleted it.\r\n\r\nTo fix that I made sure that the lock file of the process 0 stays acquired from the cache file creation to the end of the metric computation. This way the other metrics can simply sample a new hashing name in order to avoid the collision.\r\n\r\nFinally I added missing tests for separate experiments in distributed setup."
    },
    {
      "id": "204",
      "title": "interleave_datasets for map-style datasets",
      "body": "Currently the `interleave_datasets` functions only works for `IterableDataset`.\r\nLet's make it work for map-style `Dataset` objects as well.\r\n\r\nIt would work the same way: either alternate between the datasets in order or randomly given probabilities specified by the user."
    },
    {
      "id": "205",
      "title": "Add ar rest reviews",
      "body": "added restaurants reviews in Arabic for sentiment analysis tasks"
    },
    {
      "id": "206",
      "title": "GermEval 2014: new download urls",
      "body": "Hi,\r\n\r\nunfortunately, the download links for the GermEval 2014 dataset have changed: they're now located on a Google Drive.\r\n\r\nI changed the URLs and bump version from 1.0.0 to 2.0.0."
    },
    {
      "id": "207",
      "title": "`load_dataset('docred')` results in a `NonMatchingChecksumError` ",
      "body": "## Describe the bug\r\nI get consistent `NonMatchingChecksumError: Checksums didn't match for dataset source files` errors when trying to execute `datasets.load_dataset('docred')`.\r\n\r\n## Steps to reproduce the bug\r\nIt is quasi only this code:\r\n```python\r\nimport datasets\r\ndata = datasets.load_dataset('docred')\r\n```\r\n\r\n## Expected results\r\nThe DocRED dataset should be loaded without any problems.\r\n\r\n## Actual results\r\n```\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n<ipython-input-4-b1b83f25a16c> in <module>\r\n----> 1 d = datasets.load_dataset('docred')\r\n\r\n~/anaconda3/lib/python3.8/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, streaming, **config_kwargs)\r\n    845 \r\n    846     # Download and prepare data\r\n--> 847     builder_instance.download_and_prepare(\r\n    848         downlo"
    },
    {
      "id": "208",
      "title": "Add web_split dataset for Paraphase and Rephrase benchmark",
      "body": "## Describe:\r\nFor getting simple sentences from complex sentence there are dataset and task like wiki_split that is available in hugging face datasets. This web_split is a very similar dataset. There some research paper which states that by combining these two datasets we if we train the model it will yield better results on both tests data.\r\n\r\nThis dataset is made from web NLG data.\r\n\r\nAll the dataset related details are provided in the below repository\r\n\r\nGithub link: https://github.com/shashiongithub/Split-and-Rephrase\r\n\r\n"
    },
    {
      "id": "209",
      "title": "adding swedish_medical_ner",
      "body": "Adding the Swedish Medical NER dataset, listed in \"Biomedical Datasets - BigScience Workshop 2021\""
    },
    {
      "id": "210",
      "title": "Fix bad config ids that name cache directories",
      "body": "`data_dir=None` was considered a dataset config parameter, hence creating a special config_id for all dataset being loaded.\r\nSince the config_id is used to name the cache directories, this leaded to datasets being regenerated for users.\r\n\r\nI fixed this by ignoring the value of `data_dir` when it's `None` when computing the config_id.\r\nI also added a test to make sure the cache directories are not unexpectedly renamed in the future.\r\n\r\nFix https://github.com/huggingface/datasets/issues/2683"
    },
    {
      "id": "211",
      "title": "[doc] Update deploy.sh",
      "body": ""
    },
    {
      "id": "212",
      "title": "Hind_Encorp all done",
      "body": ""
    },
    {
      "id": "213",
      "title": "Add sst dataset",
      "body": "Related to #1934&mdash;Add the Stanford Sentiment Treebank dataset."
    },
    {
      "id": "214",
      "title": "Load Image Classification Dataset from Local ",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nYes - we would like to load an image classification dataset with datasets without having to write a custom data loader.\r\n\r\n**Describe the solution you'd like**\r\n\r\nGiven a folder structure with images of each class in each folder, the ability to load these folders into a HuggingFace dataset like \"cifar10\".\r\n\r\n**Describe alternatives you've considered**\r\n\r\nImplement ViT training outside of the HuggingFace Trainer and without datasets (we did this but prefer to stay on the main path)\r\n\r\nWrite custom data loader logic\r\n\r\n**Additional context**\r\n\r\nWe're training ViT on custom dataset\r\n"
    },
    {
      "id": "215",
      "title": "adding conceptnet5",
      "body": "Adding the conceptnet5 and omcs txt files used to create the conceptnet5 dataset. Conceptne5 is a common sense dataset. More info can be found here: https://github.com/commonsense/conceptnet5/wiki"
    },
    {
      "id": "216",
      "title": "Fix wmt zh-en url",
      "body": "I verified that\r\n```\r\nwget https://stuncorpusprod.blob.core.windows.net/corpusfiles/UNv1.0.en-zh.tar.gz.00\r\n```\r\nruns in 2 minutes."
    },
    {
      "id": "217",
      "title": "add xlrd to test package requirements",
      "body": "Adds `xlrd` package to the test requirements to handle scripts that use `pandas` to load excel files"
    },
    {
      "id": "218",
      "title": "Interactively doing  save_to_disk and load_from_disk corrupts the datasets object?",
      "body": " dataset_info.json file saved after using  save_to_disk gets corrupted as follows. \r\n \r\n \r\n![image](https://user-images.githubusercontent.com/16892570/110568474-ed969880-81b7-11eb-832f-2e5129656016.png)\r\n\r\nIs there a way to disable the cache that will save to /tmp/huggiface/datastes ? \r\nI have a feeling there is a serious issue with cashing."
    },
    {
      "id": "219",
      "title": "MRPC test set differences between torch and tensorflow datasets",
      "body": "## Describe the bug\r\nWhen using `load_dataset(\"glue\", \"mrpc\")` to load the MRPC dataset, the test set includes the labels. When using `tensorflow_datasets.load('glue/{}'.format('mrpc'))` to load the dataset the test set does not contain the labels. There should be consistency between torch and tensorflow ways of importing the GLUE datasets.\r\n\r\n## Steps to reproduce the bug\r\n\r\nMinimal working code  \r\n```python\r\nfrom datasets import load_dataset\r\nimport tensorflow as tf\r\nimport tensorflow_datasets\r\n\r\n# torch\r\ndataset = load_dataset(\"glue\", \"mrpc\")\r\n# tf\r\ndata = tensorflow_datasets.load('glue/{}'.format('mrpc'))\r\ndata = list(data['test'].as_numpy_iterator())\r\nfor i in range(40,50):\r\n  tf_sentence1 = data[i]['sentence1'].decode(\"utf-8\") \r\n  tf_sentence2 = data[i]['sentence2'].decode(\"utf-8\") \r\n\r\n  tf_label = data[i]['label']\r\n  \r\n  index = data[i]['idx']\r\n  print('Index {}'.format(index))\r\n  torch_sentence1 = dataset['test']['sentence1'][index]\r\n  torch_sentence2 = dataset['test']['sentenc"
    },
    {
      "id": "220",
      "title": "switching some low-level log.info's to log.debug?",
      "body": "In https://github.com/huggingface/transformers/pull/12276 we are now changing the examples to have `datasets` on the same log level as `transformers`, so that one setting can do a consistent logging across all involved components.\r\n\r\nThe trouble is that now we get a ton of these:\r\n\r\n```\r\n06/23/2021 12:15:31 - INFO - datasets.utils.filelock -   Lock 139627640431136 acquired on /home/stas/.cache/huggingface/metrics/sacrebleu/default/default_experiment-1-0.arrow.lock\r\n06/23/2021 12:15:31 - INFO - datasets.arrow_writer -   Done writing 50 examples in 12280 bytes /home/stas/.cache/huggingface/metrics/sacrebleu/default/default_experiment-1-0.arrow.\r\n06/23/2021 12:15:31 - INFO - datasets.arrow_dataset -   Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\r\n06/23/2021 12:15:31 - INFO - datasets.utils.filelock -   Lock 139627640431136 released on /home/stas/.cache/huggingface/metrics/sacrebleu/default/defa"
    },
    {
      "id": "221",
      "title": "Add WikiCREM",
      "body": "## Adding a Dataset\r\n- **Name:** WikiCREM\r\n- **Description:** A large unsupervised corpus for coreference resolution.\r\n- **Paper:** https://arxiv.org/abs/1905.06290\r\n- **Github repo:**: https://github.com/vid-koci/bert-commonsense\r\n- **Data:** https://ora.ox.ac.uk/objects/uuid:c83e94bb-7584-41a1-aef9-85b0e764d9e3\r\n- **Motivation:** Coreference resolution, common sense reasoning\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n"
    },
    {
      "id": "222",
      "title": "nlp downloads to its module path",
      "body": "I am trying to package `nlp` for Nix, because it is now an optional dependency for `transformers`. The problem that I encounter is that the `nlp` library downloads to the module path, which is typically not writable in most package management systems:\r\n\r\n```>>> import nlp\r\n>>> squad_dataset = nlp.load_dataset('squad')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/nix/store/2yhik0hhqayksmkkfb0ylqp8cf5wa5wp-python3-3.8.5-env/lib/python3.8/site-packages/nlp/load.py\", line 530, in load_dataset\r\n    module_path, hash = prepare_module(path, download_config=download_config, dataset=True)\r\n  File \"/nix/store/2yhik0hhqayksmkkfb0ylqp8cf5wa5wp-python3-3.8.5-env/lib/python3.8/site-packages/nlp/load.py\", line 329, in prepare_module\r\n    os.makedirs(main_folder_path, exist_ok=True)\r\n  File \"/nix/store/685kq8pyhrvajah1hdsfn4q7gm3j4yd4-python3-3.8.5/lib/python3.8/os.py\", line 223, in makedirs\r\n    mkdir(name, mode)\r\nOSError: [Errno 30] Read-only file system: '/ni"
    },
    {
      "id": "223",
      "title": "[Question] Create Apache Arrow dataset from raw text file",
      "body": "Hi guys, I have gathered and preprocessed about 2GB of COVID papers from CORD dataset @ Kggle. I have seen you have a text dataset as \"Crime and punishment\" in Apache arrow format. Do you have any script to do it from a raw txt file (preprocessed as for BERT like) or any guide?\r\nIs the worth of send it to you and add it to the NLP library?\r\nThanks, Manu\r\n"
    },
    {
      "id": "224",
      "title": "Add Winogender Schemas",
      "body": "## Adding a Dataset\r\n- **Name:** Winogender Schemas\r\n- **Description:** Winogender Schemas (inspired by Winograd Schemas) are minimal pairs of sentences that differ only by the gender of one pronoun in the sentence, designed to test for the presence of gender bias in automated coreference resolution systems.\r\n- **Paper:** https://arxiv.org/abs/1804.09301\r\n- **Data:** https://github.com/rudinger/winogender-schemas (see data directory)\r\n- **Motivation:** Testing gender bias in automated coreference resolution systems, improve coreference resolution in general.\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n"
    },
    {
      "id": "225",
      "title": "SciFact dataset - minor changes",
      "body": "Hi,\r\n\r\nSciFact dataset creator here. First of all, thanks for adding the dataset to Huggingface, much appreciated!\r\n\r\nI'd like to make a few minor changes, including the citation information and the `_URL` from which to download the dataset. Can I submit a PR for this?\r\n\r\nIt also looks like the dataset is being downloaded directly from Huggingface's Google cloud account rather than via the `_URL` in [scifact.py](https://github.com/huggingface/datasets/blob/master/datasets/scifact/scifact.py). Can you help me update the version on gcloud?\r\n\r\nThanks,\r\n\r\nDave"
    },
    {
      "id": "226",
      "title": "Add Hateful Memes Dataset",
      "body": "## Add Hateful Memes Dataset\r\n- **Name:** Hateful Memes\r\n- **Description:** [https://ai.facebook.com/blog/hateful-memes-challenge-and-data-set]( https://ai.facebook.com/blog/hateful-memes-challenge-and-data-set)\r\n- **Paper:** [https://arxiv.org/pdf/2005.04790.pdf](https://arxiv.org/pdf/2005.04790.pdf)\r\n- **Data:** [This link](https://drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com/XjiOc5ycDBRRNwbhRlgH.zip?AWSAccessKeyId=AKIARVBOBDCY4MWEDJKS&Signature=DaUuGgZWUgDHzEPPbyJ2PhSJ56Q%3D&Expires=1612816874)\r\n- **Motivation:** Including multi-modal datasets to ðŸ¤— datasets.\r\n\r\nI will be adding this dataset. It requires the user to sign an agreement on DrivenData. So, it will be used with a manual download.\r\n\r\nThe issue with this dataset is that the images are of different sizes. The image datasets added so far (CIFAR-10 and MNIST) have a uniform shape throughout.\r\nSo something like \r\n```python\r\n datasets.Array2D(shape=(28, 28), dtype=\"uint8\")\r\n```\r\nwon't work for the images. How wo"
    },
    {
      "id": "227",
      "title": "[GEM] add DART data-to-text generation dataset",
      "body": "## Adding a Dataset\r\n- **Name:** DART\r\n- **Description:** DART consists of 82,191 examples across different domains with each input being a semantic RDF triple set derived from data records in tables and the tree ontology of the schema, annotated with sentence descriptions that cover all facts in the triple set.\r\n- **Paper:** https://arxiv.org/abs/2007.02871v1\r\n- **Data:** https://github.com/Yale-LILY/dart\r\n- **Motivation:** the dataset will likely be included in the GEM benchmark\r\n\r\nInstructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).\r\n"
    },
    {
      "id": "228",
      "title": "Local machine/cluster Beam Datasets example/tutorial",
      "body": "Hi,\r\n\r\nI'm wondering if https://huggingface.co/docs/datasets/beam_dataset.html has an non-GCP or non-Dataflow version example/tutorial? I tried to migrate it to run on DirectRunner and SparkRunner, however, there were way too many runtime errors that I had to fix during the process, and even so I wasn't able to get either runner correctly producing the desired output.\r\n\r\nThanks!\r\nShang"
    },
    {
      "id": "229",
      "title": "Implement layered building",
      "body": "As discussed with @stas00 and @lhoestq (see also here https://github.com/huggingface/datasets/issues/2481#issuecomment-859712190):\r\n\r\n> My suggestion for this would be to have this enabled by default.\r\n> \r\n> Plus I don't know if there should be a dedicated issue to that is another functionality. But I propose layered building rather than all at once. That is:\r\n>\r\n> 1. uncompress a handful of files via a generator enough to generate one arrow file\r\n> 2. process arrow file 1\r\n> 3. delete all the files that went in and aren't needed anymore.\r\n>\r\n> rinse and repeat.\r\n> \r\n> 1. This way much less disc space will be required - e.g. on JZ we won't be running into inode limitation, also it'd help with the collaborative hub training project\r\n> 2. The user doesn't need to go and manually clean up all the huge files that were left after pre-processing\r\n> 3. It would already include deleting temp files this issue is talking about\r\n> \r\n> I wonder if the new streaming API would be of help, except her"
    },
    {
      "id": "230",
      "title": "Opus DGT added",
      "body": "Dataset : http://opus.nlpl.eu/DGT.php"
    },
    {
      "id": "231",
      "title": "Dataset",
      "body": "## Adding a Dataset\n- **Name:** *name of the dataset*\n- **Description:** *short description of the dataset (or link to social media or blog post)*\n- **Paper:** *link to the dataset paper if available*\n- **Data:** *link to the Github repository or current dataset location*\n- **Motivation:** *what are some good reasons to have this dataset*\n\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md)."
    },
    {
      "id": "232",
      "title": "Missing ClassLabel encoding in Json loader",
      "body": "Currently if you want to load a json dataset this way\r\n```python\r\ndataset = load_dataset(\"json\", data_files=data_files, features=features)\r\n```\r\nThen if your features has ClassLabel types and if your json data needs class label encoding (i.e. if the labels in the json files are strings and not integers), then it would fail:\r\n```python\r\n[...]\r\n~/Desktop/hf/datasets/src/datasets/packaged_modules/json/json.py in _generate_tables(self, files)\r\n     94             if self.config.schema:\r\n     95                 # Cast allows str <-> int/float, while parse_option explicit_schema does NOT\r\n---> 96                 pa_table = pa_table.cast(self.config.schema)\r\n     97             yield i, pa_table\r\n[...]\r\nArrowInvalid: Failed to parse string: 'O' as a scalar of type int64\r\n```\r\n\r\nThis is because it just tries to cast the string data to integers, without applying the mapping str->int first\r\n\r\nThe current workaround is to do instead\r\n```python\r\ndataset = load_dataset(\"json\", data_files=data_files"
    },
    {
      "id": "233",
      "title": "Configure library root logger at the module level",
      "body": "Configure library root logger at the datasets.logging module level (singleton-like).\r\n\r\nBy doing it this way:\r\n- we are sure configuration is done only once: module level code is only runned once\r\n- no need of global variable\r\n- no need of threading lock"
    },
    {
      "id": "234",
      "title": "Adding HendrycksTest dataset",
      "body": "Adding Hendrycks test from https://arxiv.org/abs/2009.03300.\r\nI'm having a bit of trouble with dummy data creation because some lines in the csv files aren't being loaded properly (only the first entry loaded in a row of length 6). The dataset is loading just fine. Hope you can kindly help!\r\nThank you!"
    },
    {
      "id": "235",
      "title": "Don't use take on dataset table in pyarrow 1.0.x",
      "body": "Fix #615 "
    },
    {
      "id": "236",
      "title": "Cannot load TREC dataset: ConnectionError",
      "body": "## Problem\r\nI cannot load \"trec\" dataset, it results with ConnectionError as shown below. I've tried on both Google Colab and locally. \r\n* `requests.head('http://cogcomp.org/Data/QA/QC/train_5500.label')` returns <Response [302]>. \r\n* `requests.head('http://cogcomp.org/Data/QA/QC/train_5500.label', allow_redirects=True)` raises `requests.exceptions.TooManyRedirects: Exceeded 30 redirects.`\r\n* Opening `http://cogcomp.org/Data/QA/QC/train_5500.label' in a browser works, but opens a different address\r\n* Increasing max_redirects to 100 doesn't help\r\n\r\nAlso, while debugging I've seen that requesting 'https://storage.googleapis.com/huggingface-nlp/cache/datasets/trec/default/1.1.0/dataset_info.json' returns <Response [404]> before, but it doesn't raise any errors. Not sure if that's relevant.\r\n\r\n* datasets.__version__ == '1.1.2'\r\n* requests.__version__ == '2.24.0'\r\n\r\n## Error trace\r\n```\r\n>>> import datasets\r\n>>> datasets.__version__\r\n'1.1.2'\r\n>>> dataset = load_dataset(\"trec\", split=\"train\")"
    },
    {
      "id": "237",
      "title": "Fix DuplicatedKeysError in omp",
      "body": "Close #2598."
    },
    {
      "id": "238",
      "title": "Add Matthews/Pearson/Spearman correlation metrics",
      "body": "Added three metrics:\r\n- The Matthews correlation coefficient (from sklearn)\r\n- The Pearson correlation coefficient (from scipy)\r\n- The Spearman correlation coefficient (from scipy)\r\n\r\ncc @sgugger "
    },
    {
      "id": "239",
      "title": "Check that array is not Float as nan != nan",
      "body": "The Exception wants to check for issues with StructArrays/ListArrays but catches FloatArrays with value nan as nan != nan.\r\nPass on FloatArrays as we should not raise an Exception for them."
    },
    {
      "id": "240",
      "title": "fix variable name typo",
      "body": ""
    },
    {
      "id": "241",
      "title": "How to reuse functionality of a (generic) dataset?",
      "body": "I have written a generic dataset for corpora created with the Brat annotation tool ([specification](https://brat.nlplab.org/standoff.html), [dataset code](https://github.com/ArneBinder/nlp/blob/brat/datasets/brat/brat.py)). Now I wonder how to use that to create specific dataset instances. What's the recommended way to reuse formats and loading functionality for datasets with a common format?\r\n\r\nIn my case, it took a bit of time to create the Brat dataset and I think others would appreciate to not have to think about that again. Also, I assume there are other formats (e.g. conll) that are widely used, so having this would really ease dataset onboarding and adoption of the library."
    },
    {
      "id": "242",
      "title": "Fix stream oscar",
      "body": "Previously, an additional `open` was added to oscar to make it stream-compatible: 587bbb94e891b22863b312b99696e32708c379f4.\r\n\r\nThis was argued that might be problematic: https://github.com/huggingface/datasets/pull/2786#discussion_r690045921\r\n\r\nThis PR:\r\n- removes that additional `open`\r\n- patches `gzip.open` with `xopen` + `compression=\"gzip\"`"
    },
    {
      "id": "243",
      "title": "Saving processed dataset running infinitely",
      "body": "I have a text dataset of size 220M.\r\n\r\nFor pre-processing, I need to tokenize this and filter rows with the large sequence.\r\n\r\nMy tokenization took roughly 3hrs. I used map() with batch size 1024 and multi-process with 96 processes.\r\n\r\nfilter() function was way to slow, so I used a hack to use pyarrow filter table function, which is damm fast. Mentioned [here](https://github.com/huggingface/datasets/issues/1796)\r\n\r\n```dataset._data = dataset._data.filter(...)```\r\nIt took 1 hr for the filter.\r\n\r\nThen i use `save_to_disk()` on processed dataset and it is running forever.\r\n\r\nI have been waiting since 8 hrs, it has not written a single byte. \r\n\r\nInfact it has actually read from disk more than 100GB, screenshot below shows the stats using `iotop`. \r\nSecond process is the one.\r\n<img width=\"1672\" alt=\"Screenshot 2021-02-19 at 6 36 53 PM\" src=\"https://user-images.githubusercontent.com/20911334/108508197-7325d780-72e1-11eb-8369-7c057d137d81.png\">\r\n\r\n\r\nI am not able to figure out, whether this i"
    },
    {
      "id": "244",
      "title": "Add KorNLU",
      "body": "Added Korean NLU datasets. The link to the dataset can be found [here](https://github.com/kakaobrain/KorNLUDatasets) and the paper can be found [here](https://arxiv.org/abs/2004.03289)\r\n\r\n**Note**: The MNLI tsv file is broken, so this code currently excludes the file. Please suggest other alternative if any @lhoestq \r\n\r\n- [x] Followed the instructions in CONTRIBUTING.md\r\n- [x] Ran the tests successfully\r\n- [x] Created the dummy data"
    },
    {
      "id": "245",
      "title": "A descriptive name for my changes",
      "body": "hind encorp resubmited"
    },
    {
      "id": "246",
      "title": "Add open book corpus",
      "body": "Adds book corpus based on Shawn Presser's [work](https://github.com/soskek/bookcorpus/issues/27)  @richarddwang, the author of the original BookCorpus dataset, suggested it should be named [OpenBookCorpus](https://github.com/huggingface/datasets/issues/486). I named it BookCorpusOpen to be easily located alphabetically. But, of course, we can rename it if needed. \r\n\r\nIt contains 17868 dataset items; each item contains two fields: title and text. The title is the name of the book (just the file name) while the text contains unprocessed book text. Note that bookcorpus is pre-segmented into a sentence while this bookcorpus is not. This is intentional (see https://github.com/huggingface/datasets/issues/486) as some users might want to further process the text themselves. \r\n\r\n@lhoestq and others please review this PR thoroughly. cc @shawwn "
    },
    {
      "id": "247",
      "title": "Add Spanish POR and NER Datasets",
      "body": "Hi guys,\r\nIn order to cover multilingual support a little step could be adding standard Datasets used for Spanish NER and POS tasks.\r\nI can provide it in raw and preprocessed formats."
    },
    {
      "id": "248",
      "title": "Delete CONTRIBUTING.md",
      "body": ""
    },
    {
      "id": "249",
      "title": "load the local dataset",
      "body": "your guidebook's example is like\r\n>>>from datasets import load_dataset\r\n>>> dataset = load_dataset('json', data_files='my_file.json')\r\nbut the first arg is path...\r\nso how should i do if i want to load the local dataset for model training?\r\ni will be grateful if you can help me handle this problem!\r\nthanks a lot!"
    },
    {
      "id": "250",
      "title": "[FEATURE REQUEST] Multiprocessing with for dataset.map, dataset.filter",
      "body": "It would be nice to be able to speed up `dataset.map` or `dataset.filter`. Perhaps this is as easy as sharding the dataset sending each shard to a process/thread/dask pool and using the new `nlp.concatenate_dataset()` function to join them all together?"
    },
    {
      "id": "251",
      "title": "adding HindEncorp",
      "body": "adding Hindi Wikipedia corpus"
    },
    {
      "id": "252",
      "title": "Make the json script more flexible",
      "body": "Fix https://github.com/huggingface/nlp/issues/359\r\nFix https://github.com/huggingface/nlp/issues/369\r\n\r\nJSON script now can accept JSON files containing a single dict with the records as a list in one attribute to the dict (previously it only accepted JSON files containing records as rows of dicts in the file).\r\n\r\nIn this case, you should indicate using `field=XXX` the name of the field in the JSON structure which contains the records you want to load. The records can be a dict of lists or a list of dicts.\r\n\r\nE.g. to load the SQuAD dataset JSON (without using the `squad` specific dataset loading script), in which the data rows are in the `data` field of the JSON dict, you can do:\r\n```python\r\nfrom nlp import load_dataset\r\ndataset = load_dataset('json', data_files='/PATH/TO/JSON', field='data')\r\n```"
    },
    {
      "id": "253",
      "title": "Json loader fails if user-specified features don't match the json data fields order",
      "body": "If you do\r\n```python\r\ndataset = load_dataset(\"json\", data_files=data_files, features=features)\r\n```\r\nThen depending on the order of the features in the json data field it fails:\r\n```python\r\n[...]\r\n~/Desktop/hf/datasets/src/datasets/packaged_modules/json/json.py in _generate_tables(self, files)\r\n     94             if self.config.schema:\r\n     95                 # Cast allows str <-> int/float, while parse_option explicit_schema does NOT\r\n---> 96                 pa_table = pa_table.cast(self.config.schema)\r\n     97             yield i, pa_table\r\n[...]\r\nValueError: Target schema's field names are not matching the table's field names: ['tokens', 'ner_tags'], ['ner_tags', 'tokens']\r\n```\r\n\r\nThis is because one must first re-order the columns of the table to match the `self.config.schema` before calling cast.\r\n\r\nOne way to fix the `cast` would be to replace it with:\r\n```python\r\n# reorder the arrays if necessary + cast to schema\r\n# we can't simply use .cast here because we may need to change "
    },
    {
      "id": "254",
      "title": "Fix potential DuplicatedKeysError in SQuAD",
      "body": "DONE:\r\n- Fix potential DiplicatedKeysError by ensuring keys are unique.\r\n- Align examples in the docs with SQuAD code.\r\n\r\nWe should promote as a good practice, that the keys should be programmatically generated as unique, instead of read from data (which might be not unique)."
    },
    {
      "id": "255",
      "title": "Xsum, require manual download of some files",
      "body": ""
    },
    {
      "id": "256",
      "title": "Having a dependency defining fsspec entrypoint raises an AttributeError when importing datasets",
      "body": "## Describe the bug\r\nIn one of my project, I defined a custom fsspec filesystem with an entrypoint.\r\nMy guess is that by doing so, a variable named `spec` is created in the module `fsspec` (created by entering a for loop as there are entrypoints defined, see the loop in question [here](https://github.com/intake/filesystem_spec/blob/0589358d8a029ed6b60d031018f52be2eb721291/fsspec/__init__.py#L55)).\r\nSo that `fsspec.spec`, that was previously referring to the `spec` submodule, is now referring to that `spec` variable.\r\nThis make the import of datasets failing as it is using that `fsspec.spec`.\r\n\r\n## Steps to reproduce the bug\r\nI could reproduce the bug with a dummy poetry project.\r\n\r\nHere is the pyproject.toml:\r\n```toml\r\n[tool.poetry]\r\nname = \"debug-datasets\"\r\nversion = \"0.1.0\"\r\ndescription = \"\"\r\nauthors = [\"Pierre Godard\"]\r\n\r\n[tool.poetry.dependencies]\r\npython = \"^3.8\"\r\ndatasets = \"^1.11.0\"\r\n\r\n[tool.poetry.dev-dependencies]\r\n\r\n[build-system]\r\nrequires = [\"poetry-core>=1.0.0\"]\r\nbuild-bac"
    },
    {
      "id": "257",
      "title": "Readme.md is misleading about kinds of datasets?",
      "body": "Hi!\r\n\r\nAt the README.MD, you say: \"efficient data pre-processing: simple, fast and reproducible data pre-processing for the above public datasets as well as your own local datasets in CSV/JSON/text. \"\r\n\r\nBut here:\r\nhttps://github.com/huggingface/datasets/blob/master/templates/new_dataset_script.py#L82-L117\r\n\r\nYou mention other kinds of datasets, with images and so on. I'm confused. \r\n\r\nIs it possible to use it to store, say, imagenet locally? "
    },
    {
      "id": "258",
      "title": "Fix fn kwargs in filter",
      "body": "#2836 broke the `fn_kwargs` parameter of `filter`, as mentioned in https://github.com/huggingface/datasets/issues/2927\r\n\r\nI fixed that and added a test to make sure it doesn't happen again (for either map or filter)\r\n\r\nFix #2927"
    },
    {
      "id": "259",
      "title": "How to update the \"wino_bias\" dataset",
      "body": "Hi all,\r\n\r\nThanks for the efforts to collect all the datasets! But I think there is a problem with the wino_bias dataset. The current link is not correct. How can I update that?\r\n\r\nThanks!"
    },
    {
      "id": "260",
      "title": "Add European Union Education and Culture Translation Memory (EAC-TM) dataset",
      "body": "Adding the EAC Translation Memory dataset : https://ec.europa.eu/jrc/en/language-technologies/eac-translation-memory"
    },
    {
      "id": "261",
      "title": "Add HKCanCor",
      "body": "(Apologies, didn't manage the branches properly and the PR got too messy. Going to open a new PR with everything in order)"
    },
    {
      "id": "262",
      "title": "feat(docs): navigate with left/right arrow keys",
      "body": "Enables docs navigation with left/right arrow keys. It can be useful for the ones who navigate with keyboard a lot.\r\nMore info : https://github.com/sphinx-doc/sphinx/pull/2064\r\n\r\nYou can try here : https://29353-250213286-gh.circle-artifacts.com/0/docs/_build/html/index.html"
    },
    {
      "id": "263",
      "title": "Adding babi dataset",
      "body": "Adding the English version of bAbI.\r\n\r\nSamples are taken from ParlAI for consistency with the main users at the moment.\r\n\r\nSupersede #945 (problem with the rebase) and adresses the issues mentioned in the review (dummy data are smaller now and code comments are fixed)."
    },
    {
      "id": "264",
      "title": "Improve performance of pandas arrow extractor",
      "body": "While reviewing PR #2505, I noticed that pandas arrow extractor could be refactored to be faster."
    },
    {
      "id": "265",
      "title": "Fixing the URL filtering for bad MLSUM examples in GEM",
      "body": "This updates the code and metadata to use the updated `gem_mlsum_bad_ids_fixed.json` file provided by @juand-r\r\n\r\ncc @sebastianGehrmann "
    },
    {
      "id": "266",
      "title": "can't load SNLI dataset",
      "body": "`nlp` seems to load `snli` from some URL based on nlp.stanford.edu. This subdomain is frequently down -- including right now, when I'd like to load `snli` in a Colab notebook, but can't.\r\n\r\nIs there a plan to move these datasets to huggingface servers for a more stable solution?\r\n\r\nBtw, here's the stack trace:\r\n\r\n```\r\nFile \"/content/nlp/src/nlp/builder.py\", line 432, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/content/nlp/src/nlp/builder.py\", line 466, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/content/nlp/src/nlp/datasets/snli/e417f6f2e16254938d977a17ed32f3998f5b23e4fcab0f6eb1d28784f23ea60d/snli.py\", line 76, in _split_generators\r\n    dl_dir = dl_manager.download_and_extract(_DATA_URL)\r\n  File \"/content/nlp/src/nlp/utils/download_manager.py\", line 217, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  Fil"
    },
    {
      "id": "267",
      "title": "add MS MARCO dataset",
      "body": "This PR adds the MS MARCO dataset as requested in this issue #336. MS mARCO has multiple task including:\r\n\r\n- Passage and Document Retrieval\r\n\r\n- Keyphrase Extraction\r\n\r\n- QA and NLG\r\n\r\nThis PR only adds the 2 versions of the  QA and NLG task dataset which was realeased with the original paper here https://arxiv.org/pdf/1611.09268.pdf \r\n\r\nTests are failing because of the dummy data. I tried to fix it without success. Can you please have a look at it? @patrickvonplaten , @lhoestq "
    },
    {
      "id": "268",
      "title": "Fix arrow writer for big datasets using writer_batch_size",
      "body": "This PR fixes Yacine's bug.\r\nAccording to [this](https://github.com/apache/arrow/blob/master/docs/source/cpp/arrays.rst#size-limitations-and-recommendations), it is not recommended to have pyarrow arrays bigger than 2Go.\r\n\r\nTherefore I set a default batch size of 100 000 examples per batch. In general it shouldn't exceed 2Go. If it does, I reduce the batch_size on the fly, and I notify the user with a warning."
    },
    {
      "id": "269",
      "title": "change bibtex template to author instead of authors",
      "body": "Hi,\r\nIMO when using BibTex Author should be used instead of Authors.\r\nSee here: http://www.bibtex.org/Using/de/\r\n\r\nThanks\r\nPhilip"
    },
    {
      "id": "270",
      "title": "DuplicatedKeysError when trying to load adversarial_qa",
      "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\ndataset = load_dataset('adversarial_qa', 'adversarialQA')\r\n```\r\n\r\n## Expected results\r\nThe dataset should be loaded into memory\r\n\r\n## Actual results\r\n\r\n>DuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\n>Found duplicate Key: 4d3cb5677211ee32895ca9c66dad04d7152254d4\r\n>Keys should be unique and deterministic in nature\r\n>\r\n>\r\n>During handling of the above exception, another exception occurred:\r\n>\r\n>DuplicatedKeysError                       Traceback (most recent call last)\r\n>\r\n>/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py in check_duplicate_keys(self)\r\n>    347         for hash, key in self.hkey_record:\r\n>    348             if hash in tmp_record:\r\n>--> 349                 raise DuplicatedKeysError(key)\r\n>    350             else:\r\n>    351                 tmp_record.add(hash)\r\n>\r\n>DuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\n>Found duplicate Ke"
    },
    {
      "id": "271",
      "title": "Add E2E NLG",
      "body": "Adding the E2E NLG dataset.\r\n\r\nMore info here : http://www.macs.hw.ac.uk/InteractionLab/E2E/\r\n\r\n### Checkbox\r\n\r\n- [x] Create the dataset script `/datasets/my_dataset/my_dataset.py` using the template\r\n- [x] Fill the `_DESCRIPTION` and `_CITATION` variables\r\n- [x] Implement `_infos()`, `_split_generators()` and `_generate_examples()`\r\n- [x] Make sure that the `BUILDER_CONFIGS` class attribute is filled with the different configurations of the dataset and that the `BUILDER_CONFIG_CLASS` is specified if there is a custom config class.\r\n- [x] Generate the metadata file `dataset_infos.json` for all configurations\r\n- [x] Generate the dummy data `dummy_data.zip` files to have the dataset script tested and that they don't weigh too much (<50KB)\r\n- [x] Add the dataset card `README.md` using the template and at least fill the tags \r\n- [x] Both tests for the real data and the dummy data pass.\r\n"
    },
    {
      "id": "272",
      "title": "Fix FileSystems documentation",
      "body": "### What this fixes:\r\nThis PR resolves several issues I discovered in the documentation on the `datasets.filesystems` module ([this page](https://huggingface.co/docs/datasets/filesystems.html)).\r\n\r\n### What were the issues?\r\nWhen I originally tried implementing the code examples I faced several bugs attributed to:\r\n\r\n- out of date [botocore](https://github.com/boto/botocore) call signatures\r\n- capitalization errors in the `S3FileSystem` class name (written as `S3Filesystem` in one place)\r\n- call signature errors for the `S3FileSystem` class constructor (uses parameter `sessions` instead of `session` in some places) (see [`s3fs`](https://s3fs.readthedocs.io/en/latest/api.html#s3fs.core.S3FileSystem) for where this constructor signature is defined)\r\n\r\n### Testing/reviewing notes\r\nInstructions for generating the documentation locally: [here](https://github.com/huggingface/datasets/tree/master/docs#generating-the-documentation)."
    },
    {
      "id": "273",
      "title": "initial commit for MultiReQA for second PR",
      "body": "Since last PR #1349 had some issues passing the tests. So, a new PR is generated."
    },
    {
      "id": "274",
      "title": "Set default in-memory value depending on the dataset size",
      "body": "Set a default value for `in_memory` depending on the size of the dataset to be loaded.\r\n\r\nClose #2179.\r\n\r\nTODO:\r\n- [x] Add a section in the docs about this.\r\n- ~Add a warning if someone tries to specify `cache_file_name=` in `map`, `filter` etc. on a dataset that is in memory, since the computation is not going to be cached in this case.~"
    },
    {
      "id": "275",
      "title": "UnicodeDecodeError for OSCAR (Afrikaans)",
      "body": "## Describe the bug\r\nWhen loading the [OSCAR dataset](https://huggingface.co/datasets/oscar) (specifically `unshuffled_deduplicated_af`), I encounter a `UnicodeDecodeError`.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_af\")\r\n```\r\n\r\n## Expected results\r\nAnything but an error, really.\r\n\r\n## Actual results\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_af\")\r\nDownloading: 14.7kB [00:00, 4.91MB/s]\r\nDownloading: 3.07MB [00:00, 32.6MB/s]\r\nDownloading and preparing dataset oscar/unshuffled_deduplicated_af (download: 62.93 MiB, generated: 163.38 MiB, post-processed: Unknown size, total: 226.32 MiB) to C:\\Users\\sgraaf\\.cache\\huggingface\\datasets\\oscar\\unshuffled_deduplicated_af\\1.0.0\\bd4f96df5b4512007ef9fd17bbc1ecde459fa53d2fc0049cf99392ba2efcc464...\r\nDownloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ"
    },
    {
      "id": "276",
      "title": "Cache directories changed due to recent changes in how config kwargs are handled",
      "body": "Since #2659 I can see weird cache directory names with hashes in the config id, even though no additional config kwargs are passed. For example:\r\n\r\n```python\r\nfrom datasets import load_dataset_builder\r\n\r\nc4_builder = load_dataset_builder(\"c4\", \"en\")\r\nprint(c4_builder.cache_dir)\r\n# /Users/quentinlhoest/.cache/huggingface/datasets/c4/en-174d3b7155eb68db/0.0.0/...\r\n\r\n# instead of \r\n# /Users/quentinlhoest/.cache/huggingface/datasets/c4/en/0.0.0/...\r\n```\r\nThis issue could be annoying since it would simply ignore old cache directories for users, and regenerate datasets\r\n\r\ncc @stas00 this is what you experienced a few days ago\r\n"
    },
    {
      "id": "277",
      "title": "Move cache dir root creation in builder's init",
      "body": "We use lock files in the builder initialization but sometimes the cache directory where they're supposed to be was not created. To fix that I moved the builder's cache dir root creation in the builder's init.\r\n\r\nFix #671 "
    },
    {
      "id": "278",
      "title": "adding cdt dataset",
      "body": "- **Name:** *Cyberbullying Detection Task*\r\n- **Description:** *The Cyberbullying Detection task was part of 2019 edition of PolEval competition. The goal is to predict if a given Twitter message contains a cyberbullying (harmful) content.*\r\n- **Data:** *https://github.com/ptaszynski/cyberbullying-Polish*\r\n- **Motivation:** *The KLEJ benchmark (Kompleksowa Lista Ewaluacji JÄ™zykowych) is a set of nine evaluation tasks for the Polish language understanding.*"
    },
    {
      "id": "279",
      "title": "ArrowInvalid issue for squad v2 dataset",
      "body": "Hello, I am using the huggingface official question answering example notebook (https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb). \r\n\r\nIn the prepare_validation_features function, I made some modifications to tokenize a new set of quesions with the original contexts and save them in three different list called candidate_input_dis, candidate_attetion_mask and candidate_token_type_ids. When I try to run the next cell for dataset.map, I got the following error:\r\n\r\n`ArrowInvalid: Column 1 named candidate_attention_mask expected length 1180 but got length 1178`\r\n\r\nMy code is as follows:\r\n\r\n```\r\ndef generate_candidate_questions(examples):\r\n  val_questions = examples[\"question\"]\r\n  candididate_questions = random.sample(datasets[\"train\"][\"question\"], len(val_questions))\r\n  candididate_questions = [x[:max_length] for x in candididate_questions]\r\n  return candididate_questions\r\n\r\ndef prepare_validation_features(examples, use_mixing=Fals"
    },
    {
      "id": "280",
      "title": "Use passed --cache_dir for modules cache",
      "body": "When passed `--cache_dir` arg:\r\n```shell\r\npython datasets-cli test datasets/<my-dataset-folder> --save_infos --all_configs --cache_dir <my-cache-dir>\r\n```\r\nit is not used for caching the modules, which are cached in the default location at `.cache/huggingface/modules`.\r\n\r\nWith this fix, the modules will be cached at `<my-cache-dir>/modules`."
    },
    {
      "id": "281",
      "title": "updated multi_nli dataset with missing fields",
      "body": "1) updated fields which were missing earlier\r\n2) added tags to README\r\n3) updated a few fields of README \r\n4) new dataset_infos.json and dummy files"
    },
    {
      "id": "282",
      "title": "Update XNLI download link",
      "body": "The old link isn't working anymore. I updated it with the new official link.\r\nFix #690 "
    },
    {
      "id": "283",
      "title": "Adding the BrWaC dataset",
      "body": "Adding the BrWaC dataset, a large corpus of Portuguese language texts"
    },
    {
      "id": "284",
      "title": "add BlendedSkillTalk dataset",
      "body": "This PR add the BlendedSkillTalk dataset, which is used to fine tune the blenderbot."
    },
    {
      "id": "285",
      "title": "Fix logging levels",
      "body": "Sometimes default `datasets` logging can be too verbose. One approach could be reducing some logging levels, from info to debug, or from warning to info.\r\n\r\nClose #2543.\r\n\r\ncc: @stas00 "
    },
    {
      "id": "286",
      "title": "Duplicates in the LAMA dataset",
      "body": "I observed duplicates in the LAMA probing dataset, see a minimal code below. \r\n\r\n```\r\n>>> import datasets\r\n>>> dataset = datasets.load_dataset('lama')\r\nNo config specified, defaulting to: lama/trex\r\nReusing dataset lama (/home/anam/.cache/huggingface/datasets/lama/trex/1.1.0/97deffae13eca0a18e77dfb3960bb31741e973586f5c1fe1ec0d6b5eece7bddc)\r\n>>> train_dataset = dataset['train']\r\n>>> train_dataset[0]\r\n{'description': 'language or languages a person has learned from early childhood', 'label': 'native language', 'masked_sentence': 'Louis Jules Trochu ([lwi Ê’yl tÊÉ”Êƒy]; 12 March 1815 â€“ 7 October 1896) was a [MASK] military leader and politician.', 'obj_label': 'French', 'obj_surface': 'French', 'obj_uri': 'Q150', 'predicate_id': 'P103', 'sub_label': 'Louis Jules Trochu', 'sub_surface': 'Louis Jules Trochu', 'sub_uri': 'Q441235', 'template': 'The native language of [X] is [Y] .', 'template_negated': '[X] is not owned by [Y] .', 'type': 'N-1', 'uuid': '40b2ed1c-0961-482e-844e-32596b6117c8'}\r\n>"
    },
    {
      "id": "287",
      "title": "SNLI dataset contains labels with value -1",
      "body": "```\r\nimport datasets\r\nnli_data = datasets.load_dataset(\"snli\")\r\ntrain_data = nli_data['train']\r\ntrain_labels = train_data['label']\r\nlabel_set = set(train_labels)\r\nprint(label_set)\r\n```\r\n\r\n**Output:**\r\n`{0, 1, 2, -1}`"
    },
    {
      "id": "288",
      "title": "Add arabic dialects",
      "body": "Data loading script and dataset card for Dialectal Arabic Resources dataset. \r\nFixed git issues from PR  #976"
    },
    {
      "id": "289",
      "title": "[Tests] Test files locally",
      "body": "This PR adds a `aws` and a `local` decorator to the tests so that tests now run on the local datasets. \r\n\r\nBy default, the `aws` is deactivated and `local` is activated and `slow` is deactivated, so that only 1 test per dataset runs on circle ci. \r\n\r\n**When local is activated all folders in `./datasets` are tested.**\r\n\r\n**Important** When adding a dataset, we should no longer upload it to AWS. The steps are:\r\n1. Open a PR\r\n2. Add a dataset as described in `datasets/README.md`\r\n3. If all tests pass, push to master\r\n\r\nCurrently we have 49 functional datasets in our code base. \r\n\r\nWe have 6 datasets \"under-construction\" that don't pass the tests - so I put them in a folder \"datasets_under_construction\" - it would be nice to open a PR to fix them and put them in the `datasets` folder.\r\n\r\n**Important** when running tests locally, the datasets are cached so to rerun them delete your local cache via:\r\n`rm -r ~/.cache/huggingface/datasets/*` \r\n\r\n@thomwolf @mariamabarham @lhoestq "
    },
    {
      "id": "290",
      "title": "Adding CoNLLpp dataset.",
      "body": ""
    },
    {
      "id": "291",
      "title": "Add GLUE Compat (compatible with transformers<3.5.0)",
      "body": "Link to our discussion on Slack (HF internal)\r\nhttps://huggingface.slack.com/archives/C014N4749J9/p1609668119337400\r\n\r\nThe next step is to add a compatible option in the new `run_glue.py`\r\n\r\nI duplicated `glue` and made the following changes:\r\n1. Change the name to `glue_compat`.\r\n2. Change the label assignments for MNLI and AX."
    },
    {
      "id": "292",
      "title": "Update README.md",
      "body": "Added information in the dataset card"
    },
    {
      "id": "293",
      "title": "Adding support for  generic multi dimensional tensors and auxillary image data for multimodal datasets",
      "body": "nlp/features.py:\r\n\r\nThe main factory class is MultiArray, every single time this class is called, a corresponding pyarrow extension array and type class is generated (and added to the list of globals for future use) for a given root data type and set of dimensions/shape. I provide examples on working with this in datasets/lxmert_pretraining_beta/test_multi_array.py\r\n\r\nsrc/nlp/arrow_writer.py\r\n\r\nI had to add a method for writing batches that include extension array types because despite having a unique class for each multidimensional array shape, pyarrow is unable to write any other \"array-like\" data class to a batch object unless it is of the type pyarrow.ExtensionType. The problem in this is that when writing multiple batches, the order of the schema and data to be written get mixed up (where the pyarrow datatype in the schema only refers to as ExtensionAray, but each ExtensionArray subclass has a different shape) ...  possibly I am missing something here and would be grateful if anyo"
    },
    {
      "id": "294",
      "title": "Added glucose dataset",
      "body": "This PR adds the [Glucose](https://github.com/ElementalCognition/glucose) dataset."
    },
    {
      "id": "295",
      "title": "Dataset browser url is still https://huggingface.co/nlp/viewer/",
      "body": "Might be worth updating to https://huggingface.co/datasets/viewer/"
    },
    {
      "id": "296",
      "title": "Add Mkqa dataset",
      "body": "# MKQA: Multilingual Knowledge Questions & Answers Dataset\r\nAdding the [MKQA](https://github.com/apple/ml-mkqa) dataset as part of the sprint ðŸŽ‰\r\n\r\nThere is no official data splits so I added just a `train` split.\r\n \r\ndifferently from the original:\r\n- answer:type field is a ClassLabel (I thought it might be possible to train on this as a label for categorizing questions)\r\n- answer:entity field has a default value of empty string '' (since this key is not available for all in original)\r\n- answer:alias has default value of []\r\n\r\n- [x] All tests passed\r\n- [x] Added dummy data\r\n- [x] Added data card (as much as I could)\r\n"
    },
    {
      "id": "297",
      "title": "Fix squad V2 metric script",
      "body": "The current squad v2 metric doesn't work with the squad (v1 or v2) datasets. The script is copied from `squad_evaluate` in transformers that requires the labels (with multiple answers) to be like this:\r\n```\r\nreferences = [{'id': 'a', 'answers': [\r\n    {'text': 'Denver Broncos', 'answer_start': 177},\r\n    {'text': 'Denver Broncos', 'answer_start': 177}\r\n]}]\r\n```\r\nwhile the dataset had references like this:\r\n```\r\nreferences = [{'id': 'a', 'answers': \r\n    {'text': ['Denver Broncos' 'Denver Broncos'], 'answer_start': [177, 177]}\r\n}]\r\n```\r\n\r\nUsing one or the other format fails with the current squad v2 metric:\r\n```\r\nfrom datasets import load_metric\r\nmetric = load_metric(\"squad_v2\")\r\npredictions = [{'id': 'a', 'prediction_text': 'Denver Broncos', 'no_answer_probability': 0.0}]\r\nreferences = [{'id': 'a', 'answers': [\r\n    {'text': 'Denver Broncos', 'answer_start': 177},\r\n    {'text': 'Denver Broncos', 'answer_start': 177}\r\n]}]\r\nmetric.compute(predictions=predictions, references=references)\r\n"
    },
    {
      "id": "298",
      "title": "dataset adversarial_qa has no answers in the \"test\" set",
      "body": "## Describe the bug\r\nWhen loading the adversarial_qa dataset the 'test' portion has no answers.  Only the 'train' and 'validation' portions do.  This occurs with all four of the configs ('adversarialQA', 'dbidaf', 'dbert', 'droberta')\r\n\r\n## Steps to reproduce the bug\r\n```\r\nfrom   datasets import load_dataset\r\nexamples = load_dataset('adversarial_qa', 'adversarialQA', script_version=\"master\")['test']\r\nprint('Loaded {:,} examples'.format(len(examples)))\r\nhas_answers = 0\r\nfor e in examples:\r\n    if e['answers']['text']:\r\n        has_answers += 1\r\nprint('{:,} have answers'.format(has_answers))\r\n>>> Loaded 3,000 examples\r\n>>> 0 have answers\r\n\r\nexamples = load_dataset('adversarial_qa', 'adversarialQA', script_version=\"master\")['validation']\r\n<...code above...>\r\n>>> Loaded 3,000 examples\r\n>>> 3,000 have answers\r\n```\r\n\r\n## Expected results\r\nIf 'test' is a valid dataset, it should have answers. Also note that all of the 'train' and 'validation' sets have answers, there are no \"no answer\" questi"
    },
    {
      "id": "299",
      "title": "New instruction for how to generate dataset_infos.json",
      "body": "Add additional instructions for how to generate dataset_infos.json for manual download datasets. Information courtesy of `Taimur Ibrahim` from the slack channel"
    },
    {
      "id": "300",
      "title": "Improve torch formatting performance",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nIt would be great, if possible, to further improve read performance of raw encoded datasets and their subsequent conversion to torch tensors. \r\n\r\nA bit more background.  I am working on LM pre-training using HF ecosystem. We use encoded HF Wikipedia and BookCorpus datasets. The training machines are similar to DGX-1 workstations. We use HF trainer torch.distributed training approach on a single machine with 8 GPUs.\r\n\r\nThe current performance is about 30% slower than NVidia optimized BERT [examples](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling) baseline. Quite a bit of customized code and training loop tricks were used to achieve the baseline performance. It would be great to achieve the same performance while using nothing more than off the shelf HF ecosystem. Perhaps, in the future, with @stas00 work on deepspeed integration, it could even be exceeded. \r\n\r\n**Describe the solution"
    },
    {
      "id": "301",
      "title": "fixes and improvements for the WebNLG loader",
      "body": "- fixes test sets loading in v3.0\r\n- adds additional fields for v3.0_ru\r\n- adds info to the WebNLG data card"
    },
    {
      "id": "302",
      "title": "feat: ðŸŽ¸ add a function to get a dataset config's split names",
      "body": "Also: pass additional arguments (use_auth_token) to get private configs + info of private datasets on the hub\r\n\r\nQuestions:\r\n\r\n- <strike>I'm not sure how the versions work: I changed 1.12.1.dev0 to 1.12.1.dev1, was it correct?</strike> no -> reverted\r\n- Should I add a section in https://github.com/huggingface/datasets/blob/master/docs/source/load_hub.rst? (there is no section for get_dataset_infos)"
    },
    {
      "id": "303",
      "title": "Update README.md",
      "body": "Adding relevant citations (paper accepted at AAAI 2020 & EMNLP 2020) to the benchmark"
    },
    {
      "id": "304",
      "title": "added file_permission in load_dataset",
      "body": "As discussed in #2065 I've added `file_permission` argument in `load_dataset`. \r\n\r\nAdded mainly 2 things here:\r\n1) Permission of downloaded datasets when converted to .arrow files  can be changed with argument `file_permission` argument in `load_dataset` (default is 0o644 only)\r\n2) Incase the user uses `map` later on to generate another cache file of dataset, it ensures the permissions of newly generated file are similar to that of` *-train.arrow` file inside cache_dir for that dataset."
    },
    {
      "id": "305",
      "title": "[load_dataset] shard and parallelize the process",
      "body": "- Some huge datasets take forever to build the first time. (e.g. oscar/en) as it's done in a single cpu core.\r\n- If the build crashes, everything done up to that point gets lost\r\n\r\nRequest: Shard the build over multiple arrow files, which would enable:\r\n- much faster build by parallelizing the build process\r\n- if the process crashed, the completed arrow files don't need to be re-built again\r\n\r\nThank you!\r\n\r\n@lhoestq "
    },
    {
      "id": "306",
      "title": "Add twi text",
      "body": "Add Twi texts"
    },
    {
      "id": "307",
      "title": "Add tests datasets gcp",
      "body": "Some datasets are available on our google cloud storage in arrow format, so that the users don't need to process the data.\r\nThese tests make sure that they're always available. It also makes sure that their scripts are in sync between S3 and the repo.\r\nThis should avoid future issues like #407 "
    },
    {
      "id": "308",
      "title": "Add Jigsaw unintended Bias",
      "body": "Hi,\r\n\r\nHere's a first attempt at this dataset. Would be great if it could be merged relatively quickly as it is needed for Bigscience-related stuff. \r\n\r\nThis requires manual download, and I had some trouble generating dummy_data in this setting, so welcoming feedback there."
    },
    {
      "id": "309",
      "title": "Add imagefolder dataset",
      "body": "A generic imagefolder dataset inspired by `torchvision.datasets.ImageFolder`. \r\n\r\nResolves #2508 \r\n\r\n---\r\n\r\nExample Usage:\r\n\r\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/gist/nateraw/954fa8cba4ff806f6147a782fa9efd1a/imagefolder-official-example.ipynb)"
    },
    {
      "id": "310",
      "title": "Field order issue in loading json",
      "body": "## Describe the bug\r\nThe `load_dataset` function expects columns in alphabetical order when loading json files.\r\n\r\nSimilar bug was previously reported for csv in #623 and fixed in #684.\r\n## Steps to reproduce the bug\r\n\r\nFor a json file `j.json`,\r\n```\r\n{\"c\":321, \"a\": 1, \"b\": 2}\r\n```\r\nRunning the following,\r\n```\r\nf= datasets.Features({'a': Value('int32'), 'b': Value('int32'), 'c': Value('int32')})\r\njson_data = datasets.load_dataset('json', data_files='j.json', features=f)\r\n```\r\n\r\n\r\n## Expected results\r\nA successful load.\r\n## Actual results\r\n```\r\nFile \"pyarrow/table.pxi\", line 1409, in pyarrow.lib.Table.cast\r\nValueError: Target schema's field names are not matching the table's field names: ['c', 'a', 'b'], ['a', 'b', 'c']\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.8.0\r\n- Platform: Linux-3.10.0-957.1.3.el7.x86_64-x86_64-with-glibc2.10\r\n- Python version: 3.8.8\r\n- PyArrow version: 3.0.0\r\n\r\n"
    },
    {
      "id": "311",
      "title": "Fix BLEURT metrics for backward compatibility",
      "body": "Fix #565"
    },
    {
      "id": "312",
      "title": "test data added, dataset_infos updated",
      "body": "Fixes #2262. Thanks for pointing out issue with dataset @jinmang2!"
    },
    {
      "id": "313",
      "title": "Fix text delimiter",
      "body": "I changed the delimiter in the `text` dataset script.\r\nIt should fix the `pyarrow.lib.ArrowInvalid: CSV parse error` from #622 \r\n\r\nI changed the delimiter to an unused ascii character that is not present in text files : `\\b`"
    },
    {
      "id": "314",
      "title": "Add OfisPublik Dataset",
      "body": ""
    },
    {
      "id": "315",
      "title": "adding progress bar / ETA for `load_dataset`",
      "body": "Please consider:\r\n```\r\nDownloading and preparing dataset oscar/unshuffled_deduplicated_en (download: 462.40 GiB, generated: 1.18 TiB, post-processed: Unknown size, total: 1.63 TiB) to cache/oscar/unshuffled_deduplicated_en/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2...\r\nHF google storage unreachable. Downloading and preparing it from source\r\n```\r\nand no indication whatsoever of whether things work well or when it'll be done. It's important to have an estimated completion time for when doing slurm jobs since some instances have a cap on run-time.\r\n\r\nI think for this particular job it sat for 30min in total silence and then after 30min it started generating:\r\n```\r\n897850 examples [07:24, 10286.71 examples/s]\r\n```\r\nwhich is already great!\r\n\r\nRequest: \r\n1. ETA - knowing how many hours to allocate for a slurm job\r\n2. progress bar - helps to know things are working and aren't stuck and where we are at.\r\n\r\nThank you!\r\n\r\n@lhoestq \r\n"
    },
    {
      "id": "316",
      "title": "Add TLC",
      "body": "Added TLC dataset"
    },
    {
      "id": "317",
      "title": "`yelp_polarity` is broken",
      "body": "![image](https://user-images.githubusercontent.com/22514219/120828150-c4a35b00-c58e-11eb-8083-a537cee4dbb3.png)\r\n"
    },
    {
      "id": "318",
      "title": "First commit of NarrativeQA Dataset",
      "body": "Added NarrativeQA dataset and included a manual downloading option to download scripts from the original scripts provided by the authors. "
    },
    {
      "id": "319",
      "title": "Fix unwanted tqdm bar when accessing examples",
      "body": "A change in #2814 added bad progress bars in `map_nested`. Now they're disabled by default\r\n\r\nFix #2919 "
    },
    {
      "id": "320",
      "title": "Enum used in map functions will raise a RecursionError with dill.",
      "body": "## Describe the bug\r\n\r\nEnums used in functions pass to `map` will fail at pickling with a maximum recursion exception as described here: https://github.com/uqfoundation/dill/issues/250#issuecomment-852566284\r\n\r\nIn my particular case, I use an enum to define an argument with fixed options using the `TraininigArguments` dataclass as base class and the `HfArgumentParser`. In the same file I use a `ds.map` that tries to pickle the content of the module including the definition of the enum that runs into the dill bug described above.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nfrom enum import Enum\r\n\r\nclass A(Enum):\r\n    a = 'a'\r\n\r\ndef main():\r\n    a = A.a\r\n    \r\n    def f(x):\r\n        return {} if a == a.a else x\r\n    \r\n    ds = load_dataset('cnn_dailymail', '3.0.0')['test']\r\n    ds = ds.map(f, num_proc=15)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n## Expected results\r\nThe known problem with dill could be prevented as explained in the link above "
    },
    {
      "id": "321",
      "title": "Fix bug in to_tf_dataset",
      "body": "Replace `set_format()` to `with_format()` so that we don't alter the original dataset in `to_tf_dataset()`"
    },
    {
      "id": "322",
      "title": "Remove unused head_hf_s3 function",
      "body": "Currently, the function `head_hf_s3` is not used:\r\n- neither its returned result is used\r\n- nor it raises any exception, as exceptions are catched and returned (not raised)\r\n\r\nThis PR removes it."
    },
    {
      "id": "323",
      "title": "Better message when data files is empty",
      "body": "Fix #581 "
    },
    {
      "id": "324",
      "title": "How can I compute BLEU-4 score use `load_metric` ?",
      "body": "I have found the sacrebleu metric. But, I do not know the difference between it and BLEU-4.\r\nIf I want to compute BLEU-4 score, what can i do?"
    },
    {
      "id": "325",
      "title": "UM005: Urdu <> English Translation Dataset",
      "body": "Adds Urdu-English dataset for machine translation: http://ufal.ms.mff.cuni.cz/umc/005-en-ur/"
    },
    {
      "id": "326",
      "title": "[Feature request] Add `shard()` method to dataset",
      "body": "Currently, to shard a dataset into 10 pieces on different ranks, you can run\r\n\r\n```python\r\nrank = 3 # for example\r\nsize = 10\r\ndataset = nlp.load_dataset('wikitext', 'wikitext-2-raw-v1', split=f\"train[{rank*10}%:{(rank+1)*10}%]\")\r\n```\r\n\r\nHowever, this breaks down if you have a number of ranks that doesn't divide cleanly into 100, such as 64 ranks. Is there interest in adding a method shard() that looks like this?\r\n\r\n```python\r\nrank = 3\r\nsize = 64\r\ndataset = nlp.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\").shard(rank=rank, size=size)\r\n```\r\n\r\nTensorFlow has a similar API: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shard. I'd be happy to contribute this code."
    },
    {
      "id": "327",
      "title": "DOCS: Fix typo",
      "body": "Fix typo from dictionnary -> dictionary"
    },
    {
      "id": "328",
      "title": "[Refactor] Use in-memory/memory-mapped/concatenation tables in Dataset",
      "body": "## Intro\r\n\r\nCurrently there is one assumption that we need to change: a dataset is either fully in memory (dataset._data_files is empty), or the dataset can be reloaded from disk with memory mapping (using the dataset._data_files).\r\nThis assumption is used for pickling for example:\r\n- in-memory dataset can just be pickled/unpickled in-memory\r\n- on-disk dataset can be unloaded to only keep the filepaths when pickling, and then reloaded from the disk when unpickling\r\n\r\n## Issues\r\n\r\nBecause of this assumption, we can't easily implement methods like `Dataset.add_item` to append more rows to a dataset, or `dataset.add_column` to add a column, since we can't mix data from memory and data from the disk.\r\nMoreover, `concatenate_datasets` doesn't work if the datasets to concatenate are not all from memory, or all form the disk.\r\n\r\n## Solution provided in this PR\r\n\r\nI changed this by allowing several types of Table to be used in the Dataset object.\r\nMore specifically I added three pyarrow Table "
    },
    {
      "id": "329",
      "title": "Add custom dataset to NLP?",
      "body": "Is it possible to add a custom dataset such as a .csv to the NLP library?\r\n\r\nThanks."
    },
    {
      "id": "330",
      "title": "Add ToTTo Dataset",
      "body": "Adds a brand new table to text dataset: https://github.com/google-research-datasets/ToTTo"
    },
    {
      "id": "331",
      "title": "add thainer",
      "body": "ThaiNER (v1.3) is a 6,456-sentence named entity recognition dataset created from expanding the 2,258-sentence\r\n[unnamed dataset](http://pioneer.chula.ac.th/~awirote/Data-Nutcha.zip) by\r\n[Tirasaroj and Aroonmanakun (2012)](http://pioneer.chula.ac.th/~awirote/publications/).\r\nIt is used to train NER taggers in [PyThaiNLP](https://github.com/PyThaiNLP/pythainlp).\r\nThe NER tags are annotated by [Tirasaroj and Aroonmanakun (2012)]((http://pioneer.chula.ac.th/~awirote/publications/))\r\nfor 2,258 sentences and the rest by [@wannaphong](https://github.com/wannaphong/).\r\nThe POS tags are done by [PyThaiNLP](https://github.com/PyThaiNLP/pythainlp)'s `perceptron` engine trained on `orchid_ud`.\r\n[@wannaphong](https://github.com/wannaphong/) is now the only maintainer of this dataset."
    },
    {
      "id": "332",
      "title": "Adding TamilMixSentiment",
      "body": ""
    },
    {
      "id": "333",
      "title": "Add GigaFren Dataset",
      "body": ""
    },
    {
      "id": "334",
      "title": "Fixed typo  seperate->separate",
      "body": ""
    },
    {
      "id": "335",
      "title": "Fix sacrebleu tokenizers",
      "body": "Last `sacrebleu` release (v2.0.0) has removed `sacrebleu.TOKENIZERS`: https://github.com/mjpost/sacrebleu/pull/152/files#diff-2553a315bb1f7e68c9c1b00d56eaeb74f5205aeb3a189bc3e527b122c6078795L17-R15\r\n\r\nThis PR makes a hot fix of the bug by using a private function in `sacrebleu`: `sacrebleu.metrics.bleu._get_tokenizer()`.\r\n\r\nEventually, this should be further fixed in order to use only public functions.\r\n\r\nThis is a partial hotfix of #2781."
    },
    {
      "id": "336",
      "title": "prepare_module issue when loading from read-only fs",
      "body": "## Describe the bug\r\n\r\nWhen we use prepare_module from a readonly file system, we create a FileLock using the `local_path`.\r\nThis path is not necessarily writable.\r\n\r\n`lock_path = local_path + \".lock\"`\r\n\r\n\r\n## Steps to reproduce the bug\r\n\r\nRun `load_dataset` on a readonly python loader file.\r\n```python\r\nds = load_dataset(\r\n        python_loader, data_files={\"train\": train_path, \"test\": test_path}\r\n    )\r\n```\r\n\r\nwhere `python_loader` is a path to a file located in a readonly folder.\r\n\r\n## Expected results\r\nThis should work I think?\r\n\r\n## Actual results\r\n\r\n```python\r\n    return load_dataset(\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/load.py\", line 711, in load_dataset\r\n    module_path, hash, resolved_file_path = prepare_module(\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/load.py\", line 465, in prepare_module\r\n    with FileLock(lock_path):\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/utils/filelock.py\", line 314, in __enter__\r\n    self.acquire()\r\n  "
    },
    {
      "id": "337",
      "title": "Adding a second branch for Atomic to fix git errors",
      "body": "Adding the Atomic common sense dataset.\r\nSee https://homes.cs.washington.edu/~msap/atomic/"
    },
    {
      "id": "338",
      "title": "Fix type hints pickling in python 3.6",
      "body": "Type hints can't be properly pickled in python 3.6. This was causing errors the `run_mlm.py` script from `transformers` with python 3.6\r\n\r\nHowever Cloupickle proposed a [fix](https://github.com/cloudpipe/cloudpickle/pull/318/files) to make it work anyway.\r\nThe idea is just to implement the pickling/unpickling of parameterized type hints. There is one detail though: since in python 3.6 we can't use `isinstance` on type hints, then we can't use pickle saving functions registry directly. Therefore we just wrap the `save_global` method of the Pickler.\r\n\r\nThis should fix https://github.com/huggingface/transformers/issues/8212 for python 3.6 and make `run_mlm.py` support python 3.6\r\n\r\ncc @sgugger "
    },
    {
      "id": "339",
      "title": "break statement not required",
      "body": ""
    },
    {
      "id": "340",
      "title": "Add SciTLDR Dataset (Take 2)",
      "body": "Adds the SciTLDR Dataset by AI2\r\nAdded the `README.md` card with tags to the best of my knowledge\r\n\r\nMulti-target summaries or TLDRs of Scientific Documents\r\n\r\nContinued from #986 "
    },
    {
      "id": "341",
      "title": "Remove import of transformers",
      "body": "When pickling a tokenizer within multiprocessing, check that is instance of transformers PreTrainedTokenizerBase without importing transformers.\r\n\r\nRelated to huggingface/transformers#12549 and #502."
    },
    {
      "id": "342",
      "title": "CompGuessWhat?! 0.2.0",
      "body": "We updated some metadata information associated with the dataset. In addition, we've updated the `create_dummy_data.py` script to generate data samples for the dataset. "
    },
    {
      "id": "343",
      "title": "Coc",
      "body": "## Adding a Dataset\n- **Name:** *name of the dataset*\n- **Description:** *short description of the dataset (or link to social media or blog post)*\n- **Paper:** *link to the dataset paper if available*\n- **Data:** *link to the Github repository or current dataset location*\n- **Motivation:** *what are some good reasons to have this dataset*\n\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md)."
    },
    {
      "id": "344",
      "title": "adding covid-tweets-japanese",
      "body": "Adding COVID-19 Japanese Tweets Dataset as part of the sprint.\r\n\r\nTesting with dummy data is not working (the file is said to not exist). Sorry for the incomplete PR."
    },
    {
      "id": "345",
      "title": "Bug Report: timestamp[ns] not recognized",
      "body": "Repro:\r\n\r\n```\r\nfrom datasets import Dataset\r\nimport pandas as pd\r\nimport pyarrow\r\n\r\ndf = pd.DataFrame(pd.date_range(\"2018-01-01\", periods=3, freq=\"H\"))\r\npyarrow.Table.from_pandas(df)\r\nDataset.from_pandas(df)\r\n# Throws ValueError: Neither timestamp[ns] nor timestamp[ns]_ seems to be a pyarrow data type.\r\n```\r\n\r\nThe factory function seems to be just \"timestamp\": https://arrow.apache.org/docs/python/generated/pyarrow.timestamp.html#pyarrow.timestamp\r\n\r\nIt seems like https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L36-L43 could have a little bit of additional structure for handling these cases?  I'd be happy to take a shot at opening a PR if I could receive some guidance on whether parsing something like `timestamp[ns]` and resolving it to timestamp('ns') is the goal of this method.\r\n\r\nAlternatively, if I'm using this incorrectly (e.g. is the expectation that we always provide a schema when timestamps are involved?), that would be very helpful to know as well!\r"
    },
    {
      "id": "346",
      "title": "remove wi_locness dataset due to licensing issues",
      "body": "It was brought to my attention that this dataset's license is not only missing, but also prohibits redistribution. I contacted the original author to apologize for this oversight and asked if we could still use it, but unfortunately we can't and the author kindly asked to take down this dataset."
    },
    {
      "id": "347",
      "title": "JAX integration",
      "body": "Hi !\r\n\r\nI just added the \"jax\" formatting, as we already have for pytorch, tensorflow, numpy (and also pandas and arrow).\r\nIt does pretty much the same thing as the pytorch formatter except it creates jax.numpy.ndarray objects.\r\n\r\n```python\r\nfrom datasets import Dataset\r\n\r\nd = Dataset.from_dict({\"foo\": [[0., 1., 2.]]})\r\nd = d.with_format(\"jax\")\r\nd[0]\r\n# {'foo': DeviceArray([0., 1., 2.], dtype=float32)}\r\n```\r\n\r\nA few details:\r\n- The default integer precision for jax depends on the jax configuration `jax_enable_x64` (see [here](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#double-64bit-precision)), I took that into account. Unless `jax_enable_x64` is specified, it is int32 by default\r\n- AFAIK it's not possible to do a full conversion from arrow data to jax data. We are doing arrow -> numpy -> jax but the numpy -> jax part doesn't do zero copy unfortutanely (see [here](https://github.com/google/jax/issues/4486))\r\n- the env var for disabling JAX is `USE_JAX`. Ho"
    },
    {
      "id": "348",
      "title": "Cached dataset overflowing disk space",
      "body": "I'm training a Swedish Wav2vec2 model on a Linux GPU and having issues that the huggingface cached dataset folder is completely filling up my disk space (I'm training on a dataset of around 500 gb).\r\n\r\nThe cache folder is 500gb (and now my disk space is full).\r\n\r\nIs there a way to toggle caching or set the caching to be stored on a different device (I have another drive with 4 tb that could hold the caching files).\r\n\r\nThis might not technically be a bug, but I was unsure and I felt that the bug was the closest one.\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/birger/miniconda3/envs/wav2vec2/lib/python3.7/site-packages/multiprocess/pool.py\", line 121, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"/home/birger/miniconda3/envs/wav2vec2/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 186, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/birger/miniconda3/envs/wav2vec2/lib/python3.7/site-packages/datasets/f"
    },
    {
      "id": "349",
      "title": "Add dataset post processing for faiss indexes",
      "body": "# Post processing of datasets for faiss indexes\r\n\r\nNow that we can have datasets with embeddings (see `wiki_pr` for example), we can allow users to load the dataset + get the Faiss index that comes with it to do nearest neighbors queries.\r\n\r\n## Implementation proposition\r\n\r\n- Faiss indexes have to be added to the `nlp.Dataset` object, and therefore it's in a different scope that what are doing the `_split_generators` and `_generate_examples` methods of `nlp.DatasetBuilder`. Therefore I added a new method for post processing of the `nlp.Dataset` object called `_post_process` (name could change)\r\n- The role of `_post_process` is to apply dataset transforms (filter/map etc.) or indexing functions (add_faiss_index) to modify/enrich the `nlp.Dataset` object. It is not part of the `download_and_prepare` process (that is focused on arrow files creation) so the post processing is run inside the `as_dataset` method.\r\n- `_post_process` can generate new files (cached files from dataset transforms"
    },
    {
      "id": "350",
      "title": "Filter expected warning log from transformers",
      "body": "Close #2569."
    },
    {
      "id": "351",
      "title": "Made `share_dataset` more readable",
      "body": ""
    },
    {
      "id": "352",
      "title": "Refactoring: Create config module",
      "body": "Refactorize configuration settings into their own module.\r\n\r\nThis could be seen as a Pythonic singleton-like approach. Eventually a config instance class might be created."
    },
    {
      "id": "353",
      "title": "Dataset Streaming",
      "body": "# Dataset Streaming\r\n\r\n## API\r\n\r\nCurrent API is\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\n# Load an IterableDataset without downloading data\r\nsnli = load_dataset(\"snli\", streaming=True)\r\n\r\n# Access examples by streaming data\r\nprint(next(iter(snli[\"train\"]))) \r\n# {'premise': 'A person on a horse jumps over a broken down airplane.',\r\n#  'hypothesis': 'A person is training his horse for a competition.',\r\n#  'label': 1}\r\n```\r\n\r\nI already implemented a few methods:\r\n- IterableDataset.map: apply transforms on-the-fly to the examples\r\n- IterableDataset.shuffle: shuffle the data _a la_ TFDS, i.e. with a shuffling buffer\r\n- IterableDataset.with_format: set the format to `\"torch\"` to get a `torch.utils.data.IterableDataset`\r\n- merge_datasets: merge two iterable datasets by alternating one or the other (you can specify the probabilities)\r\n\r\nI would love to have your opinion on the API design :)\r\n\r\n## Implementation details\r\n\r\n### Streaming\r\n\r\nData streaming is done using `fsspec` which h"
    },
    {
      "id": "354",
      "title": "from datasets import Dataset is failing ",
      "body": "## Describe the bug\r\nA clear and concise description of what the bug is.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nfrom datasets import Dataset\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py in <module>()\r\n     25 import posixpath\r\n     26 import requests\r\n---> 27 from tqdm.contrib.concurrent import thread_map\r\n     28 \r\n     29 from .. import __version__, config, utils\r\n\r\nModuleNotFoundError: No module named 'tqdm.contrib.concurrent'\r\n\r\n---------------------------------------------------------------------------\r\nNOTE: If your import is failing due to a missing package, you can\r\nmanually install dependencies using either !pip or !apt.\r\n\r\nTo view examples of installing some common dependencies, click the\r\n\"Open Examples\" button below.\r\n---------------------------------------------"
    },
    {
      "id": "355",
      "title": "Rearrange JSON field names to match passed features schema field names",
      "body": "This PR depends on PR #2453 (which must be merged first).\r\n\r\nClose #2366."
    },
    {
      "id": "356",
      "title": "Fix print statement in READ.md",
      "body": "print statement was throwing generator object instead of printing names of available datasets/metrics"
    },
    {
      "id": "357",
      "title": "Issues when run two programs compute the same metrics",
      "body": "I got the following error when running two different programs that both compute sacreblue metrics. It seems that both read/and/write to the same location (.cache/huggingface/metrics/sacrebleu/default/default_experiment-1-0.arrow) where it caches the batches:\r\n\r\n```\r\nFile \"train_matching_min.py\", line 160, in <module>ch_9_label\r\n    avg_loss = valid(epoch, args.batch, args.validation, args.with_label)\r\n  File \"train_matching_min.py\", line 93, in valid\r\n    bleu += eval.compute()\r\n  File \"/u/tlhoang/projects/seal/match/models/eval.py\", line 23, in compute\r\n    return self.metric.compute()['score']\r\n  File \"/dccstor/know/anaconda3/lib/python3.7/site-packages/datasets/metric.py\", line 387, in compute\r\n    self._finalize()\r\n  File \"/dccstor/know/anaconda3/lib/python3.7/site-packages/datasets/metric.py\", line 355, in _finalize\r\n    self.data = Dataset(**reader.read_files([{\"filename\": f} for f in file_paths]))\r\n  File \"/dccstor/know/anaconda3/lib/python3.7/site-packages/datasets/arrow_reader"
    },
    {
      "id": "358",
      "title": "Datasets library not suitable for huge text datasets.",
      "body": "Hi,\r\n\r\nI'm trying to use datasets library to load a 187GB dataset of pure text, with the intention of building a Language Model. The problem is that from the 187GB it goes to some TB when processed by Datasets. First of all, I think the pre-tokenizing step (with tokenizer.map()) is not really thought for datasets this big, but for fine-tuning datasets, as this process alone takes so much time, usually in expensive machines (due to the need of tpus - gpus) which is not being used for training. It would possibly be more efficient in such cases to tokenize each batch at training time (receive batch - tokenize batch - train with batch), so that the whole time the machine is up it's being used for training. \r\nMoreover, the pyarrow objects created from a 187 GB datasets are huge, I mean, we always receive OOM, or No Space left on device errors when only 10-12% of the dataset has been processed, and only that part occupies 2.1TB in disk, which is so many times the disk  usage of the pure text"
    },
    {
      "id": "359",
      "title": "add Toronto Books Corpus",
      "body": "This PR adds the Toronto Books Corpus.\r\n.\r\nIt on consider TMX and plain text files (Moses) defined in the table **Statistics and TMX/Moses Downloads** [here](http://opus.nlpl.eu/Books.php )"
    },
    {
      "id": "360",
      "title": "Add Open Subtitles Dataset",
      "body": ""
    },
    {
      "id": "361",
      "title": "Apply utf-8 encoding to all datasets",
      "body": "## Description\r\nThis PR applies utf-8 encoding for all instances of `with open(...) as f` to all Python files in `datasets/`. As suggested by @thomwolf in #468 , we use regular expressions and the following function\r\n\r\n```python\r\ndef apply_encoding_on_file_open(filepath: str):\r\n    \"\"\"Apply UTF-8 encoding for all instances where a non-binary file is opened.\"\"\"\r\n    \r\n    with open(filepath, 'r', encoding='utf-8') as input_file:\r\n        regexp = re.compile(r\"(?!.*\\b(?:encoding|rb|w|wb|w+|wb+|ab|ab+)\\b)(?<=\\s)(open)\\((.*)\\)\")\r\n        input_text = input_file.read()\r\n        match = regexp.search(input_text)\r\n        \r\n        if match:\r\n            output = regexp.sub(lambda m: m.group()[:-1]+', encoding=\"utf-8\")', input_text)\r\n            with open(filepath, 'w', encoding='utf-8') as output_file:\r\n                output_file.write(output)\r\n```\r\n\r\nto perform the replacement. \r\n\r\nNote:\r\n\r\n1. I excluded all _**binary files**_ from the search since it's possible some objects are opened for"
    },
    {
      "id": "362",
      "title": "[Feature] Keep the list of labels of a dataset as metadata",
      "body": "It would be useful to keep the list of the labels of a dataset as metadata. Either directly in the `DatasetInfo` or in the Arrow metadata."
    },
    {
      "id": "363",
      "title": "Added the WikiText-TL39 dataset and corresponding card",
      "body": "This PR adds the WikiText-TL-39 Filipino Language Modeling dataset. Restarted a new pull request since there were problems with the earlier one.\r\n\r\nPaper: https://arxiv.org/abs/1907.00409\r\nRepo: https://github.com/jcblaisecruz02/Filipino-Text-Benchmarks"
    },
    {
      "id": "364",
      "title": "Adding an Elastic Search index to a Dataset",
      "body": "## Describe the bug\r\nWhen trying to index documents from the squad dataset, the connection to ElasticSearch seems to break:\r\n\r\nReusing dataset squad (/Users/andreasmotz/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\r\n 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 9501/10570 [00:01<00:00, 6335.61docs/s]\r\n\r\nNo error is thrown, but the indexing breaks ~90%.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nfrom datasets import load_dataset\r\nfrom elasticsearch import Elasticsearch\r\nes = Elasticsearch()\r\nsquad = load_dataset('squad', split='validation')\r\nindex_name = \"corpus\"\r\nes_config = {\r\n    \"settings\": {\r\n        \"number_of_shards\": 1,\r\n        \"analysis\": {\"analyzer\": {\"stop_standard\": {\"type\": \"standard\", \" stopwords\": \"_english_\"}}},\r\n    },\r\n    \"mappings\": {\r\n        \"properties\": {\r\n            \"idx\" : {\"type\" : \"keyword\"},\r\n            \"title\" : {\"type\" : \"keyword\"},\r\n "
    },
    {
      "id": "365",
      "title": "[Dataset] RACE dataset Checksums error",
      "body": "Hi there, I just would like to use this awesome lib to perform a dataset fine-tuning on RACE dataset. I have performed the following steps:\r\n\r\n```\r\ndataset = nlp.load_dataset(\"race\")\r\nlen(dataset[\"train\"]), len(dataset[\"validation\"])\r\n```\r\n\r\nBut then I got the following error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n<ipython-input-15-8bf7603ce0ed> in <module>\r\n----> 1 dataset = nlp.load_dataset(\"race\")\r\n      2 len(dataset[\"train\"]), len(dataset[\"validation\"])\r\n\r\n~/miniconda3/envs/masters/lib/python3.8/site-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    546 \r\n    547     # Download and prepare data\r\n--> 548     builder_instance.download_and_prepare(\r\n    549         download_config=download_config, download_mode=downloa"
    },
    {
      "id": "366",
      "title": "[feature request] adding easy to remember `datasets.cache_dataset()` + `datasets.is_dataset_cached()`",
      "body": "Often, there is a need to prepare a dataset but not use it immediately, e.g. think tests suite setup, so it'd be really useful to be able to do:\r\n\r\n``` \r\nif not datasets.is_dataset_cached(ds): datasets.cache_dataset(ds)\r\n```\r\n\r\nThis can already be done with:\r\n```\r\nbuilder = load_dataset_builder(ds)\r\nif not os.path.idsir(builder.cache_dir):\r\n    builder.download_and_prepare()\r\n```\r\n\r\nbut the current way is a way less intuitive and much harder to remember than the proposed API, IMHO. \r\n\r\nOne more way is to do:\r\n\r\n```\r\n_ = load_dataset(ds)\r\n```\r\nbut it wastes resources loading the dataset when it's not needed.\r\n\r\nthis has been discussed at https://huggingface.slack.com/archives/C01229B19EX/p1630021912025800\r\n\r\nThank you!\r\n\r\n@lhoestq \r\n"
    },
    {
      "id": "367",
      "title": "Fix ade_corpus_v2 config names",
      "body": "There are currently some typos in the config names of the `ade_corpus_v2` dataset, I fixed them:\r\n\r\n- Ade_corpos_v2_classificaion -> Ade_corpus_v2_classification\r\n- Ade_corpos_v2_drug_ade_relation -> Ade_corpus_v2_drug_ade_relation\r\n- Ade_corpos_v2_drug_dosage_relation -> Ade_corpus_v2_drug_dosage_relation"
    },
    {
      "id": "368",
      "title": "Multi-task dataset mixing",
      "body": "It seems like many of the best performing models on the GLUE benchmark make some use of multitask learning (simultaneous training on multiple tasks).\r\n\r\nThe [T5 paper](https://arxiv.org/pdf/1910.10683.pdf) highlights multiple ways of mixing the tasks together during finetuning:\r\n- **Examples-proportional mixing** - sample from tasks proportionally to their dataset size\r\n- **Equal mixing** - sample uniformly from each task\r\n- **Temperature-scaled mixing** - The generalized approach used by multilingual BERT which uses a temperature T, where the mixing rate of each task is raised to the power 1/T and renormalized. When T=1 this is equivalent to equal mixing, and becomes closer to equal mixing with increasing T.\r\n\r\nFollowing this discussion https://github.com/huggingface/transformers/issues/4340 in [transformers](https://github.com/huggingface/transformers), @enzoampil suggested that the `nlp` library might be a better place for this functionality.\r\n\r\nSome method for combining datasets co"
    },
    {
      "id": "369",
      "title": "adding dataset card information to CONTRIBUTING.md",
      "body": "Added a documentation line and link to the full sprint guide in the \"How to add a dataset\" section, and a section on how to contribute to the dataset card of an existing dataset.\r\n\r\nAnd a thank you note at the end :hugs: "
    },
    {
      "id": "370",
      "title": "Add wiki dpr multiset embeddings",
      "body": "There are two DPR encoders, one trained on Natural Questions and one trained on a multiset/hybrid dataset.\r\nPreviously only the embeddings from the encoder trained on NQ were available. I'm adding the ones from the encoder trained on the multiset/hybrid dataset.\r\nIn the configuration you can now specify `embeddings_name=\"nq\"` or `embeddings_name=\"multiset\"`"
    },
    {
      "id": "371",
      "title": "hind_encorp",
      "body": "resubmit of hind_encorp file changes"
    },
    {
      "id": "372",
      "title": "Some tests hang on Windows",
      "body": "Currently, several tests hang on Windows if the max path limit of 260 characters is not disabled. This happens due to the changes introduced by #2223 that cause an infinite loop in `WindowsFileLock` described in #2220.  This can be very tricky to debug, so I think now is a good time to address these issues/PRs. IMO throwing an error is too harsh, but maybe we can emit a warning in the top-level `__init__.py ` on startup if long paths are not enabled.\r\n"
    },
    {
      "id": "373",
      "title": "Fix head_qa keys",
      "body": "There were duplicate in the keys, as mentioned in #2382 "
    },
    {
      "id": "374",
      "title": "ðŸ› Colab : type object 'pyarrow.lib.RecordBatch' has no attribute 'from_struct_array'",
      "body": "I'm trying to load CNN/DM dataset on Colab.\r\n\r\n[Colab notebook](https://colab.research.google.com/drive/11Mf7iNhIyt6GpgA1dBEtg3cyMHmMhtZS?usp=sharing)\r\n\r\nBut I meet this error :\r\n\r\n> AttributeError: type object 'pyarrow.lib.RecordBatch' has no attribute 'from_struct_array'\r\n"
    },
    {
      "id": "375",
      "title": "Update DialogRE DatasetCard",
      "body": "Update the information in the dataset card for the Dialog RE dataset. "
    },
    {
      "id": "376",
      "title": "[Datasets] Transmit format to children",
      "body": "Transmit format to children obtained when processing a dataset.\r\n\r\nAdded a test.\r\n\r\nWhen concatenating datasets, if the formats are disparate, the concatenated dataset has a format reset to defaults."
    },
    {
      "id": "377",
      "title": "Load_dataset for local CSV files",
      "body": "The method load_dataset fails to correctly load a dataset from csv. \r\n\r\nMoreover, I am working on a token-classification task ( POS tagging) , where each row in my CSV contains two columns each of them having a list of strings.\r\nrow example:\r\n```tokens  |  labels\r\n['I' , 'am', 'John']  |  ['PRON', 'AUX', 'PROPN' ] \r\n```\r\nThe method, loads each list as a string:  (i.g \"['I' , 'am', 'John']\").\r\nTo solve this issue, I copied the Datasets.Features, created Sequence types ( instead of Value)  and tried to cast the features type\r\n```\r\nnew_features['tokens'] = Sequence(feature=Value(dtype='string', id=None))\r\nnew_features['labels'] = Sequence(feature=ClassLabel(num_classes=len(tag2idx), names=list(unique_tags)))\r\ndataset = dataset.cast(new_features)\r\n```\r\nbut I got the following error \r\n```\r\nArrowNotImplementedError: Unsupported cast from string to list using function cast_list\r\n```\r\nMoreover, I tried to set feature parameter in load_dataset method, to my new_features, but this fails as well."
    },
    {
      "id": "378",
      "title": "Add Dataflow support + Wikipedia + Wiki40b",
      "body": "# Add Dataflow support + Wikipedia + Wiki40b\r\n\r\n## Support datasets processing with Apache Beam\r\n\r\nSome datasets are too big to be processed on a single machine, for example: wikipedia, wiki40b, etc. Apache Beam allows to process datasets on many execution engines like Dataflow, Spark, Flink, etc.\r\n\r\nTo process such datasets with Beam, I added a command to run beam pipelines `nlp-cli run_beam path/to/dataset/script`. Then I used it to process the english + french wikipedia, and the english of wiki40b.\r\nThe processed arrow files are on GCS and are the result of a Dataflow job.\r\n\r\nI added a markdown documentation file in `docs` that explains how to use it properly.\r\n\r\n## Load already processed datasets\r\n\r\nNow that we have those datasets already processed, I made it possible to load datasets that are already processed. You can do `load_dataset('wikipedia', '20200501.en')` and it will download the processed files from the Hugging Face GCS directly into the user's cache and be ready to use "
    },
    {
      "id": "379",
      "title": "Corelation should be Correlation",
      "body": "https://github.com/huggingface/datasets/blob/0e87e1d053220e8ecddfa679bcd89a4c7bc5af62/metrics/matthews_correlation/matthews_correlation.py#L66"
    },
    {
      "id": "380",
      "title": "added craigslist_bargians",
      "body": "`craigslist_bargains` data set from [here](https://worksheets.codalab.org/worksheets/0x453913e76b65495d8b9730d41c7e0a0c/)\r\n\r\n(Cleaned up version of #1278)"
    },
    {
      "id": "381",
      "title": "Search qa",
      "body": "add SearchQA dataset\r\n\r\n#336 "
    },
    {
      "id": "382",
      "title": "nlp.load_dataset('xsum') -> TypeError",
      "body": ""
    },
    {
      "id": "383",
      "title": "Add Schema Guided Dialogue dataset",
      "body": "This PR adds the Schema Guided Dialogue dataset created for the DSTC8 challenge\r\n- https://github.com/google-research-datasets/dstc8-schema-guided-dialogue\r\n\r\nA bit simpler than MultiWOZ, the only tricky thing is the sequence of dictionaries that had to be linearized. There is a config for the data proper, and a config for the schemas."
    },
    {
      "id": "384",
      "title": "Allow concatenation of both in-memory and on-disk datasets",
      "body": "This is a prerequisite for the addition of the `add_item` feature (see #1870).\r\nCurrently there is one assumption that we would need to change: a dataset is either fully in memory (dataset._data_files is empty), or the dataset can be reloaded from disk (using the dataset._data_files).\r\nThis assumption is used for pickling for example:\r\n- in-memory dataset can just be pickled/unpickled in-memory\r\n- on-disk dataset can be unloaded to only keep the filepaths when pickling, and then reloaded from the disk when unpickling\r\n\r\nMaybe let's have a design that allows a Dataset to have a Table that can be rebuilt from heterogenous sources like in-memory tables or on-disk tables ? This could also be further extended in the future\r\n\r\nOne idea would be to define a list of sources and each source implements a way to reload its corresponding pyarrow Table.\r\nThen the dataset would be the concatenation of all these tables.\r\n\r\nDepending on the source type, the serialization using pickle would be differen"
    },
    {
      "id": "385",
      "title": "Backwards compatibility broken for cached datasets that use `.filter()`",
      "body": "## Describe the bug\r\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \r\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\r\n\r\nRelated feature: https://github.com/huggingface/datasets/pull/2836\r\n\r\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \r\n\r\n## Workaround\r\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\r\n\r\n## Steps to reproduce the bug\r\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\r\n\r\n2. `pip install datasets==1.11.0` and run the following snippet:\r\n\r\n```python\r\nfrom datasets impo"
    },
    {
      "id": "386",
      "title": "datasets(RWTH-PHOENIX-Weather 2014 T): add initial loading script",
      "body": "This is the first sign language dataset in this repo as far as I know.\r\nFollowing an old issue I opened https://github.com/huggingface/datasets/issues/302.\r\n\r\nI added the dataset official REAMDE file, but I see it's not very standard, so it can be removed.\r\n\r\n"
    },
    {
      "id": "387",
      "title": "load_dataset doesn't include `features` in its hash",
      "body": "It looks like the function `load_dataset` does not include what's passed in the `features` argument when creating a hash for a given  dataset. As a result, if a user includes new features from an already downloaded dataset, those are ignored.\r\n\r\nExample: some models on the hub have a different ordering for the labels than what `datasets` uses for MNLI so I'd like to do something along the lines of:\r\n```\r\ndataset = load_dataset(\"glue\", \"mnli\")\r\nfeatures = dataset[\"train\"].features\r\nfeatures[\"label\"] = ClassLabel(names = ['entailment', 'contradiction', 'neutral'])  # new label order\r\ndataset = load_dataset(\"glue\", \"mnli\", features=features)\r\n```"
    },
    {
      "id": "388",
      "title": "TypeError when using save_to_disk in a dataset loaded with ReadInstruction split",
      "body": "Hi,\r\n\r\nLoading a dataset with `load_dataset` using a split defined via `ReadInstruction` and then saving it to disk results in the following error: `TypeError: Object of type ReadInstruction is not JSON serializable`.\r\n\r\nHere is the minimal reproducible example:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nfrom datasets import ReadInstruction\r\n\r\ndata_1 = load_dataset(\r\n    \"wikiann\",\r\n    \"en\",\r\n    split=\"validation\",\r\n)\r\n\r\ndata_1.save_to_disk(\"temporary_path_1\")\r\n\r\nprint(\"Save with regular split works.\")\r\n\r\ndata_2 = load_dataset(\r\n    \"wikiann\",\r\n    \"en\",\r\n    split=ReadInstruction(\"validation\", to=50, unit=\"%\"),\r\n)\r\n\r\ndata_2.save_to_disk(\"temporary_path_2\")\r\n```\r\n\r\nand the corresponding output:\r\n\r\n```\r\nReusing dataset wikiann (/xxxxx/.cache/huggingface/datasets/wikiann/en/1.1.0/0b11a6fb31eea02f38ca17610657bfba3206100685283014daceb8da291c3be9)\r\nSave with regular split works.\r\nReusing dataset wikiann (/xxxxx/.cache/huggingface/datasets/wikiann/en/1.1.0/0b11a6fb31eea02f38ca1761065"
    },
    {
      "id": "389",
      "title": "cannot load data from my loacal path",
      "body": "## Describe the bug\r\nI just want to directly load data from my local path,but find a bug.And I compare it with pandas to provide my local path is real.\r\n\r\nhere is my code\r\n```python3\r\n# print my local path\r\nprint(config.train_path)\r\n# read data and print data length\r\ntarin=pd.read_csv(config.train_path)\r\nprint(len(tarin))\r\n\r\n# loading data by load_dataset \r\ndata = load_dataset('csv',data_files=config.train_path)\r\n\r\nprint(len(data))\r\n```\r\n\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nC:\\Users\\wie\\Documents\\é¡¹ç›®\\æ–‡æœ¬åˆ†ç±»\\data\\train.csv\r\n7613\r\nTraceback (most recent call last):\r\n  File \"c:/Users/wie/Documents/é¡¹ç›®/æ–‡æœ¬åˆ†ç±»/lib/DataPrecess.py\", line 17, in <module>\r\n    data = load_dataset('csv',data_files=config.train_path)\r\n  File \"C:\\Users\\wie\\Miniconda3\\lib\\site-packages\\datasets\\load.py\", line 830, in load_dataset\r\n    **config_kwargs,\r\n  File \"C:\\Users\\wie\\Miniconda3\\lib\\site-packages\\datasets\\load.py\", line 710, in load_dataset_builder\r\n    **config_kwargs,\r\n  File \"C:\\Users\\wie\\Miniconda3\\lib\\"
    },
    {
      "id": "390",
      "title": "adding polemo2 dataset",
      "body": ""
    },
    {
      "id": "391",
      "title": "Add code of conduct to the project",
      "body": "Add code of conduct to the project and link it from README and CONTRIBUTING.\r\n\r\nThis was already done in `transformers`."
    },
    {
      "id": "392",
      "title": "add CODAH dataset",
      "body": "Adding CODAH dataset.\r\n\r\nMore info:\r\nhttps://github.com/Websail-NU/CODAH"
    },
    {
      "id": "393",
      "title": "[Reddit] add reddit",
      "body": "- Everything worked fine @mariamabarham. Made my computer nearly crash, but all seems to be working :-) "
    },
    {
      "id": "394",
      "title": "make style",
      "body": ""
    },
    {
      "id": "395",
      "title": "[wording] Update Readme.md",
      "body": "Make the features of the library clearer."
    },
    {
      "id": "396",
      "title": "Loading all answers from drop",
      "body": "Hello all,\r\n\r\nI propose this change to the DROP loading script so that all answers are loaded no matter their type. Currently, only \"span\" answers are loaded, which excludes a significant amount of answers from drop (i.e. \"number\" and \"date\").\r\n\r\nI updated the script with the version I use for my work. However, I couldn't find a way to verify that all is working when integrated with the datasets repo, since the `load_dataset` method seems to always download the script from github and not local files.\r\n\r\nNote that 9 items from the train set have no answers, as well as 1 from the validation set. The script I propose simply do not load them.\r\n\r\nLet me know if there is anything else I can do,\r\nClÃ©ment"
    },
    {
      "id": "397",
      "title": "Error in metric.compute: missing `original_instructions` argument",
      "body": "I'm running into an error using metrics for computation in the latest master as well as version 0.2.1. Here is a minimal example:\r\n\r\n```python\r\nimport nlp\r\nrte_metric = nlp.load_metric('glue', name=\"rte\")\r\nrte_metric.compute(\r\n    [0, 0, 1, 1],\r\n    [0, 1, 0, 1],\r\n)\r\n```\r\n\r\n```\r\n    181             # Read the predictions and references\r\n    182             reader = ArrowReader(path=self.data_dir, info=None)\r\n--> 183             self.data = reader.read_files(node_files)\r\n    184 \r\n    185             # Release all of our locks\r\n\r\nTypeError: read_files() missing 1 required positional argument: 'original_instructions'\r\n```\r\n\r\nI believe this might have been introduced with cc8d2508b75f7ba0e5438d0686ee02dcec43c7f4, which added the `original_instructions` argument. Elsewhere, an empty-string default is provided--perhaps that could be done here too?"
    },
    {
      "id": "398",
      "title": "enriched webnlg dataset rebase",
      "body": "Rebase of #1206 !"
    },
    {
      "id": "399",
      "title": "Features should be updated when `map()` changes schema",
      "body": "`dataset.map()` can change the schema and column names.\r\n\r\nWe should update the features in this case (with what is possible to infer)."
    },
    {
      "id": "400",
      "title": "Update the CommonGen citation information",
      "body": ""
    },
    {
      "id": "401",
      "title": "add amazon polarity dataset",
      "body": "This corresponds to the amazon (binary dataset) requested in https://github.com/huggingface/datasets/issues/353"
    },
    {
      "id": "402",
      "title": "Fix map when removing columns on a formatted dataset",
      "body": "This should fix issue #2226\r\n\r\nThe `remove_columns` argument was ignored on formatted datasets"
    },
    {
      "id": "403",
      "title": "Update README.md",
      "body": "Provides description of data instances and dataset features"
    },
    {
      "id": "404",
      "title": "Fix c4 expected files",
      "body": "Some files were not registered in the list of expected files to download\r\n\r\nFix https://github.com/huggingface/datasets/issues/2677"
    },
    {
      "id": "405",
      "title": "adding wili-2018 language identification dataset",
      "body": ""
    },
    {
      "id": "406",
      "title": "Fix missing EOL issue in to_json for old versions of pandas",
      "body": "Some versions of pandas don't add an EOL at the end of the output of `to_json`.\r\nTherefore users could end up having two samples in the same line\r\n\r\nClose https://github.com/huggingface/datasets/issues/2615"
    },
    {
      "id": "407",
      "title": "added TTC4900: A Benchmark Data for Turkish Text Categorization dataset",
      "body": "This PR adds the TTC4900 dataset which is a Turkish Text Categorization dataset by me and @basakbuluz. \r\n\r\nHomepage: [https://www.kaggle.com/savasy/ttc4900](https://www.kaggle.com/savasy/ttc4900)\r\nPoint of Contact: [SavaÅŸ YÄ±ldÄ±rÄ±m](mailto:savasy@gmail.com) / @savasy\r\n"
    },
    {
      "id": "408",
      "title": "Add code-mixed Kannada Hope speech dataset",
      "body": "## Adding a Dataset\r\n- **Name:** *KanHope*\r\n- **Description:** *A code-mixed English-Kannada dataset for Hope speech detection*\r\n- **Paper:** *https://arxiv.org/abs/2108.04616* \r\n- **Data:** *https://github.com/adeepH/KanHope/tree/main/dataset*\r\n- **Motivation:** *The dataset is amongst the very few resources available for code-mixed low-resourced Dravidian languages of India*"
    },
    {
      "id": "409",
      "title": "Add Wiki Lingua Dataset",
      "body": "Hello,\r\n\r\nThis is my first PR. \r\n\r\nI have added Wiki Lingua Dataset along with dataset card to the best of my knowledge.\r\nThere was one hiccup though. I was unable to create dummy data because the data is in pkl format.\r\nFrom the document, I see that:\r\n```At the moment it supports data files in the following format: txt, csv, tsv, jsonl, json, xml```\r\n\r\n"
    },
    {
      "id": "410",
      "title": "`ArrowInvalid` occurs while running `Dataset.map()` function",
      "body": "It seems to fail to process the final batch. This [colab](https://colab.research.google.com/drive/1_byLZRHwGP13PHMkJWo62Wp50S_Z2HMD?usp=sharing) can reproduce the error.\r\n\r\nCode:\r\n\r\n```python\r\n# train_ds = Dataset(features: {\r\n#     'title': Value(dtype='string', id=None), \r\n#     'score': Value(dtype='float64', id=None)\r\n# }, num_rows: 99999)\r\n\r\n# suggested in #665 \r\nclass PicklableTokenizer(BertJapaneseTokenizer):\r\n    def __getstate__(self):\r\n        state = dict(self.__dict__)\r\n        state['do_lower_case'] = self.word_tokenizer.do_lower_case\r\n        state['never_split'] = self.word_tokenizer.never_split\r\n        del state['word_tokenizer']\r\n        return state\r\n    \r\n    def __setstate(self):\r\n        do_lower_case = state.pop('do_lower_case')\r\n        never_split = state.pop('never_split')\r\n        self.__dict__ = state\r\n        self.word_tokenizer = MecabTokenizer(\r\n            do_lower_case=do_lower_case, never_split=never_split\r\n        )\r\n\r\nt = PicklableTokenizer.from_pret"
    },
    {
      "id": "411",
      "title": "Couldn't reach swda.py",
      "body": "ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.2.0/datasets/swda/swda.py\r\n"
    },
    {
      "id": "412",
      "title": "Add Danish NER dataset",
      "body": ""
    },
    {
      "id": "413",
      "title": "Use packaging to handle versions",
      "body": "Use packaging module to handle/validate/check versions of Python packages.\r\n\r\nRelated to #2769."
    },
    {
      "id": "414",
      "title": "Add CC-100 dataset",
      "body": "Add CC-100.\r\n\r\nClose #773 "
    },
    {
      "id": "415",
      "title": "Added PragmEval benchmark",
      "body": ""
    },
    {
      "id": "416",
      "title": "Add Igbo-English Machine Translation Dataset",
      "body": ""
    },
    {
      "id": "417",
      "title": "Test",
      "body": ""
    },
    {
      "id": "418",
      "title": "Cannot unpickle saved .pt dataset with torch.save()/load()",
      "body": "Saving a formatted torch dataset to file using `torch.save()`. Loading the same file fails during unpickling:\r\n\r\n```python\r\n>>> import torch\r\n>>> import nlp\r\n\r\n>>> squad = nlp.load_dataset(\"squad.py\", split=\"train\")\r\n>>> squad\r\nDataset(features: {'source_text': Value(dtype='string', id=None), 'target_text': Value(dtype='string', id=None)}, num_rows: 87599)\r\n>>> squad = squad.map(create_features, batched=True)\r\n>>> squad.set_format(type=\"torch\", columns=[\"source_ids\", \"target_ids\", \"attention_mask\"])\r\n>>> torch.save(squad, \"squad.pt\")\r\n\r\n>>> squad_pt = torch.load(\"squad.pt\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/vegarab/.conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py\", line 593, in load\r\n    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n  File \"/home/vegarab/.conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py\", line 773, in _legacy_load\r\n    result = unpickler.l"
    },
    {
      "id": "419",
      "title": "Updated HuggingFace Datasets README (fix typos)",
      "body": "Awesome work on ðŸ¤— Datasets. I found a couple of small typos in the README. Hope this helps.\r\n\r\n\r\n\r\n![](https://emojipedia-us.s3.dualstack.us-west-1.amazonaws.com/thumbs/160/google/56/hugging-face_1f917.png)\r\n"
    },
    {
      "id": "420",
      "title": "Add configurable options to `seqeval` metric",
      "body": "Fixes #2148\r\n\r\nAdds options to use strict mode, different schemes of evaluation, sample weight and adjust zero_division behavior, if encountered.\r\n\r\n`seqeval` provides schemes as objects, hence dynamic import from string, to avoid making the user do the import (thanks to @albertvillanova for the `importlib` idea)."
    },
    {
      "id": "421",
      "title": "[Tests] run local tests as default",
      "body": "This PR also enables local tests by default\r\n\r\nI think it's safer for now to enable both local and aws tests for every commit. The problem currently is that when we do a PR to add a dataset, the dataset is not yet on AWS on therefore not tested on the PR itself. Thus the PR will always be green even if the datasets are not correct. This PR aims at fixing this.\r\n\r\n## Suggestion on how to commit to the repo from now on:\r\nNow since the repo is \"online\", I think we should adopt a couple of best practices:\r\n1) - No direct committing to the repo anymore. Every change should be opened in a PR and be well documented so that we can find it later\r\n2) - Every PR has to be reviewed by at least x people (I guess @thomwolf you should decide here) because we now have to be much more careful when doing changes to the API for backward compatibility, etc...\r\n"
    },
    {
      "id": "422",
      "title": "Adding docstrings and some doc",
      "body": "Some doc"
    },
    {
      "id": "423",
      "title": "Fix duplicate keys",
      "body": "As noticed in https://github.com/huggingface/datasets/pull/2245, many datasets yield duplicate keys.\r\nMost of the time it was because the counter used for ids were reset at each new data file."
    },
    {
      "id": "424",
      "title": "XNLI dataset is not loading ",
      "body": "`dataset = datasets.load_dataset(path='xnli')`\r\n\r\nshowing below error \r\n```\r\n/opt/conda/lib/python3.7/site-packages/nlp/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     36     if len(bad_urls) > 0:\r\n     37         error_msg = \"Checksums didn't match\" + for_verification_name + \":\\n\"\r\n---> 38         raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\n     39     logger.info(\"All the checksums matched successfully\" + for_verification_name)\r\n     40 \r\n\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://www.nyu.edu/projects/bowman/xnli/XNLI-1.0.zip']\r\n```\r\n\r\nI think URL is now changed to \"https://cims.nyu.edu/~sbowman/xnli/XNLI-MT-1.0.zip\""
    },
    {
      "id": "425",
      "title": "add dataset card title",
      "body": "few of them were missed by me earlier which I've added now"
    },
    {
      "id": "426",
      "title": "Add Turku NLP Corpus for Finnish NER",
      "body": ""
    },
    {
      "id": "427",
      "title": "Add deal_or_no_dialog",
      "body": "Add deal_or_no_dialog Dataset\r\n\r\ngithub: https://github.com/facebookresearch/end-to-end-negotiator\r\nPaper: [Deal or No Deal? End-to-End Learning for Negotiation Dialogues](https://arxiv.org/abs/1706.05125)"
    },
    {
      "id": "428",
      "title": "Add list and inspect methods - cleanup hf_api",
      "body": "Add a bunch of methods to easily list and inspect the processing scripts up-loaded on S3:\r\n```python\r\nnlp.list_datasets()\r\nnlp.list_metrics()\r\n# Copy and prepare the scripts at `local_path` for easy inspection/modification.\r\nnlp.inspect_dataset(path, local_path) \r\n# Copy and prepare the scripts at `local_path` for easy inspection/modification.\r\nnlp.inspect_metric(path, local_path) \r\n```\r\n\r\nAlso clean up the `HfAPI` to use `dataclasses` for better user-experience"
    },
    {
      "id": "429",
      "title": "Fix some metrics feature types",
      "body": "Replace `int` feature type to `int32` since `int` is not a pyarrow dtype in those metrics:\r\n- accuracy\r\n- precision\r\n- recall\r\n- f1\r\nI also added the sklearn citation and used keyword arguments to remove future warnings"
    },
    {
      "id": "430",
      "title": "Add multi_x_science_sum",
      "body": "Add Multi-XScience Dataset. \r\n\r\ngithub repo: https://github.com/yaolu/Multi-XScience\r\npaper: [Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles](https://arxiv.org/abs/2010.14235)"
    },
    {
      "id": "431",
      "title": "wmt datasets fail to load",
      "body": "on master:\r\n```\r\npython -c 'from datasets import load_dataset; load_dataset(\"wmt14\", \"de-en\")'\r\nDownloading and preparing dataset wmt14/de-en (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/stas/.cache/huggingface/datasets/wmt14/de-en/1.0.0/43e717d978d2261502b0194999583acb874ba73b0f4aed0ada2889d1bb00f36e...\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/load.py\", line 740, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/builder.py\", line 578, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/mnt/nvme1/code/huggingface/datasets-master/src/datasets/builder.py\", line 634, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/home/stas/.cache/huggingface/modules/datasets_modul"
    },
    {
      "id": "432",
      "title": "add id_liputan6 dataset",
      "body": "id_liputan6 is a large-scale Indonesian summarization dataset. The articles were harvested from an online news portal, and obtain 215,827 document-summary pairs: https://arxiv.org/abs/2011.00679"
    },
    {
      "id": "433",
      "title": "Can't pass `sep=None` to load_dataset(\"csv\", ...) to infer the separator via pandas.read_csv",
      "body": "When doing `load_dataset(\"csv\", sep=None)`, the `sep` passed to `pd.read_csv` is still the default `sep=\",\"` instead, which makes it impossible to make the csv loader infer the separator.\r\n\r\nRelated to https://github.com/huggingface/datasets/pull/2656\r\n\r\ncc @SBrandeis "
    },
    {
      "id": "434",
      "title": "Create release script",
      "body": "Create a script so that releases can be done automatically (as done in `transformers`)."
    },
    {
      "id": "435",
      "title": "Make datasets PEP-561 compliant",
      "body": "Allows to type-check datasets with `mypy` when imported as a third-party library\r\n\r\nPEP-561: https://www.python.org/dev/peps/pep-0561\r\nMyPy doc on the subject: https://mypy.readthedocs.io/en/stable/installed_packages.html\r\n"
    },
    {
      "id": "436",
      "title": "FIX matinf link in ADD_NEW_DATASET.md",
      "body": ""
    },
    {
      "id": "437",
      "title": "add sharc dataset",
      "body": "This PR adds the ShARC dataset.\r\n\r\nMore info:\r\nhttps://sharc-data.github.io/index.html"
    },
    {
      "id": "438",
      "title": "document `config.HF_DATASETS_OFFLINE` and precedence",
      "body": "https://github.com/huggingface/datasets/pull/1976 implemented `HF_DATASETS_OFFLINE`, but:\r\n1. `config.HF_DATASETS_OFFLINE` is not documented\r\n2. the precedence is not documented (env, config)\r\n\r\nI'm thinking it probably should be similar to what it says https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub about `datasets.config.IN_MEMORY_MAX_SIZE`:\r\n\r\nQuote:\r\n> The default in ðŸ¤— Datasets is to memory-map the dataset on disk unless you set datasets.config.IN_MEMORY_MAX_SIZE different from 0 bytes (default). In that case, the dataset will be copied in-memory if its size is smaller than datasets.config.IN_MEMORY_MAX_SIZE bytes, and memory-mapped otherwise. This behavior can be enabled by setting either the configuration option datasets.config.IN_MEMORY_MAX_SIZE (higher precedence) or the environment variable HF_DATASETS_IN_MEMORY_MAX_SIZE (lower precedence) to nonzero.\r\n\r\nContext: trying to use `config.HF_DATASETS_OFFLINE` here:\r\nhttps://github.com/bigscience"
    },
    {
      "id": "439",
      "title": "Add SD task for SUPERB",
      "body": "Include the SD (Speaker Diarization) task as described in the [SUPERB paper](https://arxiv.org/abs/2105.01051) and `s3prl` [instructions](https://github.com/s3prl/s3prl/tree/master/s3prl/downstream#sd-speaker-diarization).\r\n\r\nTODO:\r\n- [x] Generate the LibriMix corpus\r\n- [x] Prepare the corpus for diarization\r\n- [x] Upload these files to the superb-data repo\r\n- [x] Transcribe the corresponding s3prl processing of these files into our superb loading script\r\n- [x] README: tags + description sections\r\n- ~~Add DER metric~~ (we leave the DER metric for a follow-up PR)\r\n\r\nRelated to #2619.\r\n\r\nClose #2653.\r\n\r\ncc: @lewtun "
    },
    {
      "id": "440",
      "title": "Add humicroedit dataset",
      "body": "Pull request for adding humicroedit dataset"
    },
    {
      "id": "441",
      "title": "Add AllocinÃ© Dataset",
      "body": "This is a french binary sentiment classification dataset, which was used to train this model: https://huggingface.co/tblard/tf-allocine.\r\n\r\nBasically, it's a french \"IMDB\" dataset, with more reviews.\r\n\r\nMore info on [this repo](https://github.com/TheophileBlard/french-sentiment-analysis-with-bert). "
    },
    {
      "id": "442",
      "title": "data loader for reading comprehension task",
      "body": "added doc2dial data loader and dummy data for reading comprehension task."
    },
    {
      "id": "443",
      "title": "Concatenate several datasets with removed columns is not working.",
      "body": "## Describe the bug\r\n\r\nYou can't concatenate datasets when you removed columns before.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset, concatenate_datasets\r\n\r\nwikiann= load_dataset(\"wikiann\",\"en\")\r\n\r\nwikiann[\"train\"] = wikiann[\"train\"].remove_columns([\"langs\",\"spans\"])\r\nwikiann[\"test\"] = wikiann[\"test\"].remove_columns([\"langs\",\"spans\"])\r\n\r\nassert wikiann[\"train\"].features.type == wikiann[\"test\"].features.type\r\n\r\nconcate = concatenate_datasets([wikiann[\"train\"],wikiann[\"test\"]])\r\n```\r\n\r\n## Expected results\r\nMerged dataset \r\n\r\n\r\n## Actual results\r\n```python\r\nValueError: External features info don't match the dataset:\r\nGot\r\n{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], names_file=None, id=None), length=-1, id=None), 'langs': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'spans': "
    },
    {
      "id": "444",
      "title": "[BUG FIX] typo in the import path for metrics",
      "body": "This tiny PR fixes a typo introduced in https://github.com/huggingface/datasets/pull/1726 which prevents loading new metrics"
    },
    {
      "id": "445",
      "title": "Refactorize tests to use Dataset as context manager",
      "body": "Refactorize Dataset tests to use Dataset as context manager."
    },
    {
      "id": "446",
      "title": "Add Arabic sarcasm dataset",
      "body": "This MIT license dataset: https://github.com/iabufarha/ArSarcasm\r\n\r\nVia https://sites.google.com/view/ar-sarcasm-sentiment-detection/"
    },
    {
      "id": "447",
      "title": "TypeError: '<' not supported between instances of 'NamedSplit' and 'NamedSplit'",
      "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.3.1 (installed from master)\r\n- `datasets` version: 1.0.2 (installed as a dependency from transformers)\r\n- Platform: Linux-4.15.0-118-generic-x86_64-with-debian-stretch-sid\r\n- Python version: 3.7.9\r\n\r\nI'm testing my own text classification dataset using [this example](https://github.com/huggingface/transformers/tree/master/examples/text-classification#run-generic-text-classification-script-in-tensorflow) from transformers. The dataset is split into train / dev / test, and in csv format, containing just a text and a label columns, using comma as sep. Here's a sample:\r\n```\r\ntext,label\r\n\"Registra-se a presenÃ§a do acadÃªmico <name> . <REL_SEP> Ao me deparar com a descriÃ§Ã£o de dois autores no polo ativo da aÃ§Ã£o junto ao PJe , margem esquerda foi informado pela procuradora do recla"
    },
    {
      "id": "448",
      "title": "app_reviews_by_users",
      "body": "Software Applications User Reviews "
    },
    {
      "id": "449",
      "title": "Keys yielded while generating dataset are not being checked",
      "body": "The keys used in the dataset generation script to ensure the same order is generated on every user's end should be checked for their types (i.e either `str` or `int`) as well as whether they are unique or not.\r\nCurrently, the keys are not being checked for any of these, as evident from `xnli' dataset generation:\r\nhttps://github.com/huggingface/datasets/blob/56346791aed417306d054d89bd693d6b7eab17f7/datasets/xnli/xnli.py#L196\r\nEven after having a tuple as key, the dataset is generated without any warning.\r\n\r\nAlso, as tested in the case of `anli` dataset (I tweeked the dataset script to use `1` as a key for every example):\r\n```\r\n>>> import datasets\r\n>>> nik = datasets.load_dataset('anli')\r\nDownloading and preparing dataset anli/plain_text (download: 17.76 MiB, generated: 73.55 MiB, post-processed: Unknown size, total: 91.31 MiB) to C:\\Users\\nikhil\\.cache\\huggingface\\datasets\\anli\\plain_text\\0.1.0\\43fa2c99c10bf8478f1fa0860f7b122c6b277c4c41306255b7641257cf4e3299...\r\n0 examples [00:00, ? exa"
    },
    {
      "id": "450",
      "title": "ValueError when a split is empty",
      "body": "When a split is empty either TEST, VALIDATION or TRAIN I get the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/load.py\", line 295, in load\r\n    ds = dbuilder.as_dataset(**as_dataset_kwargs)\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/builder.py\", line 587, in as_dataset\r\n    datasets = utils.map_nested(build_single_dataset, split, map_tuple=True)\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/utils/py_utils.py\", line 158, in map_nested\r\n    for k, v in data_struct.items()\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/utils/py_utils.py\", line 158, in <dictcomp>\r\n    for k, v in data_struct.items()\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/utils/py_utils.py\", line 172, in map_nested\r\n    return function(data_struct)\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/builder.py\", line 601, in _build_single_dataset\r\n    split=split,\r\n  File \"/home/jplu/dev/jplu/datasets/src/nlp/builder.py\", lin"
    },
    {
      "id": "451",
      "title": "Add hotpot QA",
      "body": "Added the [HotpotQA](https://github.com/hotpotqa/hotpot) multi-hop question answering dataset.\r\n\r\n"
    },
    {
      "id": "452",
      "title": "add METEOR metric",
      "body": "Added the METEOR metric. Can be used like this:\r\n\r\n```python\r\nimport nlp\r\nmeteor = nlp.load_metric('metrics/meteor')\r\nmeteor.compute([\"some string\", \"some string\"], [\"some string\", \"some similar string\"])\r\n# {'meteor': 0.6411637931034483}\r\nmeteor.add(\"some string\", \"some string\")\r\nmeteor.add('some string\", \"some similar string\")\r\nmeteor.compute()\r\n# {'meteor': 0.6411637931034483}\r\n```\r\n\r\nUses [NLTK's implementation](https://www.nltk.org/api/nltk.translate.html#module-nltk.translate.meteor_score), [(source)](https://github.com/nltk/nltk/blob/develop/nltk/translate/meteor_score.py)"
    },
    {
      "id": "453",
      "title": "SacreBLEU update",
      "body": "With the latest release of [sacrebleu](https://github.com/mjpost/sacrebleu), `datasets.metrics.sacrebleu` is broken, and getting error.\r\n\r\n    AttributeError: module 'sacrebleu' has no attribute 'DEFAULT_TOKENIZER'\r\n\r\nthis happens since in new version of sacrebleu there is no `DEFAULT_TOKENIZER`, but sacrebleu.py tries to import it anyways. This can be fixed currently with fixing `sacrebleu==1.5.0`\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nsacrebleu= datasets.load_metric('sacrebleu')\r\npredictions = [\"It is a guide to action which ensures that the military always obeys the commands of the party\"]\r\nreferences = [\"It is a guide to action that ensures that the military will forever heed Party commands\"]\r\nresults = sacrebleu.compute(predictions=predictions, references=references)\r\nprint(results)\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform: Windows-10-10.0.19041-SP0\r\n- Pytho"
    },
    {
      "id": "454",
      "title": "\"counter\" dataset raises an error in normal mode, but not in streaming mode",
      "body": "## Describe the bug\r\n\r\n`counter` dataset raises an error on `load_dataset()`, but simply returns an empty iterator in streaming mode.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\n>>> import datasets as ds\r\n>>> a = ds.load_dataset('counter', split=\"train\", streaming=False)\r\nUsing custom data configuration default\r\nDownloading and preparing dataset counter/default (download: 1.29 MiB, generated: 2.48 MiB, post-processed: Unknown size, total: 3.77 MiB) to /home/slesage/.cache/huggingface/datasets/counter/default/1.0.0/9f84962fa0f35bec5a34fe0bdff8681838d497008c457f7856c48654476ec0e9...\r\nTraceback (most recent call last):\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 726, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py\", line 1124, in _prepare_split\r\n    for key, record in utils"
    },
    {
      "id": "455",
      "title": "The FLUE Benchmark",
      "body": "This PR adds the [FLUE](https://github.com/getalp/Flaubert/tree/master/flue) benchmark which is a set of different datasets to evaluate models for French content.\r\n\r\nTwo datasets are missing, the French Treebank that we can use only for research purpose and we are not allowed to distribute, and the Word Sense disambiguation for Nouns that will be added later."
    },
    {
      "id": "456",
      "title": "dataset viewer does not work anymore ",
      "body": "Hi\r\nI normally use this link to see all datasets and how I can load them \r\n\r\n\r\nhttps://huggingface.co/datasets/viewer/\r\n\r\nNow I am getting \r\n\r\n502 Bad Gateway\r\nnginx/1.18.0 (Ubuntu)\r\n\r\ncould you bring this webpage back ? this was very helpful @lhoestq \r\nthanks for your help "
    },
    {
      "id": "457",
      "title": "adding dyk dataset",
      "body": ""
    },
    {
      "id": "458",
      "title": "Support sliced list arrays in cast",
      "body": "There is this issue in pyarrow:\r\n```python\r\nimport pyarrow as pa\r\n\r\narr = pa.array([[i * 10] for i in range(4)])\r\narr.cast(pa.list_(pa.int32()))  # works\r\n\r\narr = arr.slice(1)\r\narr.cast(pa.list_(pa.int32()))  # fails\r\n# ArrowNotImplementedError(\"Casting sliced lists (non-zero offset) not yet implemented\")\r\n```\r\n\r\nHowever in `Dataset.cast` we slice tables to cast their types (it's memory intensive), so we have the same issue.\r\nBecause of this it is currently not possible to cast a Dataset with a Sequence feature type (unless the table is small enough to not be sliced).\r\n\r\nIn this PR I fixed this by resetting the offset of `pyarrow.ListArray` arrays to zero in the table before casting.\r\nI used `pyarrow.compute.subtract` function to update the offsets of the ListArray.\r\n\r\ncc @abhi1thakur @SBrandeis "
    },
    {
      "id": "459",
      "title": "adding meta_woz dataset",
      "body": ""
    },
    {
      "id": "460",
      "title": "Give a user feedback if the dataset he loads is streamable or not",
      "body": "**Is your feature request related to a problem? Please describe.**\r\nI would love to know if a `dataset` is with the current implementation streamable or not. \r\n\r\n**Describe the solution you'd like**\r\nWe could show a warning when a dataset is loaded with `load_dataset('...',streaming=True)` when its lot streamable, e.g. if it is an archive. \r\n\r\n**Describe alternatives you've considered**\r\nAdd a new metadata tag for \"streaming\"\r\n"
    },
    {
      "id": "461",
      "title": "Add the Multilingual Amazon Reviews Corpus",
      "body": "- **Name:** Multilingual Amazon Reviews Corpus* (`amazon_reviews_multi`)\r\n- **Description:** A collection of Amazon reviews in English, Japanese, German, French, Spanish and Chinese.\r\n- **Paper:** https://arxiv.org/abs/2010.02573\r\n\r\n### Checkbox\r\n\r\n- [x] Create the dataset script `/datasets/my_dataset/my_dataset.py` using the template\r\n- [x] Fill the `_DESCRIPTION` and `_CITATION` variables\r\n- [x] Implement `_infos()`, `_split_generators()` and `_generate_examples()`\r\n- [x] Make sure that the `BUILDER_CONFIGS` class attribute is filled with the different configurations of the dataset and that the `BUILDER_CONFIG_CLASS` is specified if there is a custom config class.\r\n- [x] Generate the metadata file `dataset_infos.json` for all configurations\r\n- [x] Generate the dummy data `dummy_data.zip` files to have the dataset script tested and that they don't weigh too much (<50KB)\r\n- [x] Add the dataset card `README.md` using the template : fill the tags and the various paragraphs\r\n- [x] Both te"
    },
    {
      "id": "462",
      "title": "Add NCBI Disease Corpus dataset",
      "body": ""
    },
    {
      "id": "463",
      "title": "Style change",
      "body": "make quality and make style ran on scripts"
    },
    {
      "id": "464",
      "title": "NonMatchingSplitsSizesError error when reading the IMDB dataset",
      "body": "Hi!\r\n\r\nI am trying to load the `imdb` dataset with this line:\r\n\r\n`dataset = nlp.load_dataset('imdb', data_dir='/A/PATH', cache_dir='/A/PATH')`\r\n\r\nbut I am getting the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/mounts/Users/cisintern/antmarakis/anaconda3/lib/python3.7/site-packages/nlp/load.py\", line 517, in load_dataset\r\n    save_infos=save_infos,\r\n  File \"/mounts/Users/cisintern/antmarakis/anaconda3/lib/python3.7/site-packages/nlp/builder.py\", line 363, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File \"/mounts/Users/cisintern/antmarakis/anaconda3/lib/python3.7/site-packages/nlp/builder.py\", line 421, in _download_and_prepare\r\n    verify_splits(self.info.splits, split_dict)\r\n  File \"/mounts/Users/cisintern/antmarakis/anaconda3/lib/python3.7/site-packages/nlp/utils/info_utils.py\", line 70, in verify_splits\r\n    raise NonMatchingSplitsSizesError(str(bad_sp"
    },
    {
      "id": "465",
      "title": "Add  KorQuAD v1 Dataset",
      "body": "# The Korean Question Answering Dataset\r\nAdding the [KorQuAD](https://korquad.github.io/KorQuad%201.0/) v1 dataset as part of the sprint ðŸŽ‰ \r\nThis dataset is very similar to SQuAD which is why I added it as `squad_kor_v1`. There is also a v2 which I added [here](https://github.com/huggingface/datasets/pull/1180).\r\n\r\n- Crowd generated questions and answer (1-answer per question) for Wikipedia articles.\r\n\r\n- [x] All tests passed\r\n- [x] Added dummy data\r\n- [x] Added data card (as much as I could)\r\n"
    },
    {
      "id": "466",
      "title": "Add AmbigQA dataset",
      "body": "# AmbigQA: Answering Ambiguous Open-domain Questions Dataset\r\nAdding the [AmbigQA](https://nlp.cs.washington.edu/ambigqa/) dataset as part of the sprint ðŸŽ‰ (from Open dataset list for Dataset sprint)\r\n\r\nAdded both the light and full versions (as seen on the dataset homepage)\r\nThe json format changes based on the value of one 'type' field, so I set the unavailable field to an empty list. This is explained in the README -> Data Fields\r\n\r\n```py\r\ntrain_light_dataset = load_dataset('./datasets/ambig_qa',\"light\",split=\"train\")\r\nval_light_dataset = load_dataset('./datasets/ambig_qa',\"light\",split=\"validation\")\r\ntrain_full_dataset = load_dataset('./datasets/ambig_qa',\"full\",split=\"train\")\r\nval_full_dataset = load_dataset('./datasets/ambig_qa',\"full\",split=\"validation\")\r\n\r\n\r\nfor example in train_light_dataset:\r\n    for i,t in enumerate(example['annotations']['type']):\r\n        if t =='singleAnswer':\r\n            # use the example['annotations']['answer'][i]\r\n            # example['annotations']["
    },
    {
      "id": "467",
      "title": "adding swedish_medical_ner",
      "body": "Adding the Swedish Medical NER dataset, listed in \"Biomedical Datasets - BigScience Workshop 2021\"\r\n\r\nCode refactored "
    },
    {
      "id": "468",
      "title": "[Clean up] remove bogus folder",
      "body": "@mariamabarham  - I think you accidentally placed it there."
    },
    {
      "id": "469",
      "title": "Dataset viewer issues",
      "body": "I was looking through the dataset viewer and I like it a lot. Version numbers, citation information, everything's there! I've spotted a few issues/bugs though:\r\n\r\n- the URL is still under `nlp`, perhaps an alias for `datasets` can be made\r\n- when I remove a **feature** (and the feature list is empty), I get an error. This is probably expected, but perhaps a better error message can be shown to the user\r\n\r\n```bash\r\nIndexError: list index out of range\r\nTraceback:\r\nFile \"/home/sasha/streamlit/lib/streamlit/ScriptRunner.py\", line 322, in _run_script\r\n    exec(code, module.__dict__)\r\nFile \"/home/sasha/nlp-viewer/run.py\", line 316, in <module>\r\n    st.table(style)\r\nFile \"/home/sasha/streamlit/lib/streamlit/DeltaGenerator.py\", line 122, in wrapped_method\r\n    return dg._enqueue_new_element_delta(marshall_element, delta_type, last_index)\r\nFile \"/home/sasha/streamlit/lib/streamlit/DeltaGenerator.py\", line 367, in _enqueue_new_element_delta\r\n    rv = marshall_element(msg.delta.new_element)\r\nFile"
    },
    {
      "id": "470",
      "title": "Add Cryptonite dataset",
      "body": "cc @aviaefrat who's the original author of the dataset & paper, see https://github.com/aviaefrat/cryptonite"
    },
    {
      "id": "471",
      "title": "[speedup] Use indices mappings instead of deepcopy for all the samples reordering methods",
      "body": "Use an indices mapping instead of rewriting the dataset for all the samples re-ordering/selection methods (`select`, `sort`, `shuffle`, `shard`, `train_test_split`).\r\n\r\nAdded a `flatten_indices` method which copy the dataset to a new table to remove the indices mapping with tests.\r\n\r\nAll the samples re-ordering/selection methods should be a lot faster. The downside is that iterating on very large batch of the dataset might be a little slower when we have changed the order of the samples since with in these case we use `pyarrow.Table.take` instead of `pyarrow.Table.slice`. There is no free lunch but the speed of iterating over the dataset is rarely the bottleneck.\r\n\r\n*Backward breaking change*: the `cache_file_name` argument in all the samples re-ordering/selection methods (`select`, `sort`, `shuffle`, `shard`, `train_test_split`) is now called `indices_cache_file_name` on purpose to make it explicit to the user that this caching file is used for caching the indices mapping and not the "
    },
    {
      "id": "472",
      "title": "Faster caching for text dataset",
      "body": "As mentioned in #546 and #548 , hashing `data_files` contents to get the cache directory name for a text dataset can take a long time.\r\n\r\nTo make it faster I changed the hashing so that it takes into account the `path` and the `last modified timestamp` of each data file, instead of iterating through the content of each file to get a hash."
    },
    {
      "id": "473",
      "title": "Add proto_qa dataset",
      "body": "Added dataset tags as required."
    },
    {
      "id": "474",
      "title": "Implement ClassLabel encoding in JSON loader",
      "body": "Close #2365."
    },
    {
      "id": "475",
      "title": "UnicodeDecodeError when downloading GLUE-MNLI",
      "body": "When I run\r\n```python\r\ndataset = nlp.load_dataset('glue', 'mnli')\r\n```\r\nI get an encoding error (could it be because I'm using Windows?) :\r\n```python\r\n# Lots of error log lines later...\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\std.py in __iter__(self)\r\n   1128         try:\r\n-> 1129             for obj in iterable:\r\n   1130                 yield obj\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\site-packages\\nlp\\datasets\\glue\\5256cc2368cf84497abef1f1a5f66648522d5854b225162148cb8fc78a5a91cc\\glue.py in _generate_examples(self, data_file, split, mrpc_files)\r\n    529 \r\n--> 530                 for n, row in enumerate(reader):\r\n    531                     if is_cola_non_test:\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\csv.py in __next__(self)\r\n    110             self.fieldnames\r\n--> 111         row = next(self.reader)\r\n    112         self.line_num = self.reader.line_num\r\n\r\n~\\Miniconda3\\envs\\nlp\\lib\\encodings\\cp1252.py in decode(self, input, final)\r\n     22     def decode(self, input, final=False):\r\n---> 23         r"
    },
    {
      "id": "476",
      "title": "Fix shuffle on IterableDataset that disables batching in case any functions were mapped",
      "body": "Made a very minor change to fix the issue#2716. Added the missing argument in the constructor call.\r\n\r\nAs discussed in the bug report, the change is made to prevent the `shuffle` method call from resetting the value of `batched` attribute in `MappedExamplesIterable`\r\n\r\nFix #2716."
    },
    {
      "id": "477",
      "title": "adding  ted_talks_iwslt",
      "body": "UPDATE2: (2nd Jan) Wrote a long writeup on the slack channel. I don't think this approach is correct. Basically this created language pairs (109*108) \r\nRunning the `pytest `went for more than 40+ hours and it was still running!  \r\nSo working on a different approach, such that the number of configs = number of languages. Will make a new pull request with that. \r\n\r\nUPDATE: This requires manual download dataset\r\n\r\nThis is a draft version "
    },
    {
      "id": "478",
      "title": "Eduge",
      "body": "Hi, awesome folks behind the huggingface! \r\n\r\nHere is my PR for the text classification dataset in Mongolian.\r\n\r\nPlease do let me know in case you have anything to clarify. \r\n\r\nThanks & Regards,\r\nEnod"
    },
    {
      "id": "479",
      "title": "[Question] Best way to batch a large dataset?",
      "body": "I'm training on large datasets such as Wikipedia and BookCorpus. Following the instructions in [the tutorial notebook](https://colab.research.google.com/github/huggingface/nlp/blob/master/notebooks/Overview.ipynb), I see the following recommended for TensorFlow:\r\n\r\n```python\r\ntrain_tf_dataset = train_tf_dataset.filter(remove_none_values, load_from_cache_file=False)\r\ncolumns = ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']\r\ntrain_tf_dataset.set_format(type='tensorflow', columns=columns)\r\nfeatures = {x: train_tf_dataset[x].to_tensor(default_value=0, shape=[None, tokenizer.max_len]) for x in columns[:3]} \r\nlabels = {\"output_1\": train_tf_dataset[\"start_positions\"].to_tensor(default_value=0, shape=[None, 1])}\r\nlabels[\"output_2\"] = train_tf_dataset[\"end_positions\"].to_tensor(default_value=0, shape=[None, 1])\r\n### Question about this last line ###\r\ntfdataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(8)\r\n```\r\n\r\nThis code works for som"
    },
    {
      "id": "480",
      "title": "Add Google wellformed query dataset",
      "body": "This pull request will add Google wellformed_query dataset. Link of dataset is https://github.com/google-research-datasets/query-wellformedness"
    },
    {
      "id": "481",
      "title": "[Tests] Clean tests",
      "body": "the abseil testing library (https://abseil.io/docs/python/quickstart.html) is better than the one I had before, so I decided to switch to that and changed the `setup.py` config file. \r\nAbseil has more support and a cleaner API for parametrized testing I think. \r\n\r\nI added a list of all dataset scripts that are currently on AWS, but will replace that once the \r\nAPI is integrated into this lib. \r\n\r\nOne can now easily test for just a single function for a single dataset with:\r\n`tests/test_dataset_common.py::DatasetTest::test_load_dataset_wikipedia` \r\n\r\nNOTE: This PR is rebased on PR #29 so should be merged after."
    },
    {
      "id": "482",
      "title": "Update README.md of covid-tweets-japanese",
      "body": "Update README.md of covid-tweets-japanese added by PR https://github.com/huggingface/datasets/pull/1367 and https://github.com/huggingface/datasets/pull/1402.\r\n\r\n- Update \"Data Splits\" to be more precise that no information is provided for now.\r\n  - old: [More Information Needed]\r\n  - new: No information about data splits is provided for now.\r\n\r\n- The automatic generation of links seemed not working properly, so I added a space before and after the URL to make the links work correctly."
    },
    {
      "id": "483",
      "title": "dataset.search_batch() function outputs all -1 indices sometime.",
      "body": "I am working with RAG and playing around with different faiss indexes. At the moment I use **index = faiss.index_factory(768, \"IVF65536_HNSW32,Flat\")**.\r\n\r\nDuring the retrieval phase exactly in [this line of retrieval_rag.py](https://github.com/huggingface/transformers/blob/master/src/transformers/models/rag/retrieval_rag.py#L231) an error issue when all retrieved indices are -1.  Please refer to the screenshot of a PID worker. \r\n\r\n![image](https://user-images.githubusercontent.com/16892570/113782387-37a67600-9786-11eb-9c29-acad661a9648.png)\r\n\r\n\r\nHere, my retrieve batch size is 2 and n_docs is 5. I can solve this by working around np. stack, but I want to ask, why we get an output index of -1. Do you have any idea :) ?\r\n\r\nIs this a problem of the index, where the faiss can't find any similar vector?\r\nIs there documentation on the output index being -1?\r\n\r\n@lhoestq \r\n "
    },
    {
      "id": "484",
      "title": "`Can not decode content-encoding: gzip` when loading `scitldr` dataset with streaming",
      "body": "## Describe the bug\r\n\r\nTrying to load the `\"FullText\"` config of the `\"scitldr\"` dataset with `streaming=True` raises an error from `aiohttp`:\r\n```python\r\nClientPayloadError: 400, message='Can not decode content-encoding: gzip'\r\n```\r\n\r\ncc @lhoestq \r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\niter_dset = iter(\r\n    load_dataset(\"scitldr\", name=\"FullText\", split=\"test\", streaming=True)\r\n)\r\n\r\nnext(iter_dset)\r\n```\r\n\r\n## Expected results\r\nReturns the first sample of the dataset\r\n\r\n## Actual results\r\nCalling `__next__` crashes with the following Traceback:\r\n\r\n```python\r\n----> 1 next(dset_iter)\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in __iter__(self)\r\n    339\r\n    340     def __iter__(self):\r\n--> 341         for key, example in self._iter():\r\n    342             if self.features:\r\n    343                 # we encode the example for ClassLabel feature types for example\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\da"
    },
    {
      "id": "485",
      "title": "Add version-specific BibTeX",
      "body": "As pointed out by @lhoestq in #2411, after the creation of the Zenodo DOI for Datasets, a new BibTeX entry is created with each release.\r\n\r\nThis PR adds a version-specific BibTeX entry, besides the existing one which is generic for the project.\r\n\r\nSee version-specific BibTeX entry here: https://zenodo.org/record/4817769/export/hx#.YLSyd6j7RPY"
    },
    {
      "id": "486",
      "title": "Add a Text Classification dataset: KanHope",
      "body": "## Adding a Dataset\r\n- **Name:** *KanHope*\r\n- **Description:** *A code-mixed English-Kannada dataset for Hope speech detection*\r\n- **Paper:** *https://arxiv.org/abs/2108.04616* (I am the author of the paper}\r\n- **Author:** *[AdeepH](https://github.com/adeepH)*\r\n- **Data:** *https://github.com/adeepH/KanHope/tree/main/dataset*\r\n- **Motivation:** *The dataset is amongst the very few resources available for code-mixed Dravidian languages*\r\n\r\n- I tried following the steps as per the instructions. However, could not resolve an error. Any help would be appreciated.\r\n\r\n- The dataset card and the scripts for the dataset *https://github.com/adeepH/datasets/tree/multilingual-hope-speech/datasets/mhs_eval*\r\n\r\n```\r\nUsing custom data configuration default\r\nDownloading and preparing dataset bn_hate_speech/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/bn_hate_speech/default/0.0.0/5f417ddc89777278abd299"
    },
    {
      "id": "487",
      "title": "add labr dataset",
      "body": "Arabic Book Reviews dataset. "
    },
    {
      "id": "488",
      "title": "Add -DOCSTART- note to dataset card of conll-like datasets",
      "body": "Closes #1983"
    },
    {
      "id": "489",
      "title": "Better arrow dataset iter",
      "body": "I tried to play around with `tf.data.Dataset.from_generator` and I found out that the `__iter__` that we have for `nlp.arrow_dataset.Dataset` ignores the format that has been set (torch or tensorflow).\r\nWith these changes I should be able to come up with a `tf.data.Dataset` that uses lazy loading, as asked in #193."
    },
    {
      "id": "490",
      "title": "Fix QA4MRE download URLs",
      "body": "The URLs in the `dataset_infos` and `README` are correct, only the ones in the download script needed updating."
    },
    {
      "id": "491",
      "title": "CoNLL 2003 dataset not including German",
      "body": "Hello, thanks for all the work on developing and maintaining this amazing platform, which I am enjoying working with!\r\n\r\nI was wondering if there is a reason why the German CoNLL 2003 dataset is not included in the [repository](https://github.com/huggingface/datasets/tree/master/datasets/conll2003), since a copy of it could be found in some places on the internet such as GitHub? I could help adding the German data to the hub, unless there are some copyright issues that I am unaware of...\r\n\r\nThis is considering that many work use the union of CoNLL 2002 and 2003 datasets for comparing cross-lingual NER transfer performance in `en`, `de`, `es`, and `nl`. E.g., [XLM-R](https://www.aclweb.org/anthology/2020.acl-main.747.pdf).\r\n\r\n## Adding a Dataset\r\n- **Name:** CoNLL 2003 German\r\n- **Paper:** https://www.aclweb.org/anthology/W03-0419/\r\n- **Data:** https://github.com/huggingface/datasets/tree/master/datasets/conll2003\r\n"
    },
    {
      "id": "492",
      "title": "shuffle with torch generator ",
      "body": "Hi\r\nI need to shuffle mutliple large datasets with `generator = torch.Generator()` for a distributed sampler which needs to make sure datasets are consistent across different cores, for this, this is really necessary for me to use  torch generator, based on documentation this generator is not supported with datasets, I really need to make shuffle work with this generator and I was wondering what I can do about this issue, thanks for your help \r\n\r\n@lhoestq "
    },
    {
      "id": "493",
      "title": "add concatenate_datasets to the docs",
      "body": ""
    },
    {
      "id": "494",
      "title": "Add SQA",
      "body": "## Adding a Dataset\r\n- **Name:** SQA (Sequential Question Answering) by Microsoft. \r\n- **Description:** The SQA dataset was created to explore the task of answering sequences of inter-related questions on HTML tables. It has 6,066 sequences with 17,553 questions in total.\r\n- **Paper:** https://www.microsoft.com/en-us/research/publication/search-based-neural-structured-learning-sequential-question-answering/\r\n- **Data:** https://www.microsoft.com/en-us/download/details.aspx?id=54253\r\n- **Motivation:** currently, the [Tapas](https://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html) algorithm by Google AI is being added to the Transformers library (see https://github.com/huggingface/transformers/pull/8113). It would be great to use that model in combination with this dataset, on which it achieves SOTA results (average question accuracy of 0.71).\r\n\r\nNote 1: this dataset actually consists of 2 types of files: \r\n1) TSV files, containing the questions, answer coordinates a"
    },
    {
      "id": "495",
      "title": "Fix data URLs and metadata in DocRED dataset",
      "body": "The host of `docred` dataset has updated the `dev` data file. This PR:\r\n- Updates the dev URL\r\n- Updates dataset metadata\r\n\r\nThis PR also fixes the URL of the `train_distant` split, which was wrong.\r\n\r\nFix #2882."
    },
    {
      "id": "496",
      "title": "My new dataset PEC",
      "body": "A new dataset PEC published in EMNLP 2020."
    },
    {
      "id": "497",
      "title": "feat: ðŸŽ¸ add a function to get a dataset config's split names",
      "body": "Also: pass additional arguments (use_auth_token) to get private configs + info of private datasets on the hub\r\n\r\nQuestions:\r\n\r\n- <strike>I'm not sure how the versions work: I changed 1.12.1.dev0 to 1.12.1.dev1, was it correct?</strike> no -> reverted\r\n- Should I add a section in https://github.com/huggingface/datasets/blob/master/docs/source/load_hub.rst? (there is no section for get_dataset_infos)"
    },
    {
      "id": "498",
      "title": "404 Not Found Error when loading LAMA dataset",
      "body": "The [LAMA](https://huggingface.co/datasets/viewer/?dataset=lama) probing dataset is not available for download:  \r\n\r\nSteps to Reproduce: \r\n\r\n1. `from datasets import load_dataset`\r\n2. `dataset = load_dataset('lama', 'trex')`. \r\n\r\n\r\nResults:  \r\n`FileNotFoundError: Couldn't find file locally at lama/lama.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/lama/lama.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/lama/lama.py`"
    },
    {
      "id": "499",
      "title": "copy.deepcopy os.environ instead of copy",
      "body": "Fixes: https://github.com/huggingface/datasets/issues/2115\r\n\r\n- bug fix: using envrion.copy() returns a dict.\r\n- using deepcopy(environ) returns an `_environ` object\r\n- Changing the datatype of the _environ object can break code, if subsequent libraries perform operations using apis exclusive to the environ object, like `environ.getenv()` for example.\r\n\r\n\r\nTesting:\r\n\r\nTested the change on my terminal:\r\n\r\n```\r\n>>> import os\r\n>>> x = deepcopy(os.environ)\r\n>>> y = os.environ\r\n>>> x is y\r\nFalse\r\n>>> isinstance(x, type(os.environ))\r\nTrue\r\n>>> z = os.environ.copy()\r\n>>> isinstance(z, type(os.environ))\r\nFalse\r\n>>> isinstance(z, dict)\r\nTrue\r\n```"
    }
  ]
}