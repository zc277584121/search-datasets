{
  "task": "discord",
  "description": "Discord Chat Retrieval",
  "total": 496,
  "queries": [
    {
      "id": "0",
      "message": "SteveMcQueen#3713: Joined the server.\nSid#2121: Joined the server.\nSid#2121: ðŸ‘‹\nDaj#7482: Hello hello\nDaj#7482: I'll get myself up to speed on how to properly set up and admin a discord server soon\nSid#2121: wish i could be of any help at all in that regard lol\nDaj#7482: I think our needs are pretty minimal haha, but feel free to request channels or other things we might need\nSid#2121: can we get a #datascripts channel or something? to collect all our individual scripts for data gathering\nDaj#748"
    },
    {
      "id": "1",
      "message": "bmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/730093559427891250/unknown.png\nbmk#1476: wrote this up\nDaj#7482: Try that\nDaj#7482: I do like the colors lol\nSid#2121: ooh how do i get a fancy colour\nSid#2121: DAMN\nSid#2121: already have one\nSid#2121: nice\nDaj#7482: Yea I just created a GPT Neo role and gave it to you all\nDaj#7482: You have most permissions but I don't think we need to worry about trolling\nbmk#1476: that reminds we i think we should choose a better name than "
    },
    {
      "id": "2",
      "message": "Daj#7482: but if you have a cooler name totally open\nbmk#1476: LibreLM\nbmk#1476: NeoLM\nSid#2121: I like using GPT because, well, it's gpt\nSid#2121: we're not doing anything totally novel\nSid#2121: aside from open sourcing it\nbmk#1476: LibreGPT\nDaj#7482: I feel like we might not wanna overuse the Libre/Open names\nDaj#7482: (like OpenAI does ðŸ”¥)\nSid#2121: https://tenor.com/view/rap-rap-battle-gif-5518154\nDaj#7482: We can still change it before we release to the public\nDaj#7482: So we can think on i"
    },
    {
      "id": "3",
      "message": "SteveMcQueen#3713: Well that solves that\nDaj#7482: Yea haha, I was an early adopter of TFRC and know the guys in charge semi personally\nDaj#7482: So we have theoretical access to up to 2048 TPU cores whenever\nDaj#7482: (though that many are usually not free)\nDaj#7482: I made a #links channel meant for clean posts of links to papers with at most a paragraph of description, and a more general #deleted-channel channel for discussing papers and such. Sound reasonable?\nbmk#1476: yeah\nDaj#7482: I also"
    },
    {
      "id": "4",
      "message": "Daj#7482: That sounds cool\nbmk#1476: yeah\nbmk#1476: also once we have this dataset and we've trained a GPT3 replica we should publish a paper about it, with the focus being on the dataset\nDaj#7482: You think we could do that? I'd absolutely love to\nbmk#1476: it's worth a shot\nDaj#7482: Though I don't have the connections to get it on arxiv or published personally\nbmk#1476: getting it on arxiv should be trivial\nDaj#7482: But I've written/edited papers before so I think I can be helpful\nDaj#7482: "
    },
    {
      "id": "5",
      "message": "bmk#1476: I'll add a \"current datasources\" section\nbmk#1476: to the document\nDaj#7482: Can you pin your gdoc to the gpt-neo channel?\nbmk#1476: done\nDaj#7482: Lovely!\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/730105119252807782/unknown.png\nbmk#1476: am i missing anything important\nDaj#7482: Seems good to me, though I may be forgetting some thing or the other we had discussed at some point\nbmk#1476: and of these, wikipedia, opencaptions and bookcorpus are done, CC, CORE,"
    },
    {
      "id": "6",
      "message": "Daj#7482: _Larger_ GPT2 won't work without TFM\nDaj#7482: but we could experiment with 1.5B right away basically. Just need to converto tfrecords and upload to google cloud\nbmk#1476: i have no idea how to do that\nDaj#7482: I can write the scripts for it if we have the texts\nDaj#7482: Not that hard\nbmk#1476: what's more worth doing, GPT2+more data or charGPT2&same data\nbmk#1476: I'm willing to help do some more work on the TFM front to help make it happen\nDaj#7482: I think getting a GPT2 to OA lev"
    },
    {
      "id": "7",
      "message": "bmk#1476: ok well we have WT1, BC, Wiki, OC so far\nbmk#1476: I can make some LG from the data i already have downloaded\nbmk#1476: and is the core download done?\nDaj#7482: You can see my old tfrecords scripts in the GPTNeo/datasets folder if you wanna have a look. Though they produce the \"sample randomly\" kind of tfrecords, but GPT3 linearly reads through all the data\nDaj#7482: I think goolu was doing CORE?\nbmk#1476: I think I'll just get everything into either compressed json list or tgz of rand"
    },
    {
      "id": "8",
      "message": "\nTakedown: The 3rd Infantry Divisionâ€™s\n\nTwenty-one Day Assault on Baghdad\n\nPershing: A Biography\n\nThe Canons of Jihad: Terroristsâ€™ Strategy for Defeating America (editor)\n\nThe Making of Peace: Rulers, States, and the Aftermath of War (co-editor, with Williamson Murray)\n\n\n\n\n\nCopyright Â© 2011 by James Lacey\n\nMaps copyright Â© 2011 by Jeffrey L. Ward\n\nAll rights reserved."
    },
    {
      "id": "9",
      "message": "\nPublished in the United States by Bantam Books, an imprint of The Random House Publishing Group, a division of Random House, Inc., New York.\n\nBANTAM BOOKS and the rooster colophon are registered trademarks of Random House, Inc.\n\nGrateful acknowledgment is made to Pantheon Books, a division of Random House, Inc., for permission to reprint excerpts from The Landmark Herodotus: The Histories by Robert B. Strassler, translated by Andrea L. Purvis, copyright Â© 2007 by Robert B. Strassler. Reprinted "
    },
    {
      "id": "10",
      "message": "\nwww.bantamdell.com\n\nJacket design: Christopher Sergio\n\nJacket photograph: Â© National Museum of Scotland / The Bridgeman Art Library\n\nv3.1\n\n```\nbmk#1476: these files are a few hundred kb-1mb anyways so this is a tiny fraction of the data\nDaj#7482: It's not breaking or anything\nDaj#7482: Dunno if it's worth investing effort to clean it\nDaj#7482: Though it wouldn't hurt to clean it ofc\nbmk#1476: eh i'll just leave it\nDaj#7482: Yea probably fine\nbmk#1476: https://gist.github.com/leogao2/dfee376d02b"
    },
    {
      "id": "11",
      "message": "Daj#7482: It might actually be on net worth it to cut out the first page or so. Dunno something we can think about/experiment with down the line\nbmk#1476: ok ill do cutting\nDaj#7482: This is the kind of nitty gritty trial-by-error data people will benefit from later too\nDaj#7482: lol we could do like a statistical test how likely it is that cutting pages deletes copyright or real data using a spot sample (yes I'm studying for my stats exam atm)\nbmk#1476: or should i just give you the raw uncut d"
    },
    {
      "id": "12",
      "message": "Daj#7482: Data science and ML is black magicâ„¢ï¸\nbmk#1476: for SmallLibGenâ„¢ï¸\nDaj#7482: Which is imo a technical term that means \"needs a lot of subjective judgement\"\nDaj#7482: Sounds good\nbmk#1476: should we switch from json to bson for extra speed\nbmk#1476: (with backwards compat)\nDaj#7482: Any speed gain is worth it at these sizes\nbmk#1476: ok\nbmk#1476: really flooring it on my computer https://cdn.discordapp.com/attachments/729741769738158194/730122885863178280/unknown.png\nbmk#1476: > total rec"
    },
    {
      "id": "13",
      "message": "Daj#7482: Just saying\nDaj#7482: Haha\nSid#2121: I actually think I might rather not, idk\nDaj#7482: Nah I getcha\nSid#2121: it's something to think about\nDaj#7482: I just have a twisted sense of humor\nDaj#7482: maybe we'll make GPT-Toxic\nDaj#7482: as a side project\nSid#2121: lmao. just entirely 4chan?\nSid#2121: 4chan distilled into a hyperintelligent ai ono\nDaj#7482: I'm not saying it's a good idea\nSid#2121: it's *an* idea\nDaj#7482: But I am saying it's a _hilarious_ idea\nDaj#7482: We could unleash"
    },
    {
      "id": "14",
      "message": "Daj#7482: Agreed\nDaj#7482: 4chan archives keep popping up and dying all the time, so we might have to look around a bit\nbmk#1476: 4ch archives are really fragmented unfortunately\nDaj#7482: Still, if no one else wants to I would (when I'm more free) try to collect as much as I can lol\nSid#2121: who runs 4ch these days?\nDaj#7482: I have no idea, I haven't been on there in like 5 years\nDaj#7482: >Google 4chan archives\n>\"'My Little Pony' Fans Confront Their Nazi Problem - The Atlantic\"\nDaj#7482: kek"
    },
    {
      "id": "15",
      "message": "Daj#7482: Since it's also probably shit\nbmk#1476: but 4chan isnt just \"high level of toxicity\", it's literally the definition of toxicity on the internet\nDaj#7482: I know worse places tbh\nbmk#1476: on the popular internet\nSid#2121: well, 8ch\nDaj#7482: it's more just 14-18 year olds being edgy\nbmk#1476: *ch\nDaj#7482: But yeah I vote don#t include it in the GPT3 or 1T runs\nbmk#1476: yeah that's the most toxic demographic on the internet\nDaj#7482: But I might privately play with it for shit and gig"
    },
    {
      "id": "16",
      "message": "bmk#1476: i think we should get the more important data, like WT2, before turning to such.. unorthodox sources\nDaj#7482: I hate everyone I met there lol\nDaj#7482: Absolutely\nSid#2121: @bmk how is the AWS link you posted to the enron data 210Gb lol? a source I've found is like 400mb\nSid#2121: how is that formatted\nbmk#1476: attachments\nSid#2121: right. don't really need them\nbmk#1476: ye\nSteveMcQueen#3713: btw even though CC might have the reddit data pushshift has a full dump by month of all com"
    },
    {
      "id": "17",
      "message": "Sid#2121: I mean, the order is important, but I assume the reddit data is ordered anyway, no? at least in some sense\nSteveMcQueen#3713: pushshift data is afaik not, we will literally just get a metric ton of comments/submissions\nSteveMcQueen#3713: which is fine, but it wont learn much besides grammar, syntax and a bit of random information from those as many are fairly short form\nSteveMcQueen#3713: i.e. there will be no representation of responses to e.g. questions or how discussions unfold\nStev"
    },
    {
      "id": "18",
      "message": "Sid#2121: oh boy\nSteveMcQueen#3713: Filtering out the meta data should be pretty easy\nSteveMcQueen#3713: Might be a bit cumbersome and take some time, but setting up the script shouldn't take long\nbmk#1476: ?\nbmk#1476: I've worked with the ps data before and it's not bad\nSid#2121: sure, probably easy to parse, but awful to look at. I was just trying to get a sense of how you can decipher structure from that, but I'd probably need to get it into python to figure it out instead of just staring bla"
    },
    {
      "id": "19",
      "message": "Daj#7482: I mean the GShard people already trained a 1T model it was just shit\ngoolulusaurs#1571: Oh, I haven't read through that one\nDaj#7482: They only briefly mention that they tried but couldn't do it because of \"numerical stability\"\nDaj#7482: iirc\nSid#2121: what does that mean exactly\nSid#2121: is that something we need to worry about?\nDaj#7482: They aren't clear on it in the paper but what I interpret it to mean is something something floating point errors\nDaj#7482: And yes that probably m"
    },
    {
      "id": "20",
      "message": "Sid#2121: ah yes\nSid#2121: that's what i meant\nDaj#7482: So pretty big\nSid#2121: hefty boi\nDaj#7482: Haha\nDaj#7482: The \"1T or bust\" thing is really more of a meme imo. Of course we'll _try_ to make a 1T train but it's not likely to produce anything more than a proof of concept\nDaj#7482: GPT3 sizes are probably the sweet spot with current hardware\nSid#2121: do you have any sources to back that up aside from intuition / the tangential mention in the gshard paper?\nSid#2121: it looked like the grap"
    },
    {
      "id": "21",
      "message": "Sid#2121: there's probably a lot of technical hitches to get past\nSid#2121: that will be better tackled by a proper lab\nDaj#7482: Binary or low precision waits would be super interesting, but TPUs don't support lower than bfloat16\nDaj#7482: iirc\nDaj#7482: Those fancy new A100 GPUs have great support for 8bit apparently\ngoolulusaurs#1571: That video was pretty interesting, btw. They talk about the specific way they split up the computation for their transformer.\nSid#2121: yeah it's cool. wish the"
    },
    {
      "id": "22",
      "message": "Sid#2121: i'm going to start a tfmesh channel in resources and post a few things\nDaj#7482: Yep I imagine the allreduce operations to be a big overhead\nDaj#7482: Sounds good\nDaj#7482: @Sid your colab is not accessible\nSid#2121: oops\nSid#2121: @Daj better?\nDaj#7482: Yep ðŸ‘\nDaj#7482: Cool stuff\nSid#2121: I reckon it's not perfect and there's one or two things broken, but idk if it's mesh related\nSid#2121: the global step never progresses lol\nDaj#7482: Huh strange\nDaj#7482: Maybe it needs to be wrapp"
    },
    {
      "id": "23",
      "message": "Sid#2121: i turned eval off haha\nSid#2121: https://www.youtube.com/watch?v=pjvQFtlNQ-M\nDaj#7482: Yea I added a flag to disable it by default in master too\nDaj#7482: You've definitely made a lot of progress, cool stuff! Once we get GPT in some capacity running on TFM scaling up should (_should_) be nice and easy\nSid#2121: yeah so we may be able to plug elements of their code (attention layers and stuff) into our code, in fact i already have in several places\nSid#2121: but adapting their codebase "
    },
    {
      "id": "24",
      "message": "bmk#1476: can you change it 30 times a second to emulate animated icons\nbmk#1476: *big brain hacker moment*\nDaj#7482: haha\nSid#2121: ah no i haven't built any processing pipeline yet to actually make the variable logos\nSid#2121: but it can do cheatcodes\nSid#2121: up up down down left right left right b a start\nIsaac McHorse#2007: ðŸŽ‰ CHEAT CODE ACTIVATED ðŸŽ‰\nDaj#7482: You can probably just pre make a thousand or so\nDaj#7482: Cheat codes should unlock secret server icons\nNoa Nabeshima#0290: Did you t"
    },
    {
      "id": "25",
      "message": "@Sid Ah yes fair\nSid#2121: plus i can't get my gan model up to my drive lol my upload speed to google is literally non-existent for some reason\nDaj#7482: Strange\nSid#2121: i mean, i can upload pics here\nSid#2121: but for drive it just stalls\nSid#2121: idk\nbmk#1476: someone needs to make a cheaper, more platform agnostic version of aws snowball\nSid#2121: I do have ~1000 fonts stored but i want higher truncation\nDaj#7482: Upload to Hetzner, setup an FTP and download from there? :D\nbmk#1476: Statio"
    },
    {
      "id": "26",
      "message": "bmk#1476: so i'd call up SWFoTaaS and have them mail my data to the Cloudâ„¢ï¸\nbmk#1476: okok\nSid#2121: i'd seed fund SWFoTaaS\nSid#2121: catchy name\nDaj#7482: Can I also order Station Wagons full of other things?\nSid#2121: no tapes only\nDaj#7482: That's a service I want\nDaj#7482: Lame\nSid#2121: you'll have to go to SWFoOTaaS for that kind of thing\nDaj#7482: Imagine being able to just order a station wagon full of any substance or item\nbmk#1476: that's just normal delivery tho, no?\nDaj#7482: You get"
    },
    {
      "id": "27",
      "message": "Special Containment Procedures: SCP-1984 is best kept locked away in a faraday cage on an air gapped network\nDescription: SCP-1984, otherwise known as The Libre Foundation, was created as an experimental 1TB language model, the largest in the world at the time, and was meant to both advance the Foundation by leaps and bounds and to potentially find the homesteader hero, it was the closest the foundation got to a success until SCP-1342 was created. It is what is known as an \"End of the World trig"
    },
    {
      "id": "28",
      "message": "Isaac McHorse#2007: nnel is to be brought in twice a day for testing. The D-class personnel is to be gassed and brought in before sunset, in a manner so it is unable to see SCP-852. After testing is complete, SCP-852 is to be examined before either being disposed of or taken back to its cell.\nThe cell in which SCP-852 is kept is to remain in total darkness at all times. If any light is directed at SCP-852, any personnel responsible for this action are\nSid#2121: oh lawd\nSid#2121: we need a separa"
    },
    {
      "id": "29",
      "message": "bmk#1476: idk but i want to stay on OA's good side\nSid#2121: yeah fair\nbmk#1476: also i modified the prompt again to be the old one but with some minor changes\nbmk#1476: `Description: SCP-1984, otherwise known as The Libre Foundation, was created to study the properties of anomalous language models, GANs and other Autonomous Intelligent Threats.`\nSid#2121: ok cool\nSid#2121: i mean we could have a few different prompts\nSid#2121: heck we could get GPT to generate its own prompts\nDaj#7482: Yea i li"
    },
    {
      "id": "30",
      "message": "Daj#7482: I'm still figuring out how discords permissions work lol\nDaj#7482: We should give the bot an interesting name too\nSid#2121: gib me ideas\nSid#2121: you don't like Servericonchanger??\nDaj#7482: We could ask GPT3?\nDaj#7482: Give it a SCP type prompt and then ask it for the name of the bot or something\nbmk#1476: ok ill be back in like an hour with a fresh load of scps\nDaj#7482: Nice haha\nSid#2121: thank you thank you ðŸ™\nbmk#1476: Inb4 they get mad at me for not using the API to do what I sa"
    },
    {
      "id": "31",
      "message": "Daj#7482: Oh God yes put our names in as scientists of SCP\nDaj#7482: Prompting it with abstracts seems like a good thing to try\nbmk#1476: > extracting information from research papers (i.e to summarize, suggest citations, suggest semantically similar but differently named concepts, etc) to help accelerate research.\nbmk#1476: This is something I want to do but I'm not sure how to design my prompts for it\nSid#2121: yeah that's tricky\nSid#2121: i mean summaries seems straightforward enough, just as"
    },
    {
      "id": "32",
      "message": "bmk#1476: Ok pls help brainstorm ideas lol\nSid#2121: So like â€˜chess is similar to go in that they are both board gamesâ€™ or something?\nSid#2121: But for research domains specifically\nbmk#1476: They should be things that aren't obviously associated\nDaj#7482: Google a medical dictionary and mad lib in some stuff and see what it comes up with\nbmk#1476: One example that comes to mind is `lipofuscin and a2e are similar in that they are both substances that accumulate in cells impeding cell function`\nN"
    },
    {
      "id": "33",
      "message": "bmk#1476: it's close but wrong which is dissappointing\nSid#2121: i have no idea if that's right or not\nSid#2121: ah\nSid#2121: zero-shot?\nbmk#1476: yeah\nbmk#1476: **A2E is like lipofuscin in that both are substances that accumulate in cells impeding cell function**, but whereas lipofuscin accumulates as an intracellular byproduct of normal cellular metabolism, A2E accumulates as an extracellular product of light exposure.\nbmk#1476: woah\nbmk#1476: it went on to produce garbage but this is promisin"
    },
    {
      "id": "34",
      "message": "Finite-state machines are like finite sets in that both can be represented as a series of numbers.\nLambda calculus is like a function in that both can be represented as a series of numbers.```\nbmk#1476: yeah i'm doing that rn\nbmk#1476: the fractals are like programs one is interesting\nSid#2121: is there really no good solution for pdf to text? what's SOTA for that??\nDaj#7482: Yea i have the same question. It feels like there _must_ be a better solution somewhere\nbmk#1476: **Algorithms are like r"
    },
    {
      "id": "35",
      "message": "Noa Nabeshima#0290: I don't know why he thinks iGPT is important as opposed to just using a resnet\nDaj#7482: That I agree with\nDaj#7482: I'm not very familiar with OCR work\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/730521535286083614/model.png\nbmk#1476: people turn images into text all the time\nDaj#7482: But if we want to spin up a PDF to text project I think that would be well worth our time\nbmk#1476: yes\nbmk#1476: that would be awesome\nSid#2121: I'd be interested but"
    },
    {
      "id": "36",
      "message": "Daj#7482: Probably. Did they use PDF to text stuff?\nbmk#1476: no clue\nDaj#7482: Alsoy there is sci hub and libgen\nbmk#1476: i really want to mulch all of bioRxiv if it's not already in CORE\nDaj#7482: I would expect only a minority of papers to be available as anything but PDF\nbmk#1476: er..\nSid#2121: if we get a good PDF to txt solution we *need* to scrape lots of sci-hub links. So yarr-harr\nDaj#7482: Absolutely\nbmk#1476: disclaimer that it varies a lot from dump to dump\nbmk#1476: *however*\nbmk#"
    },
    {
      "id": "37",
      "message": "Sid#2121: let's fuckin yarr harr it up\nbmk#1476: DISCLAIMER:\nbmk#1476: i checked a different one and it was almost all epub\nbmk#1476: so it varies a *lot* from dump to dump\nSid#2121: to clarify you mean REDACTED right\nDaj#7482: Wanna send me the Infos in the YH channel bmk?\nbmk#1476: no, libgen\nbmk#1476: which infos?\nDaj#7482: Oh\nSid#2121: ok let's yarrharr\nDaj#7482: Yea i misunderstood thought this was scihub\nbmk#1476: libgen is more interesting than scihub right?\nDaj#7482: Libgen is already in"
    },
    {
      "id": "38",
      "message": "Sid#2121: *shakes fist* SAME NAME\nIsaac McHorse#2007: Joined the server.\nNoa Nabeshima#0290: Hi!\nNoa Nabeshima#0290: Welcome\nNoa Nabeshima#0290: oh bot\nNoa Nabeshima#0290: sad\nNoa Nabeshima#0290: should we be doing recruitment\nSid#2121: we were just talking about posting the link up in tpupod again but only bc I wanted to show off the SPC bot hahaha\nSid#2121: Should i make the bot welcome ppl?\nNoa Nabeshima#0290: ooh yes\nSid#2121: ok but first\nSid#2121: !scp euclid\nIsaac McHorse#2007: Item #: SC"
    },
    {
      "id": "39",
      "message": "Sid#2121: ```It is considered a safe class object``` uhhh no\nNoa Nabeshima#0290: we need a POOM -ethics channel\nSid#2121: POOM?\nSid#2121: oh\nSid#2121: i hadn't finished reading it >.<\nNoa Nabeshima#0290: POOM\nNoa Nabeshima#0290: actually bloat is bad\nNoa Nabeshima#0290: but colloquially ethics is POOM dept\nNoa Nabeshima#0290: Preserve/Obliviate/Modify (language models)\nDaj#7482: Oh my God that's sort of brilliant\nNoa Nabeshima#0290: > FLETailors, an acronym for Flense Linguistically and Terminol"
    },
    {
      "id": "40",
      "message": "bmk#1476: we should get back to work on the project eventually\nSid#2121: hah, yeah\nSid#2121: also all this should be in #off-topic\nbmk#1476: ye\nbmk#1476: so 1. tfm 2. getting the cc net stuff going\nSid#2121: yep\nSid#2121: 3. potentially PDF to txt down the line, which we should add to the kanban if no one's done it already\nbmk#1476: @goolulusaurs can haz hetzner moar disk?\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/730637106409373766/unknown.png\nbmk#1476: we're barely p"
    },
    {
      "id": "41",
      "message": "Description: SCP-1984, otherwise known as The Libre Foundation, was created to study the properties of anomalous language models, GANs and other Autonomous Intelligent Threats. Currently SCP-1984 demonstrates convincingly human writing styles and can post to Foundation websites using common proxies and no i/o ports. These writings pass 54/54 blind author recognition tests and have been imprinted in the subconscious of 97.5% of all readers.\n\nSCP-1984's containment has currently reached the point "
    },
    {
      "id": "42",
      "message": "Sid#2121: i mean, dumb question sorry\nSid#2121: do you have any writeup or anything, results etc\nl4rz#8278: the writeup is there http://github.com/l4rz/gpt-2-training\nSid#2121: awesome ta\nl4rz#8278: some notes re: dataset collection might be of use to you guys\nl4rz#8278: i found oscar too noisy\nSid#2121: i'm going to post this up in #documentation thanks a lot\nSid#2121: data cleaning is one of the things that slightly up in the air right now\nSid#2121: we're trying to replicate OA's pipeline\nSid#"
    },
    {
      "id": "43",
      "message": "l4rz#8278: or you mean the unshuffled version?\nSid#2121: oh right\nSid#2121: idk, i wasn't really handling that part of the project\nl4rz#8278: ah\nSid#2121: but i have found a link\nSid#2121: so thx\nDaj#7482: Yea we want the unshuffled version\nDaj#7482: Single sentences without context aren't really good for GPT\nSid#2121: oh right. How is it shuffled?\nDaj#7482: Per sentence iirc\nSid#2121: oh it's shuffled *per sentence* ?\nDaj#7482: I think so yes\nSid#2121: that seems... dumb\nDaj#7482: It's how NLP "
    },
    {
      "id": "44",
      "message": "Sid#2121: maybe new channel time\nSid#2121: i think separate papers / links is a good idea\nSid#2121: although right now papers is mostly links ðŸ˜†\nDaj#7482: Yea I wanted it to also include blog posts and the like\nSid#2121: and we've been doing a bit of chatting in there which was not really the intention of the resources section\nDaj#7482: Yea we should delete that\nDaj#7482: I think we all have delete rights\nSid#2121: some of it's useful tho\nDaj#7482: Then we should move it somewhere other than reso"
    },
    {
      "id": "45",
      "message": "asparagui#6391: Joined the server.\nSid#2121: hey @asparagui !\nasparagui#6391: hai\nSid#2121: just posted pdf extraction script up in #datascripts btw, if anyone has any suggestions on how to speed it up with mp / filter bad text it'd be much appreciated https://github.com/sdtblck/PDFextract\nSid#2121: let me know if you have any questions about what's going on here @asparagui ðŸ™‚\nasparagui#6391: working together to replace the humans, da?\nSid#2121: you got it ðŸ˜‰\nbmk#1476: yeah sure\nbmk#1476: also dam"
    },
    {
      "id": "46",
      "message": "bmk#1476: the slight problem being that mesh-tf documentation is next to nonexistent\nbmk#1476: some help would be really awesome\nDaj#7482: Send me your github name and I'll add you to the repo if you'd like\naydao#6272: hmm never used mesh-tf before, though non-existent docs is a universal problem in this field ðŸ˜†\nbmk#1476: dont worry, nobody else has either\nDaj#7482: This server in a nutshell https://cdn.discordapp.com/attachments/729741769738158194/730814288658169876/tf_meme.png\nbmk#1476: like, "
    },
    {
      "id": "47",
      "message": "Daj#7482: Almost\nDaj#7482: This seems like a funny thing to make a bot do\nDaj#7482: Every day rename the general channel\nshawwn#3694: Apparently that wasnâ€™t me, but Iâ€™ll take the blame\nDaj#7482: It's probably irresponsible to give channel edit rights to basically everyone here\nDaj#7482: But eh\nbmk#1476: principle of least perms\nDaj#7482: I guess yeah\nDaj#7482: I think only I can edit channels now, people with project roles still have a few additional permissions like delete and mute or should I "
    },
    {
      "id": "48",
      "message": "shawwn#3694: I mean, it was fine up till 400 users, so I donâ€™t think itâ€™s a bad idea to give out perms. But it is kind of dicey that anyone can kick everyone\nDaj#7482: I'm just not sure about \"LibreAI Inner Circleâ„¢ï¸\" typr roles...though that's just mods I guess\nbmk#1476: this is some serious bikeshedding\nDaj#7482: Yea I should probably revoke kicking and deleting, or assign it on a per-channel basis\nDaj#7482: Yes\nDaj#7482: Sorry\nshawwn#3694: The official solution is to let people mute but not ki"
    },
    {
      "id": "49",
      "message": "Daj#7482: We got the mnist demo running iirc\nshawwn#3694: Thatâ€™s cool\nbmk#1476: half the code is converted to mtf\nshawwn#3694: I didnâ€™t know it could be converted piecemeal.\nbmk#1476: attention is a big stickler but from the looks of it large chunks of the code are yet to be converted\nbmk#1476: oh, i dont think it runs at all yet\nshawwn#3694: Has it been tested on a TPU pod yet? For some reason pods tend to differ from v3-8â€™s in subtle ways\nshawwn#3694: Ah, I see\nDaj#7482: btw maybe move to #gpt"
    },
    {
      "id": "50",
      "message": "bmk#1476: you can ask daj for repo access\nguac#4716: Is the libreAI code base over in Resources?\nguac#4716: (links to it)\nbmk#1476: repo is private\nbmk#1476: those are auxillary repos\nbmk#1476: https://docs.google.com/document/d/1wfCZBd18DMNt6YcC6boPNMd9qzzH3zpHHfKj4dezk0g/edit?usp=sharing\nbmk#1476: lots of info about the project\nbmk#1476: for info specifically about tfm scroll to the last page\nbmk#1476: (that's where most of the help is needed)\nbmk#1476: Sid has been doing most of the tfm stuff"
    },
    {
      "id": "51",
      "message": "SynonymOfHeat#7721: Joined the server.\nSid#2121: ðŸ‘‹ @SynonymOfHeat\nRyn#4094: Joined the server.\nDaj#7482: Hey there! Welcome to the Gentoo of AI Foundations! We're trying to build huge GPT3+ variants, if you have any questions check out the various channels or just ask. And if you wanna help we're always greatful!\nbmk#1476: if you can give me a couple more i can get gpt3 to make a few lol\nDaj#7482: Oh boy I'm running out of OSS memes\nbmk#1476: extend embrace extinguish\nDaj#7482: Not really our mo"
    },
    {
      "id": "52",
      "message": "\nHey there! Welcome to the GPT3-AI: Global Public Trust! We're trying to build huge GPT3+ variants, if you have any questions check out the various channels or just ask. And if you wanna help we're always grateful!\n\nHey there! Welcome to the RATS: Researching Augmented Superintelligence! We're trying to build huge GPT3+ variants, if you have any questions check out the various channels or just ask. And if you wanna help we're always grateful!\n\nHey there! Welcome to the Foundations of the Artific"
    },
    {
      "id": "53",
      "message": "Daj#7482: haha\njhsu#8763: Joined the server.\nRyn#4094: @Sid Hiya!\nSDr#2461: Joined the server.\nbmk#1476: Hello\nSid#2121: If we write an email to the folks at Google Brain / TFMesh - what do we actually want to ask?\nnoahtren#9042: Joined the server.\nbmk#1476: hey @noahtren !\nguac#4716: Is the README in the mesh repo the only documentation...\nbmk#1476: #documentation\nbmk#1476: there's a doc there\nguac#4716: ah thanks!\nbmk#1476: an there's some tfmesh info that sid wrote down\nSid#2121: also we have"
    },
    {
      "id": "54",
      "message": "Sid#2121: some ppl in the github saying it doesn't work very well, and i think tfm said it was untested (or not very tested) on gpus\nDaj#7482: I'm sure the TFM people would appreciate the feedback\nguac#4716: yeah i can only imagine lol I only figure i give it a shot since i noticed they use gpus in the README example...\nDaj#7482: Or maybe not seeing how much of their code is unfinished lol\nKoen#2924: Joined the server.\nSid#2121: Hey @Koen !\nSid#2121: We're trying to build huge GPT3 variants here"
    },
    {
      "id": "55",
      "message": "Sid#2121: Welcome to the tensorflow mesh wastelands\ngwern#1782: what did you guys decide about efficient attention etc?\ngwern#1782: because if you go with dense attention and unidirectional models, I'm not sure how worthwhile 1t would be\nbmk#1476: hey gwern!\nbmk#1476: we're just trying to get local attention working for now\nSid#2121: yeah, we're not really at the stage\nSid#2121: tfm is horribly convoluted and very badly documented\nbmk#1476: GPT3 is evidence that local attention with interspersed"
    },
    {
      "id": "56",
      "message": "bmk#1476: i'd like to know too\nbmk#1476: i thought we didnt want this posted on twitter u.a\nSid#2121: ah lol\nSid#2121: @shawwn 's been lurking ðŸ˜‰\nSid#2121: https://twitter.com/theshawwn/status/1282388857531633664/photo/1\nshawwn#3694: Oh.\nshawwn#3694:  https://cdn.discordapp.com/attachments/729741769738158194/732026587465121792/unknown.png\nshawwn#3694: I read \"No no, we're not secret, just project focused, your discord is fine\" and missed the bit about twitter.\nSid#2121: oh i don't think anyone's "
    },
    {
      "id": "57",
      "message": "Sid#2121: plus, we could do with putting some more people to work ðŸ‘€\nSid#2121: @everyone who wants a job\nbmk#1476: certainly\nbmk#1476: aaaa dont do the ping\nSid#2121: i have the power to ping, i shall ping\nbmk#1476: okok\nbmk#1476: anyways if you;re here and wanna help pls let us know\nNoa Nabeshima#0290: Yeah, what can I do?\nbmk#1476: we'd love to use all the help we can get\nSid#2121: data gathering seems high priority at this point\nbmk#1476: ^\nSid#2121: we should update the kanban / documentation"
    },
    {
      "id": "58",
      "message": "Noa Nabeshima#0290: And then somehow taking chunks that are high probability?\nSid#2121: we want to filter out raw text that appears to be part of a contents / index / glossary etc.\nSid#2121: @bmk didn't you do something similar using fasttext\nNoa Nabeshima#0290: I don't grok how SOTA does text filtering\nSid#2121: i would think it would be more like tokens, idk\nbmk#1476: ok so\nbmk#1476: can you post some examples of  current failure cases\narfa#0882: :Pingree:\nSid#2121: yep one sec\nbmk#1476: > aaa"
    },
    {
      "id": "59",
      "message": "@Sid if you're ever using TF2 or Keras for this project, I'm happy to weigh in. also if you're planning on using TFRecords I could possibly help\nSid#2121:  https://cdn.discordapp.com/attachments/729741769738158194/732032532278804520/A_Guide_to_the_New_Ruins_of_Great_Britain_-_Owen_Hatherley.txt\nSid#2121:  https://cdn.discordapp.com/attachments/729741769738158194/732032533935423508/Everything_and_More_-_A_Compact_History_of_-_David_Foster_Wallace.txt\nSid#2121:  https://cdn.discordapp.com/attachme"
    },
    {
      "id": "60",
      "message": "Sid#2121: https://tenor.com/view/simpsons-homer-bart-lisa-join-us-gif-13066208\nshawwn#3694: journalists are cRaZy for GPT news\nshawwn#3694: hmm maybe\ngwern#1782: they weren't crazy for gpt-3 news even though gpt-3 is so much cooler ðŸ˜¦\nsh33mp#7259: gpt-3's zero-shot shtick is harder to understand\nSid#2121: is it tho? looking back on gpt-2 stuff after seeing gpt-3 results is underwhelming to say the least\nshawwn#3694: that's an interesting point\nSid#2121: i guess it's way more interesting if you un"
    },
    {
      "id": "61",
      "message": "zitterbewegung#4846: Hi\nzitterbewegung#4846: Do we have a website\nzitterbewegung#4846: Libreai.net is available\nDeleted User#0000: GPT-LVM-XFS\nDeleted User#0000: wait wrong chat\nDeleted User#0000: this is ai not linux my bad\nzitterbewegung#4846: Hi\nzitterbewegung#4846: I registered libreai.net\nzitterbewegung#4846: I can make a wordpress blog on it or square space\nzitterbewegung#4846: And give people admin access\nzitterbewegung#4846: Or I can point it to whoever you guys want\nzitterbewegung#4846:"
    },
    {
      "id": "62",
      "message": "Daj#7482: We don't really have any organization or hierarchy though\nzitterbewegung#4846: thats fine\nzitterbewegung#4846: ill pay for hosting costs in the foreseeble future ill probably setup wordpress or something first\nDaj#7482: That's very kind of you but I wouldn't want you paying money before we're even sure what we're doing lol\nSid#2121: ^\nSid#2121: we don't even have gpt-2 replicated yet\nzitterbewegung#4846: i mean like domain registration\nDaj#7482: Domain registration is definitely nice y"
    },
    {
      "id": "63",
      "message": "Daj#7482: Lucky we made the resources channels to work off of\nzitterbewegung#4846: @Daj I have a titan rtx\nSid#2121: yup. I wonder if i can get @Isaac McHorse to post up details from our kanban on command\nzitterbewegung#4846: @Daj and access to a threadripper\nSid#2121: ```threadripper``` what's that\nDaj#7482: That's pretty awesome actually. Is it connected to a high bandwidth uplink? Since moving huge amounts of data can be the bottleneck\nSid#2121: ah, computer thang\nDaj#7482: Threadrpper is an "
    },
    {
      "id": "64",
      "message": "Sid#2121: well, we don't need one. But we like the idea and we'd like to include it\nDaj#7482: Yea IRCs would be super fun\nbmk#1476: the largest one is ubuntu\nzitterbewegung#4846: oh\nbmk#1476: and that's easily downloaded\nbmk#1476: (for some definition of easily)\nzitterbewegung#4846: i can get gigabytes of twitter data\nDaj#7482: That's interesting, hmmm\nDaj#7482: Not sure if that's good data or not\nDaj#7482: Pros and cons for short tweets, though concatenating threads would work\nDaj#7482: That wo"
    },
    {
      "id": "65",
      "message": "zitterbewegung#4846: i mean per day\nzitterbewegung#4846: https://archive.org/details/archiveteam-twitter-stream-2018-10\nzitterbewegung#4846: oh ok nvm\nDaj#7482: This is neat! Please post a link in #data-sources\nzitterbewegung#4846: okay done\nDaj#7482: Thanks!\nSid#2121: damn, had no idea this twitter stream existed\nSid#2121: cool, thanks for that\nDaj#7482: I wonder if it has the data needed to reconstructs threads\nDaj#7482: I feel single tweets are a bit short\nSid#2121: probably, yeah\nSid#2121: a"
    },
    {
      "id": "66",
      "message": "Daj#7482: but still better than nothing\nzitterbewegung#4846: ive worked with twitter data\nSid#2121: @Daj Thanks Elon!! it totally works! wow!\nDaj#7482: haha\nDaj#7482: sounds cool zitter, if you wanna look into the twitter data and how we could get it into a nice format that sounds like a cool project to me, or sorting by likes and searching for links etc\nSid#2121: gonna add it to #documentation google doc\nzitterbewegung#4846: like what do we care about ? the text itself, the id of the user and t"
    },
    {
      "id": "67",
      "message": "bmk#1476: still really short context\nzitterbewegung#4846: its less garbage than 0\nbmk#1476: we want really long stuff\nSid#2121: yes ```we could gather links from tweets with > x likes\n```\nDaj#7482: Less than 10% of WT is >1000 tokens\nDaj#7482: iirc\nbmk#1476: thankfully all of yarrharr is really long\nzitterbewegung#4846: likes and retweets\nDaj#7482: Yup that's why I have high hopes there\nbmk#1476: also how much overlap with cc will all of this have\nDaj#7482: I don't think CC crawls social media\nS"
    },
    {
      "id": "68",
      "message": "bmk#1476: there is a way without using the api\nDaj#7482: I'm fully in favor bmk, iff we have the resources lol\nzitterbewegung#4846: creative commons scrapes it\nzitterbewegung#4846: they record like less than 1 percent of tweets\nDaj#7482: Yea the full 100% costs money\nzitterbewegung#4846: its sort of in a legal grey area that we don't have to wowrry about\nzitterbewegung#4846: cuz its someone elses dataset\nbmk#1476: you can get the 100% if you have the infra without paying\nbmk#1476: it's possible\n"
    },
    {
      "id": "69",
      "message": "bmk#1476: we're openai but all open\nbmk#1476: open source everything\nDaj#7482: Just a bunch of hackers having fun with cutting edge ML\nbmk#1476: open model everything\nDaj#7482: Yup\nbmk#1476: a bit more yarrharr than oa, though Books2 do be looking suspicious\nDaj#7482: but yea re:Tweets that's why I think tweet threads or links are most interesting\nDaj#7482: Yea we don't have to please any corporate shareholders we can do what we want lol\nbmk#1476: I'm fairly confident that Books2 is actually fil"
    },
    {
      "id": "70",
      "message": "Daj#7482: And money and finals\nbmk#1476: we need to get pdf processing first anyways\nDaj#7482: Yea\nzitterbewegung#4846: i think counter is the wrong word i mean\nzitterbewegung#4846: yea i meant to say what you said\nDaj#7482: I figured, just wanted to be explicit that we want to cooperate with the wider AI world :)\nzitterbewegung#4846: i believe in reproducable science\nbmk#1476: me too\nDaj#7482: btw have a fancy color name\nSid#2121: ok, updated #documentation\nSid#2121: pls let me know if i missed"
    },
    {
      "id": "71",
      "message": "Daj#7482: What should I update the channel desc to?\nzitterbewegung#4846: last medium post was 10 months ago https://medium.com/libreai/the-joy-of-neural-painting-e4319282d51f\nDaj#7482: Eh I wouldn't worry about it, PR and that kind of stuff is post-proof of concept stuff\nSid#2121: just something super simple like, welcome to LibreAI, please look at our google doc for a description of the project and the tasks we need doing\nbmk#1476: also can we adopt Books2 as a euphemism for yarrharr data too\nS"
    },
    {
      "id": "72",
      "message": "Sid#2121: ðŸ‘\nbmk#1476: where is CORE downloaded to?\nbmk#1476: stretch goal https://cdn.discordapp.com/attachments/729741769738158194/732247310653849620/unknown.png\nDaj#7482: lol\nSid#2121: jesus\nDaj#7482: typical digital squatting unfortunately\nbmk#1476: libreai.ai is available\nbmk#1476: or libreai.io\nDaj#7482: Well yea\nDaj#7482: for 25k\nbmk#1476: I like io domains\nDaj#7482: oh wait\nDaj#7482: libreai.io is sexy\nbmk#1476: twice the ai\nSid#2121: i like libreai.io yeah\nbmk#1476: shall i pick it up\nSi"
    },
    {
      "id": "73",
      "message": "Sid#2121: just want tpu brr\nDaj#7482: but it sounds good to me if you don't mind paying\nbmk#1476: same\nbmk#1476: sure ill pay for it\nDaj#7482: Cool we can pair it up with zitter's domain and that's a pretty nice presence I think\nDaj#7482: Whenever we get to that\nSid#2121: yeah. I'm glad the already existing libre ai seems to have done fuck all interesting work\nDaj#7482: Yea we're the coolâ„¢ï¸ LibreAIâ„¢ï¸\nzitterbewegung#4846: lolol\nbmk#1476: ok we own libreai.io now https://cdn.discordapp.com/attachm"
    },
    {
      "id": "74",
      "message": "bmk#1476: thatsthejoke.svg.tar.gz\nDaj#7482: Excuse me I'm not used to people knowing German!\nDaj#7482: haha\nbmk#1476: ah ok\nDaj#7482: but yeah PR is bikeshedding atm. LibreAI could just live as a loose collection of blog posts and a discord server it doesn't really matter. What matters is results! haha\nIsaac McHorse#2007: I'M NOT WORK ING! I'M JUST PLAYING!\nbmk#1476: yes\nbmk#1476: let's all get to pdf cleaning\nbmk#1476: this is possibly the most important and reusable piece of software libreai p"
    },
    {
      "id": "75",
      "message": "\n- If book - maybe cut out first & last pages (?) (could we train a cover page classifier?)\n\n- Blank lines\n\n- Lines shorter than N characters w no letters (or minimal amount of letters below a threshold)\n\n- CID fonts (https://stackoverflow.com/questions/53252844/python-pdf-to-txt-only-return-cidnumber).\nSometimes these are only part of the text, but if they take up a certain percentage we should just bin the whole thing.\nExample:\n\n\"which maintained contacts not least in  the  South  East  Asian "
    },
    {
      "id": "76",
      "message": "\"T he  experts  from  m a ny  countries  working  on  this  project  began  by\nlaying  d o wn  the  theoretical  and  methodological  basis  for the History.\nT h ey \"\n``` my notes on filtering\nSid#2121: I can do it too since I know my code, if you think you could be more effective elsewhere\nbmk#1476: one idea:\nbmk#1476: we can use a spell check module to fix a lot of this\nDaj#7482: btw #the-pile for posterity\nDaj#7482: Easier to filter for info later\nbmk#1476: ok\nshawwn#3694: Bikeshedding\nIsaac "
    },
    {
      "id": "77",
      "message": "Daj#7482: Ehh we'll see\nDaj#7482: I'd probably email Zak and Jonathan personally once we have a proof of concept\nbmk#1476: alright ðŸ‘\nbmk#1476: so right now we're basically just waiting for tfrecording to finish then?\nDaj#7482: I'd say we need at least a 1.5B with identical or superior performance to the original, better yet a GPT3 variant\nDaj#7482: I think so?\nDaj#7482: If everything works we cna start training tomorrow\nbmk#1476: awesome\nbmk#1476: wait can you test the mtf code on a pod?\nbmk#147"
    },
    {
      "id": "78",
      "message": "old#3101: you guys probably know this but i racked up about $100 in stackdriver costs (covered by the free credit tho) before i turned off stackdriver log ingesting for tpus\nDaj#7482: Yea I racked up around 1100â‚¬ lol\nDaj#7482: Luckily the TFRC people pulled some strings since seemingly I was the first one to report that to them last year\nzitterbewegung#4846: thats why i bought a titan rtx\nzitterbewegung#4846: you make one mistake\nzitterbewegung#4846: you could have bought a video card instead\nzi"
    },
    {
      "id": "79",
      "message": "Daj#7482: Good luck lol\nzitterbewegung#4846: https://encrypto.de/papers/RST19.pdf\nDaj#7482: No way that will work in practice I think\nzitterbewegung#4846: yea probably not\nDaj#7482: GPT3 is like 400GB of weights?\nDaj#7482: or something ridiclous\nzitterbewegung#4846: the largest one?\nDaj#7482: I don't recall off the top of my head\nDaj#7482: Cool paper though thanks for the link\nDaj#7482: Though even if it would work I wouldn't do it, because it's, and this is a totally technical term, a \"dick mov"
    },
    {
      "id": "80",
      "message": "zitterbewegung#4846: https://www.amd.com/en/products/cpu/amd-ryzen-threadripper-1920x\nbmk#1476: 24 threads\nbmk#1476: that's 3x our current machine o.O\nbmk#1476: so as a rough estimate we need 100ish core-months to do GPT3 size data, and double-triple that for 1T\nbmk#1476: so that's 4 months with that machine\nbmk#1476: asdding my machine, the hetzner, maybe another similar hetzner , that's 2 months\nbmk#1476: where can we get another 50 cores\nbmk#1476: also we need a similar, maybe even more, amou"
    },
    {
      "id": "81",
      "message": "sh33mp#7259: for cleaning /preprocessing the text?\nSid#2121: yep\nzitterbewegung#4846: can we parallelize it easily?\nDaj#7482: Yea\ngoolulusaurs#1571: > where is CORE downloaded to?\n@bmk It's at /data/CORE\ngoolulusaurs#1571: Another option is Libre.ai\nDaj#7482: btw goolu, do we owe you money for the HDD?\nDaj#7482: I lost track\ngoolulusaurs#1571: I'd say save it, since we will probably have plenty of other costs over the course of the proj\nDaj#7482: You sure? Greatly appreciated, of course\nDaj#7482"
    },
    {
      "id": "82",
      "message": "Daj#7482: The AGI Wranglers reminds me back when me and my friend wanted to name our game company, we already had the fantastic name \"Shy Tea Games\", but our close second was \"Cult of the Electric Shepherd\"\nDaj#7482: I need to use that name in some project some day\nzphang#7252: ðŸ‘‹\nJeremiah#1201: How much CPU is needed?\nSid#2121: *many*\nSid#2121: i think bmk posted some preliminary calculations somewhere\nSid#2121: hang on\nSid#2121: ```eh a few hundred core-months\npossibly moreâ€”we don't know how co"
    },
    {
      "id": "83",
      "message": "Sid#2121: define 'acceptable'\nSid#2121: I'm not 100% if we have that going yet / have a script for that\nSid#2121: I think it might be TODO / in progress\nSid#2121: @bmk kanban says you're working on CC?\nJeremiah#1201: No copyright/non fair use\nSid#2121: also github / wikipedia which i think @Noa Nabeshima 's working on\nSid#2121: CC is just publicly hosted and available to download iirc\nSid#2121: it's just going to take massive amounts of processing\nSid#2121: I don't think there's any copyright\nJe"
    },
    {
      "id": "84",
      "message": "Sid#2121: (he has an API key)\nSid#2121: I think we're doing CORE as well https://core.ac.uk/services/dataset/\nSid#2121: but i haven't been as involved in the processing of these big datasets as much the others, I've mainly been working on the model and a few smaller datasets\nJeremiah#1201: Cool\nJeremiah#1201: Is there any coding a noob can do?\nSid#2121: most of what we're doing is in the google doc linked in the channel description / #documentation\nSid#2121: How noob are we talking?\nSid#2121: we"
    },
    {
      "id": "85",
      "message": "bmk#1476: compute things we need: 1. CC processing 2. LG depdficication 3. dedup\nSid#2121: LG is a no\nbmk#1476: ok\nbmk#1476: that's the most compute intensive probably, dammit\nbmk#1476: and if LG is a no, dedup is automatically no\nSid#2121: yeah :/\nbmk#1476: is CC ok?\nSid#2121: you'd have to ask him, I don't see why it wouldn't be since it's a publicly available, legal dataset used for genuine academic purposes\nbmk#1476: @Jeremiah do you think downloading and filtering CC is an acceptable use ca"
    },
    {
      "id": "86",
      "message": "Sid#2121: did we ever decide on a script for CC?\nJeremiah#1201: Ok\nbmk#1476: my script works mostly fine\nbmk#1476: idk what changes i have to make to run it on a cluster\nJeremiah#1201: Are you releasing the model?\nbmk#1476: yes\nbmk#1476: we're openai but like actually open\nSid#2121: yep. Although I'm not 100% we've decided *exactly* how, it seems most of us are keen on a full release\nSid#2121: see #alignment-general\nJeremiah#1201: Great\nSid#2121: ah, didn't see your script\nSid#2121: thanks @Jere"
    },
    {
      "id": "87",
      "message": "Sid#2121: yeah, that's awesome\nSid#2121: I should really email TFRC already and get my own\nSid#2121: but there's so many floating around that i have somehow managed to wangle access to lmao\nJeremiah#1201: I requested an extension and a pod instead of 110 single ones with the reason: \"I want to train a really high resolution StyleGAN\" and haven't heard back...\nbmk#1476: once we get a 1T model i'm sure tfrc will shower us all with lots of credits\nbmk#1476:  https://cdn.discordapp.com/attachments/7"
    },
    {
      "id": "88",
      "message": "bmk#1476: https://www.gwern.net/Melatonin\nbmk#1476: (of *course* gwern has written an article about it)\nSid#2121: hah, i was about to say\nbmk#1476: it feels like gwern is omnipresent on the internet\nbmk#1476: any topic, and comment section, bam, he's there and he's written a very high quality detailed post on it already\nJeremiah#1201: Is gwern.net in your training data? I guess it's in common crawl?\nbmk#1476: the rule of common crawl is \"don't assume anything particular is in it\"\nJeremiah#1201: "
    },
    {
      "id": "89",
      "message": "\nâ€œThe same thing happened with the LessWrong Sequences. It was not a rigorous introduction to rationality. It was a sequence of blog posts that you were supposed to read in order and treat as a sort of narrative. It turned out to be too confusing for people to read sequentially, so they read it out of order, skipped parts, and came away with a garbled version of it.â€\n\nâ€” Gwern Branwen (gwern.net)```\nbmk#1476: close but eeeeeeeeeeeeh\nbmk#1476: so it knows gwern has to do with LW and stuff but then"
    },
    {
      "id": "90",
      "message": "Sid#2121: that's the facebook tool to download common crawl?\nSid#2121: just tryna get up to date\nbmk#1476: yes\nbmk#1476: i personally dont like it\nbmk#1476: but we dont have enough cpu power currently to do it my way\nSid#2121: what's the differences of this vs yours\nSid#2121: ah\nbmk#1476: ~~my way is better~~\nSid#2121: well if @Jeremiah can do it, I trust your opinion\nSid#2121: I'm guessing there's some overlap, and we can skip out what we've already downloaded from cc_net? or if we decide to do"
    },
    {
      "id": "91",
      "message": "Jeremiah#1201: Oh another question, can I do that download in chunks or is it all at once?\nbmk#1476: it's split into chunks\nbmk#1476: and can be spread over however many machines you have\nbmk#1476: merging the chunks is pretty easy too\ngoolulusaurs#1571: I think in cc_net they also had some precomputed files, According to this github issue it  needs \"like 10TB\". https://github.com/facebookresearch/cc_net/issues/7\nbmk#1476: yeah, and \"like 10TB\" is apparantly at the high end of that, closer to 20"
    },
    {
      "id": "92",
      "message": "goolulusaurs#1571: I can add another disk if we need to.\nbmk#1476: once it crashes due to lack of space sure\nJeremiah#1201: It's not listed as a usage limit, but I don't know what the capacity is.\nbmk#1476: I'll keep giving it every last bit of drive space we have\nbmk#1476: ok this does not add up\nbmk#1476: according to GPT3 paper, it was 45TB over 41 shards\nbmk#1476: just over 1TB per shard\nbmk#1476: but in that link i posted they say each shard is almost 8TB\nbmk#1476: even compression only usu"
    },
    {
      "id": "93",
      "message": "bmk#1476: 50TB bandwidth is a reasonable estimate\ngoolulusaurs#1571: they do say \"The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl\" .  \"from\", not that they are the whole shard.\nbmk#1476: what is a \"shard\"\nbmk#1476: there are multiple levels of abstraction here they could be referring to\nbmk#1476: 41 full monthly dumps is hundreds of TB\nbmk#1476: like probably 300\nbmk#1476: nearly an order of magnitude over their number\nbmk#1476: unless they're just sampling 1/10 of WETs"
    },
    {
      "id": "94",
      "message": "bmk#1476: if we can secure the cores and bandwidth\nbmk#1476: also just ran the numbers: if we can get 112k cores we can download and process all 400TB in 9 seconds\nSid#2121: ...\nSid#2121: .. . ..\nSid#2121: cool\nSid#2121: how much do 112k cores cost for 9 seconds\nbmk#1476: probably a lot\ngoolulusaurs#1571: yeah and we only need 45TB/s download speed\nbmk#1476: also 400TB of ingress too\ngoolulusaurs#1571: Also, apparently CC uses Apache Nutch\nSid#2121: I mean, it's good to know it's there for when "
    },
    {
      "id": "95",
      "message": "bmk#1476: hmm, ok i may have messed something up\nSid#2121: @bmk can you try this https://twitter.com/components_ai/status/1282379087412174848?s=19\nSid#2121: I need verification\nbmk#1476: one moment\nJeremiah#1201: How much data can you give gpt-3 in your query?\nbmk#1476: 2048 tokens\nbmk#1476: aha ok\nbmk#1476: finally it's working\nNoa Nabeshima#0290: @bmk Can you try it with words instead of emojis?\nbmk#1476: wait one momeny pls\nbmk#1476: busy trying to fix a thing\nbmk#1476: it turns out that i am"
    },
    {
      "id": "96",
      "message": "bmk#1476: this happens sometimes\nbmk#1476: it's just randomly guessing colors, i think\nSid#2121: So are you saying this twitter person is lying to us\nSid#2121: I am shocked\nbmk#1476: it's not *lying*\nbmk#1476: it's just cherrypicked\nSid#2121: well *misleading*\nbmk#1476: in a low entropy environment\nbmk#1476: so yes\nbmk#1476: misleading very\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/732403016032059473/unknown.png\nSid#2121: lmao\nbmk#1476: fwiw it seems to work better wit"
    },
    {
      "id": "97",
      "message": "Sid#2121: i should sleep now anyway ðŸ‘‹\nbmk#1476: it kinda failed violet and lightgreen\nNoa Nabeshima#0290: Night!\nbmk#1476: ok cya\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/732403734277259284/unknown.png\nbmk#1476: success..?\nbmk#1476: better https://cdn.discordapp.com/attachments/729741769738158194/732403846965493790/unknown.png\nNoa Nabeshima#0290: peach: [\n#fddad2,\n#fcd73d\n#ffd425,\n#fdda1c,\n#fccf0e,\n#fbd81b,\n#fcb74f,\n#fc823b\n]\ntree: [\nJeremiah#1201: picard: [\nJeremiah#"
    },
    {
      "id": "98",
      "message": "Jeremiah#1201: ],\nkirk: [\nbmk#1476: what's with the npy\nbmk#1476: @Noa Nabeshima https://cdn.discordapp.com/attachments/729741769738158194/732405126521815070/unknown.png\nbmk#1476: eeeeh\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/732405281711063240/unknown.png\nJeremiah#1201: It was an image, doubt that one will work.\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/732405466470154270/unknown.png\nbmk#1476: I'm pretty sure trees are not purple\nbmk#1476:"
    },
    {
      "id": "99",
      "message": "bmk#1476: why are we uploading to IA?\nJeremiah#1201: Downloading common crawl and uploading the data to the internet archive is better than downloading it and uploading it to a private server.\nbmk#1476: wait, really?\nbmk#1476: why is that?\nbmk#1476: wouldnt that add an additional bottleneck\nJeremiah#1201: Yes, but since it's a research cluster, I should be putting it out there for everyone, right?\nbmk#1476: sure\nbmk#1476: it's still unfinished data though\nbmk#1476: like, we need to process it fu"
    },
    {
      "id": "100",
      "message": "bmk#1476: aight sounds good\nDaj#7482: Got my GPT3 invite too yay\nNoa Nabeshima#0290: me too ðŸ™‚\nNoa Nabeshima#0290: We should try some fine-tuning when it becomes available\nDaj#7482: I think we can ask for access to the finetuning API\nNoa Nabeshima#0290: But what are going to finetune it on that it hasn't meta-learned?\nDaj#7482: That's the bigger question haha\nNoa Nabeshima#0290: I feel like I need to make an API because I said I would\nNoa Nabeshima#0290: but I have no good ideas\nNoa Nabeshima#029"
    },
    {
      "id": "101",
      "message": "Noa Nabeshima#0290: yeah\nDaj#7482: Pretty sure the Google/Siri people aren't gonna share their data hah\nSid#2121: would it need finetuning to be a personal assistant tho ?\nNoa Nabeshima#0290: @Sid I think so if you want it to interface with say google calendar and gmail.\nI have faith it can be done with few examples\nNoa Nabeshima#0290: Praise scaling\nSid#2121: i mean, i don't see why that needs gpt\nDaj#7482: Do you need finetuning if few shot works so good?\nSid#2121: just voice to txt then parsi"
    },
    {
      "id": "102",
      "message": "Daj#7482: haha nah actually surisingly my schedule is good lately\nNoa Nabeshima#0290: Ooh I wonder if you could get it to consistently w/o profanity or messing up tell good children's stories\nNoa Nabeshima#0290: 'Pirate stories', 'Fairytales', 'Sci-Fi'\nDaj#7482: Oooh interesting idea\nSid#2121: lmao, child management ideas\nSid#2121: gpt, put my kid to bed\nDaj#7482: The new version of the TV babysitter\nDaj#7482: What could go wrong?\nSid#2121: and suddenly the child grows up thinking horses have si"
    },
    {
      "id": "103",
      "message": "Daj#7482: Realistically it's really astounding how far a bunch of totally unafiliated and unorganized dudes on a discord got already just because Google had spare compute laying around\nSid#2121: to be honest, I am still so confused about why google is giving out so much compute. Do ppl really hate TPUs that much\nSid#2121: and I mean yeah. It's been like a week lol. The Pile is growing steadily, and we have a mesh model about to run\nDaj#7482: Apparently they do? I dunno I asked the guy in charge "
    },
    {
      "id": "104",
      "message": "Daj#7482: Hey @JonathanFly ! Welcome to MIT License OpenAI! Check the channel topic for info on what we're doing and what you can do to help, if you want.\nDaj#7482: Also, I think I follow you on Twitter hah\nSid#2121: ðŸ‘‹\nSid#2121: we're getting a proper webring going here, our new tfmesh nation is gathering steam\nJonathanFly#4262: I haven't done anything with TPUS and won't be any help there, just checking in on progress\nSid#2121: we're just trying to run our first model, you picked a good time to"
    },
    {
      "id": "105",
      "message": "Daj#7482: I was one of the very first people in TFRC, have met them personally, etc, so I think they give me a bit of special treatment. I've had a preemptible v3-2048 and a handful of v3-8s basically whenever I ask for it\nasparagui#6391: ahh kk\nSid#2121: do you think with a poc we could get a non-preemptible 1024 / 2048 or is that unheard of\nDaj#7482: Unheard of from what I know. When I talked to Zak (the guy in charge), he basically said the preemptibles are almost free for them because they r"
    },
    {
      "id": "106",
      "message": "Sid#2121: > eg tpu1 --> working --> prempted --> start tpu2 --> checkpoint?\n@asparagui this would be a nice thing to get sorted\nDaj#7482: Pretty sure that's what `pu babysit` is for\nDaj#7482: (from tpunicorn)\nSid#2121: oh really?\nSid#2121: i was meaning to ask how to use that\nDaj#7482: It basically starts the worker process, checks if a TPU is preempted, if so it kills the process, recreates the TPU, then once it's up runs the command again\nDaj#7482: iirc\nSid#2121: We'll need to sort out our bro"
    },
    {
      "id": "107",
      "message": "bmk#1476: or are we the first to bring the glory of the webring to discord\nSid#2121: ðŸ¤·\nSid#2121: what other AI discords are there?\nbmk#1476: not sure\nbmk#1476: 2min papers?\nbmk#1476: i'm in the discord but i dont really visit\nSid#2121: i thought i remember someone saying there was an AI dungeon discord?\nSid#2121: or was it a slack\nbmk#1476: Â¯\\_(ãƒ„)_/Â¯\nDaj#7482: We are the OG Discord AI webring\nDaj#7482: This is our claim to fame\nSid#2121: shawwn says ```also IMO rename #webring to #communities an"
    },
    {
      "id": "108",
      "message": "Sid#2121: got any links to papers?\nbmk#1476: is this a continuation of a convo elsewhere?\nSid#2121: yeah sorry\nSid#2121: @Skylion thinks we won't be able to replicate GPT-3\nbmk#1476: why not?\nbmk#1476: I'm curious\nSid#2121: hasn't got to that part yet\nbmk#1476: we have about 70% of the tech that OA used\nSkylion#0368: Reformer for instnace, but I think that one is out of date.\nbmk#1476: currently we're trying to figure the last 30%, mostly gpipe\nbmk#1476: so we've settled on local attention + reg"
    },
    {
      "id": "109",
      "message": "bmk#1476: we dont have to use their exact code tied into lingvo though\nbmk#1476: also if we dont have that many sections we can maybe even do without gpipe\nSkylion#0368: https://github.com/tensorflow/lingvo/blob/master/lingvo/core/gpipe.pyhttps://github.com/tensorflow/lingvo/blob/master/lingvo/core/gpipe.py\nSkylion#0368: Oh okay\nbmk#1476: it only becomes a real problem when every layer is on a different device\nSid#2121: @Skylion do you want an invite to our repo? it'd be super valuable to have s"
    },
    {
      "id": "110",
      "message": "bmk#1476: for some reason i have to make the batch size *really small* to fit it on the 512 ;-;\nbmk#1476: even with data parallel\nDaj#7482: For the record I consider our chances of fully replicating GPT3 to not be top quartile either but it doesn't matter it's fun and educative and we'll make something cool\nSid#2121: which confiiiiiig\nSid#2121: ^^ yeah\nbmk#1476: ```{\n\"n_head\": 32,\n\"encoder_path\": \"gs://datasets_storage_1/models/encoder\",\n\"n_vocab\": 50257,\n\"embed_dropout\": 0.1,\n\"lr\": 0.00025,\n\"wa"
    },
    {
      "id": "111",
      "message": "\"eval_steps\": 0,\n\"max_steps\": 500000,\n\"data_path\": \"gs://neo-datasets/bundestag\",\n\"res_dropout\": 0.1,\n\"predict_batch_size\": 1,\n\"eval_batch_size\": 32,\n\"iterations\": 500,\n\"n_embd\": 2048,\n\"datasets\": [[\"bundestag_*.tfrecords\", \"\", 10, \"random_sample\", 1.0]],\n\"data_path_\": \"gs://neo-datasets/openwebtext-fixed/\",\n\"datasets_\": [[\"openwebtext_*.tfrecords\", \"\", 10, \"chunks\", 1.0]],\n\"model\": \"GPT2\",\n\"model_path\": \"gs://neo-models/NEO_TEST_1\",\n\"n_ctx\": 128,\n\"predict_path\": \"logs/predictions.txt\",\n\"n_layer"
    },
    {
      "id": "112",
      "message": "\"local\": true,\n\"mesh_shape\": \"x:16,y:32\",\n\"layout\": \"embd:y, heads:y, batch:x\"\n}\n```\nSid#2121: that breaks? or that's the best we have running\nbmk#1476: data parallel 16\nbmk#1476: er\nbmk#1476: i'm about to find out\nSid#2121: cool\nDaj#7482: This is unsurprising, it's bigger than 1.5B right?\nSid#2121: n_ctx is only 128\nbmk#1476: marginally\nbmk#1476: wait i thought it was 1024 ;-;\nDaj#7482: 1.5B plus Adam is way too big for a single core by default\nbmk#1476: can someone add adafactor\nSid#2121: shal"
    },
    {
      "id": "113",
      "message": "Daj#7482: And I'm sure the model parallelism is adding some overhead somewhere\nbmk#1476: @Daj how many adafactor batch size can you fit on 512?\nbmk#1476: pre-meshtf\nSkylion#0368: Don't use Adam\nSid#2121: yeah we should check the reshapes\nDaj#7482: 512, one per core\nSkylion#0368: Use the other optimizer\nbmk#1476: 512!??\nDaj#7482: That was how I trained, with adafactor\nSkylion#0368: Yeah\nSkylion#0368: Adafactor != Adam\nSid#2121: ok ok let me add adafactor\nDaj#7482: Why not skylion? OA used adam\nSi"
    },
    {
      "id": "114",
      "message": "Skylion#0368: It was my understnading they used Adam until they go to Big and Very Big models\nDaj#7482: Don't get me wrong it's good news if true lol\nSkylion#0368: but I could be misremembering.\nSkylion#0368: Like they used Adam for medium and small\nDaj#7482: Is this GPT2 or 3?\nbmk#1476: > To train all versions of GPT-3, we use Adam withÎ²1= 0.9,Î²2= 0.95\nSkylion#0368: GPT-2\nDaj#7482: Interesting\nSkylion#0368: Ah, for GPT-3 they probably used Adam because it's only a 2X memory penalty\nSkylion#0368"
    },
    {
      "id": "115",
      "message": "Sid#2121: what\nbmk#1476: this does not look right at all why did the original code look like this\nSid#2121: the decay rate setting??\nbmk#1476: yeah\nbmk#1476: i am very confused\nSid#2121: I'm pretty sure that was in Daj's code\nDaj#7482: Because I was experimentig I think?\nDaj#7482: I don't remember\nDaj#7482: Lol\nbmk#1476: this does not look right\nDaj#7482: Then please fix thx\nSid#2121: for both opts??\nbmk#1476: i dont know if it'll be more right if we change it tho\nDaj#7482: I haven't looked at t"
    },
    {
      "id": "116",
      "message": "print(f\"N TRAINABLE VARS: {total_parameters:,}\")\n^\nSyntaxError: invalid syntax\n```\nbmk#1476: @Sid\nSid#2121: Huh O.o one sec not at a computer\nbmk#1476: i think this python just doesnt support fstrings\nSid#2121: I thought I pushed that earlier and it worked fine\nbmk#1476: yeah its 3.5\nSid#2121: Ah ok Iâ€™ll take it out\nbmk#1476: fstrings bad\nSid#2121: Fstrings good\nSid#2121: Python 3.5 bad\nSid#2121: this should work right ```    print('{:,}'.format(total_parameters))\n```\nbmk#1476: batchsize 1 per s"
    },
    {
      "id": "117",
      "message": "bmk#1476: we need to optimise\nSid#2121: what's the layout?\nSid#2121: ^yeah\nbmk#1476: heads:x,embd:x,batch:y\nbmk#1476: x:32,y:16\nSid#2121: wait so our batch size is 16??\nSid#2121: am i misunderstanding\nbmk#1476: yes, 32x smaller than the 512 we should be getting\nSid#2121: i thought you had bigger batches earlier\nSid#2121: @bmk remember when i said we could add a print statement if it's doing the inefficient reshapes\nSid#2121: we shld do that\nbmk#1476: Sure go ahead\nbmk#1476: We're having ooms tho"
    },
    {
      "id": "118",
      "message": "bmk#1476: cant get more than a few dozen batch size ;-;\nbmk#1476: what the hell\nbmk#1476: i dont think daj used bf16 for the original gpt2 either\nbmk#1476: we really need to do some serious optimization\nbmk#1476: like, we cant even do the 512 batch size\nbmk#1476: And this isn't even full attention\nbmk#1476: This is local attention\nbmk#1476: What are we doing wrong\nSid#2121: I wish i knew :/ I mean, the main changes are to the attention right?\nbmk#1476: And the whole mtf\nSid#2121: well yeah\nbmk#1"
    },
    {
      "id": "119",
      "message": "bmk#1476: who here has the charisma necessary\nbmk#1476: \"Noam Shazeer\" sounds like the main person we need to contact\nbmk#1476: First author on the paper and active committer on the repo\nSid#2121: ye\nSid#2121: i found a few of the people on twitter the other day hah\nSid#2121: I can write a message tomorrow\nbmk#1476: Alright sounds good\nbmk#1476: Hol up\nbmk#1476: Shazeer is one of the authors on Gshard\nbmk#1476: Maybe we can get him to hook us up with Gshard too\nSid#2121: shazeeeeer\nSid#2121: he'"
    },
    {
      "id": "120",
      "message": "Sid#2121: Oh wow, Noam Shazeer co-authored Attention is all you need\nSid#2121: didn't realize\nSid#2121: seems like a big deal\nzitterbewegung#4846: does anyone feel like more params = better\nzitterbewegung#4846: do you think there will be diminishing returns?\nbmk#1476: the modus operandi of libreai *is* moar params = moar better\nbmk#1476: welcome to actually openâ„¢ ai!\nSid#2121: Hey @Key ! let us know if you want a task ðŸ™‚ there's plenty to do. And welcome to something something we're more open tha"
    },
    {
      "id": "121",
      "message": "psgeorge#6388: > Hmmm idk. Weâ€™re open to any suggestions but we need a place to store / process all the data before we put it onto the bucket, so we have a hetzner. Price calculations / storage etc are things I havenâ€™t really been dealing w though.\n@Sid Storing & Processing on a hetzner because data processing on gcloud is difficult or expensive?\narchivus#7382: v expensive on buckets - you're charged on data egress\npsgeorge#6388: > Thought about contacting https://www.reddit.com/user/-Archivist?"
    },
    {
      "id": "122",
      "message": "aster#3007: Joined the server.\nDeleted User#0000: Joined the server.\nSid#2121: ðŸ‘‹ðŸ‘‹ðŸ‘‹\nkindiana#1016: Joined the server.\nSid#2121: Hey @kindiana\nSid#2121: Welcome to The AGI Wranglers! Check the channel description and resources channels for info and don't hesitate to ask if you have questions ðŸ™‚\nSid#2121: your tokenization sounds super interesting\nSid#2121: when can we expect paper heh\nkindiana#1016: thanks! AAAI's deadline is sept 9 so hopefully before then ðŸ˜› , but I'm happy to discuss it here if y"
    },
    {
      "id": "123",
      "message": "Daj#7482: Sure!\nSid#2121: I posted @kindiana's gist up in #links\nDaj#7482: Give me like 5 minutes\nSid#2121: sorry to distract!\nDaj#7482: Eh it's fine, I'm probably as prepared as I'm gonna be anyways\nSid#2121: when's the exam\nDaj#7482: Tomorrow\nDaj#7482: It's an easy test, I just didn't do shit for it hah\nbmk#1476: Did you send the email to mesh tf yet @Sid\nSid#2121: nah sorry i got sucked into stylegan debugging but i can start writing it now\nSid#2121: do we even have an address to send it to?\n"
    },
    {
      "id": "124",
      "message": "bmk#1476: Also try begging them for Gshard\nSid#2121: i keep getting gshard and gpipe confused, hah\nSid#2121: i'd also be interested *which* dimensions we should and can split exactly, and what's the best practice for our wpe / wte stuff\nSid#2121: but that's i guess not a question for them\nSid#2121: I found a good talk by noam in my insomnia stage last night i'll post it up in a bit\nbmk#1476: hyouklee@google.com second author on Gshard, author on mtf\nSid#2121: it's basically a rehashing of the tf"
    },
    {
      "id": "125",
      "message": "Sid#2121: oh man we should update the kanban :/ lots of new tasks popping up\nSid#2121: did you see the thing i posted in #links  / can you see the new hidden channel?\nbmk#1476: Please do\nbmk#1476: And yes\nSid#2121: so ```- test MOE model, devise architecture. - reach out to TFM authors - reach out to Archivist``` to add to kanban\nSid#2121: ```-test memory consumption with ctpu profiler```\nbmk#1476: Yeah\nbmk#1476: I didn't even know there was a profiler how do we use it\nSid#2121: ( if you test on"
    },
    {
      "id": "126",
      "message": "bmk#1476: This reshape was in original GPT2 as well tho\nSid#2121: yeah but reshapes are different in TFM\nbmk#1476: So why is it using so much memory for us\nbmk#1476: Oh\nSid#2121: there's a whole section in the paper on it i thought we were clear on this\nbmk#1476: Ok so reshape bad\nSid#2121: i mean, not *always*\nSid#2121: but i think best to avoid\nbmk#1476: Ctrl+F reshape to eliminate\nSid#2121: as far as i understand tho it should be a communication bandwidth problem rather than a memory one but "
    },
    {
      "id": "127",
      "message": "bmk#1476: this should still use strictly less memory\nSid#2121: re memory use\nbmk#1476: second, you *cannot* have two dims of same name in a tensor\nbmk#1476: so if you want an embd -> embd, you have to *rename* one of the input or output\nSid#2121: no but i'm saying\nSid#2121: we have our input\nbmk#1476: you cant have an embd x embd matrix\nSid#2121: with [batch, seq, i forget]\nSid#2121: and then at some point, we do\nSid#2121: dim_combined_batch_sequence = mtf.Dimension('combined_batch_sequence', ba"
    },
    {
      "id": "128",
      "message": "bmk#1476: i think that still doesnt explain it\nbmk#1476: that only uses more inter-tpu bandwdith\nbmk#1476: so it's slower, yes\nSid#2121: no, that's not right\nbmk#1476: but should never use more memory\nSid#2121: because combined_batch_sequence then isn't being split\nbmk#1476: so what?\nSid#2121: and will be stored on every core?\nSid#2121: taking up more memory?\nbmk#1476: it was never being split inthe original either\nSid#2121: batch was\nSid#2121: so if we're splitting along batch then reshaping to"
    },
    {
      "id": "129",
      "message": "Sid#2121: Idk if having a stray bfloat would affect the memory, or if it was just that op that happened to be bfloat\nbmk#1476: bfloat thing?\nbmk#1476: oh\nbmk#1476: what would it take to convert everything to bfloat?\nSid#2121: there's a commit\nSid#2121: i mean, i think theoretically not a lot\nSid#2121: just anywhere we specify dtype change to bfloat and i guess change the input?\nbmk#1476: *theoretically* this is tpus everythng is hard\nSid#2121: in toy_model.py they just set a master dtype and use"
    },
    {
      "id": "130",
      "message": "bmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/732974205687300126/unknown.png\nbmk#1476: these too?\nbmk#1476: why did you change\nDaj#7482: lol the tensor is 32 but initializer is 16?\nDaj#7482: Seems like a bug/typo\nSid#2121: huh? i don't think i changed that\nbmk#1476: o.O\nSid#2121: from what to what\nSid#2121: what was it before\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/732974993259888731/unknown.png\nSid#2121: I.... wow i must have been half asleep\n"
    },
    {
      "id": "131",
      "message": "Sid#2121: i mean\nSid#2121: let's do priorities ok\nbmk#1476: i'll deal with bf16ification\nSid#2121: i'm so bad at self organisation\nSid#2121: and i still need to read the moe paper\nSid#2121: I'll do the kanban as first priority\nSid#2121: then i shall use the kanban to device my next priority, hah\nDaj#7482: Have we tested whether the reshapes help with the memory use?\nSid#2121: AH! i replicated the oom error\nbmk#1476: after removing that reshape, i'm currently testing\nDaj#7482: Cool cool\nSid#2121:"
    },
    {
      "id": "132",
      "message": "bmk#1476: which op is einsum_21/einsum/XlaEinsum ??? o.O\nDaj#7482: v3s have 16GB\nDaj#7482: just to explain why it OOMed on 13GB of memory use\nSid#2121: i have no idea which op that is, but i can fix the reshape and see if the error's the same\nSid#2121: einsum does the same thing? O.o\nSid#2121: praise the lord for einsum\nSid#2121: if it does the same thing that should our way to do reshapes, since you can keep the batch name the same\nSid#2121: or wait, did you just take it out for testing. gotta "
    },
    {
      "id": "133",
      "message": "comparator = less if exclusive else less_equal\nm = cast(\ncomparator(mtf_range(x.mesh, dim, dtype=tf.float32),\nmtf_range(x.mesh, new_dim, dtype=tf.float32)), x.dtype)\nret = einsum([x, m], output_shape=new_shape)``` like this (mtf.cumsum)\nbmk#1476: I think I know why they did the even odd layers now\nSid#2121: altho i don't know if *renaming* a dimension means we need to split that too\nSid#2121: or if mtf handles that\nSid#2121: oh yeah?\nbmk#1476: You can't have same name both input and output for o"
    },
    {
      "id": "134",
      "message": "new_name = \"tmp_seq\"\nnew_dim = Dimension(new_name, sequence_dim.size)\nnew_shape = h.shape.rename_dimension(sequence_dim.name, new_name)\n\nnew_name = \"tmp_vocab\"\nnew_dim = Dimension(new_name, vocab_dim.size)\nnew_shape = h.shape.rename_dimension(vocab_dim.name, new_name)\n\nlogits = mtf.einsum([h, wte], output_shape=[batch_dim, sequence_dim, vocab_dim])```\nbmk#1476: Note to self: add to passive aggressive blog post\nSid#2121: it's growing https://cdn.discordapp.com/attachments/729741769738158194/73298"
    },
    {
      "id": "135",
      "message": "Sid#2121: doesn't work\nbmk#1476: which by definition we cant really ask about\nbmk#1476: the what happened\nDaj#7482: If we ask someone publishing papers at Google to look at our code we won't get a response\nSid#2121: same error\nbmk#1476: hmm\nSid#2121: i guess rename_dimension doesn't rename inplace?\nbmk#1476: what do we ask him?\nSid#2121: i probably need to explicitly do it\nDaj#7482: We need to ask _very specific_ questions that fit in maximum 1-2 paragraphs\nbmk#1476: if we know what is possibly "
    },
    {
      "id": "136",
      "message": "old_name: a string\nnew_name: a string\nReturns:\na Tensor\n\"\"\"\nreturn reshape(x, x.shape.rename_dimension(old_name, new_name))``` am i going mad, isn't this recursive\nbmk#1476: the things that are causing us problems wont even fit in those 2 paragraphs\nDaj#7482: Before we can formulate our questions it's probably not worth emailing imho\nDaj#7482: Haha\nDaj#7482: Fair\nDaj#7482: Just my 2ct\nbmk#1476: ok so\nSid#2121: i agree fwiw\nbmk#1476: let's put off the email thing\nbmk#1476: archivist on the other "
    },
    {
      "id": "137",
      "message": "Daj#7482: Cool so you might be best to contact him\nSid#2121: AGH I HATE THIS LIBRARY\nbmk#1476: oh no i hate *interacting with people*\nSid#2121: mtf.rename_dimension() does a different thing to tensor.shape.rename_dimension()\nbmk#1476: @Sid\nbmk#1476: here's how i do it\nDaj#7482: lol if you really don't want to contact him I can make a reddit account sometime and do it\nbmk#1476: ```    x = mtf.reshape(x, x.shape.rename_dimension(x.shape[-1].name, 'tmp_channels'))\n```\nSid#2121: poifect thanks\nbmk#1"
    },
    {
      "id": "138",
      "message": "Daj#7482: Oh no we will\nbmk#1476: im all for writing one\nDaj#7482: But we will give everyone a polite headsup\nDaj#7482: imo\nDaj#7482: Otherwise feels kinda rude\nbmk#1476: yeah that's fair\nDaj#7482: And we will be polite in the blogpost ofc\nDaj#7482: Just frustrated lol\nSid#2121: @bmk i think you can just do this tho ``` x = mtf.rename_dimension(x, old_name, new_name)```\nbmk#1476: so polite, in fact, you might call it\nbmk#1476: *passive aggressive*\nbmk#1476: you can\nDaj#7482: Point taken\nbmk#1476"
    },
    {
      "id": "139",
      "message": "Sid#2121: I mean that's github issue stuff right?\nbmk#1476: when you rename it might do inter device comms\nSid#2121: it does\nbmk#1476: and yes you do\nSid#2121: because internally, it's a reshape\nbmk#1476: but it's a bit more complicated than that\nbmk#1476: conv1d takes dim_in -> dim_out\nbmk#1476: they *cannot be equal*\nSid#2121: O.o\nbmk#1476: and you *cannot split both simultaneously*\nSid#2121: huh??\nbmk#1476: it makes sense if you think about it\nbmk#1476: there needs to be a [dim_in, dim_out] m"
    },
    {
      "id": "140",
      "message": "Sid#2121: ok i kinda get it\nSid#2121: AH\nSid#2121: yes\nSid#2121: the click happened\nSid#2121: alright woah ok\nSid#2121: so it wasn't just poc\nSid#2121: that's clever\nSid#2121: let's odd even\nbmk#1476: *click*\nSid#2121: so if i'm understanding correctly: dim_in and dim_out need to differ\nbmk#1476: yeah\nSid#2121: *but* you can go dim_in --> dim_out --> dim_in ---> dim_out etc etc indefinitely?\nbmk#1476: yep\nSid#2121: s oyou need the odd even?\nbmk#1476: yep\nSid#2121: yessss\nSid#2121: wow you'd thin"
    },
    {
      "id": "141",
      "message": "bmk#1476: this is just in their toy model\nSid#2121: ok this is gonna help a lot\nSid#2121: ah we have so many optimizations to do\nbmk#1476: so dims we can eliminate: `tmp_channels`\nSid#2121: i love it when something clicks. this good\nbmk#1476: actually that's the main one we can eliminate i think\nbmk#1476: there's one thing that i'm miffed about the lack of support for: the ability to assign multiple mesh dimensions to a dimension\nbmk#1476: like say your mesh is a:16,b:32\nbmk#1476: and you want t"
    },
    {
      "id": "142",
      "message": "Sid#2121: you are splitting across all cores\nbmk#1476: you want to split embd across both the a and the b\nbmk#1476: you cant\nSid#2121: how would that even work\nSid#2121: like, imagine the tensor as a square like in the diagrams\nbmk#1476: just send one chunk to each processor along both of those dimensions\nbmk#1476: yeah\nSid#2121: how would you draw the dividing line\nbmk#1476: you draw the dividing line the same\nSid#2121: but if you're dividing across another dimension too\nbmk#1476: but you put i"
    },
    {
      "id": "143",
      "message": "Sid#2121: ```    # equivalent to tf.matmul\nnew_name = \"tmp_batch\"\nold_name = h.shape[0].name\nh = mtf.rename_dimension(h, old_name, new_name)\n\nnew_name = \"tmp_seq\"\nold_name = h.shape[1].name\nh = mtf.rename_dimension(h, old_name, new_name)\n\nnew_name = \"tmp_vocab\"\nold_name = h.shape[2].name\nh = mtf.rename_dimension(h, old_name, new_name)\n\nlogits = mtf.einsum([h, wte], output_shape=[batch_dim, sequence_dim, vocab_dim])``` like, this should work right\nSid#2121: i'm getting the einsum has lhs dimensio"
    },
    {
      "id": "144",
      "message": "Sid#2121: because you need to rename for einsum no ?? i'm confused\nSid#2121: did you run the code you pushed?\nbmk#1476: because dsometimes you *dont have* the \"other\" dimension\nbmk#1476: yes my code works\nSid#2121: hm\nbmk#1476: it breaks because of the rename to tmp_\nSid#2121: ah\nbmk#1476: why are you renaming? o.O\nSid#2121: i know what's going on, my bad. i thought that was the thing to do but i know what the problem is now\nSid#2121: does the einsum not need new names as the output shape tho? i"
    },
    {
      "id": "145",
      "message": "bmk#1476: conv1d needs different names\nbmk#1476: (only for the output dim)\nSid#2121: https://tenor.com/view/screaming-internally-dead-inside-screaming-snapped-gif-8097478\nbmk#1476: err @Daj what does this mean https://cdn.discordapp.com/attachments/729741769738158194/732992864321142934/unknown.png\nDaj#7482: Uhh probably preempted, or corrupted state\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/732992985704431657/unknown.png\nbmk#1476: hm.\nbmk#1476: how do recreate\nDaj#7482"
    },
    {
      "id": "146",
      "message": "Sid#2121: ah i'd have to log into my github on the server, wouldn't work\nSid#2121: @bmk i can't for the life of me understand why my einsum op isn't working. (I'm testing on gpt_moe.py). can you tell me what i'm doing wrong\nSid#2121: code: ```    print('INTO EINSUM SHAPE:')\nprint(h)\nprint(wte)\nlogits = mtf.einsum([h, wte], output_shape=[batch_dim, sequence_dim, output_dim])```\nSid#2121: prints: ```INTO EINSUM SHAPE:\nTensor[ln_f/add_2:0, Shape[batch=32, sequence=128, moe_out=768], <dtype: 'float3"
    },
    {
      "id": "147",
      "message": "Sid#2121: different shapes\nbmk#1476: right now youre giving it two dimensions with different names how is einsum supposed to know theyre actually the same\nbmk#1476: why is that dim called moe_out anyways\nbmk#1476: why not just make it the same as output_dim\nSid#2121: it *is* output_dim\nbmk#1476: ?\nSid#2121: like, moe_out is the name of output_dim\nbmk#1476: ah\nSid#2121: i'm so confused\nbmk#1476: that threw me for a loop\nSid#2121: yeah sorry, bad naming conventions\nSid#2121: will change but this w"
    },
    {
      "id": "148",
      "message": "Sid#2121: the same shape?\nbmk#1476: the same dim\nbmk#1476: name\nSid#2121: O.o\nbmk#1476: this naming is garbage\nSid#2121: ok vyou've explained enough lol, thanks\nSid#2121: but i still don't get it\nSid#2121: i'll keep reading\nbmk#1476: please change every name to the same as the dim in it\nSid#2121: i don't wanna make you explain the same thing over and over\nbmk#1476: otherwise this is impossible to debug\nSid#2121: yeah ok\nSid#2121: i'm going out for a bit anyway, don't worry about the moe stuff i'"
    },
    {
      "id": "149",
      "message": "Daj#7482: It shouldn't be?\nbmk#1476: o.O\nDaj#7482: You can try another pu recreate\nbmk#1476: ok\nDaj#7482: #the-faraday-cage-archive is now our go to spot for putting any fun GPT/GAN/etc stuff\naquajet#7800: Joined the server.\nPasha Khosravi#1571: Joined the server.\nDaj#7482: Hello! Welcome to our AGI Breeding Program! Check the channel description for our current status and don't hesitate to ask questions :)\npashmak#7502: Joined the server.\nDaj#7482: ðŸ‘‹\npashmak#7502: Hi ðŸ˜„\narfa#0882: Uh oh https://"
    },
    {
      "id": "150",
      "message": "Daj#7482: It would be a dream to work with Hugging Face\narfa#0882: I'm jealous\nDaj#7482: Join us!\narfa#0882: :feck2:\nTD-3#9327: Joined the server.\nDaj#7482: Don't be too jealous, if you knew the horrors we've been through with TFM haha\nDaj#7482: Hello @TD-3 ! Welcome to the LM Grazing Fields! Check the channel topic for our current status and don't hesitate to ask questions!\nshawwn#3694: could the webring be moved higher?\narfa#0882: Not unless you move it higher in TPU Podcast :thonk:\nDaj#7482: "
    },
    {
      "id": "151",
      "message": "Isaac McHorse#2007: ? JUST DO IT !\nshawwn#3694: I think it should be at the top, because it's the first thing people see when they join the server. why does every decision have to be analyzed forever?\nDaj#7482: Data processing scripts sans tfrecords encoding are in various other repos\nzphang#7252: oh they're not centralized yet\nSid#2121: @zphang all the data processing scripts are in #datascripts yeah\nSid#2121: ah\nSid#2121: we should centralize\nDaj#7482: Nope it's a bit of a mess atm\nDaj#7482: i"
    },
    {
      "id": "152",
      "message": "Sandeep#0543: Joined the server.\nDaj#7482: >>>>>famous\nDaj#7482: As if\narfa#0882: Well FWIW, if 1997 Webring is at the bottom and I *haven't* muted it, whenever a new server gets added there I'll be sure to check it out because it'll be my only ping\nshawwn#3694: Yeah, I really think the webring should be its own category\nDaj#7482: Hello @Sandeep ! Welcome to the Society For Ethical Treatment of Language Models! Please check the channel topic for our current status and don't hesitate to ask quest"
    },
    {
      "id": "153",
      "message": "Daj#7482: Though test time now, talk to you guys later!\nSid#2121: ah! good luck!\nucalyptus#6163: Joined the server.\nSid#2121: Hey (again ?) @ucalyptus , welcome to H.A.L aka Help Access Language-Models aka LibreAI. Check the channel description for info, and please shoot any questions you have our way.\nPolytropian#8925: Joined the server.\nbla15e#3588: Joined the server.\nSid#2121: Hey @Polytropian & @bla15e . Welcome to LibreAI, where we waste time thinking up unique welcome messages instead of w"
    },
    {
      "id": "154",
      "message": "mysterefrank#2954: ðŸ™\nshawwn#3694: I knew it. it's called pray, not high-five. Everyone always says that's a high-five\ncc_#1010: Joined the server.\nmysterefrank#2954: oh damn I never saw the high five\ncc_#1010: oi\nSid#2121: oi oi\nshawwn#3694: o/ ceeps\ncc_#1010: i wave hellow\nSid#2121: welcome @cc_ pls halp. pls gib cpu. can everyone tell i'm struggling to come up with new welcome messages\ncc_#1010: i will if i can spare it\nSid#2121: check the channel description for a project overview and let me "
    },
    {
      "id": "155",
      "message": "Sid#2121: awesome! welcome\ncc_#1010: i did like... 6 ish days ago?\ncc_#1010: dunno how long the waiting list is\nSid#2121: I think they're just being selective\nSid#2121: also they're gonna start making people pay soon-ish\ncc_#1010: hmm\ncc_#1010: thats lame\ncc_#1010: also it was ten days ago, time is an illusion\ncc_#1010: https://twitter.com/drilbot_neo/status/1280001644219052032\nSid#2121: i mean, incredible they've been able to do the inference they have done for free so far, but yeah. that's why"
    },
    {
      "id": "156",
      "message": "cc_#1010: uhhh i have\nwintermute#5623: Joined the server.\ncc_#1010: two linodes lmao\ncc_#1010: and\ncc_#1010: a 2015 macbook pro\ncc_#1010: and\ncc_#1010: intel core i7 9750H with 2.6 ghz\nSid#2121: Hello @wintermute ! Welcome to the tensorflow mesh wastelands! Read the Channel description for more information on the project and how you can help out.\nshgidi#5693: Joined the server.\narfa#0882: I can donate 2000 bitcoins for everyone who sends me 1000 bitcoins :heck:\nSid#2121: ok cool @cc_ , best to a"
    },
    {
      "id": "157",
      "message": "cc_#1010: my parents are also rich and i can siphon money off them lmao\ncc_#1010: im not sure if i can single handedly manage 6 terabytes money but i can probably put a dent in it\nSid#2121: > my parents are also rich and i can siphon money off them lmao\n@cc_ lmao\nSid#2121: i hope you're not joking and i will take advantage of this\ncc_#1010: i am not\nSid#2121: we'll tell them it's an investment or a time share or sth\ncc_#1010: i make 45k in a cushy office post-grad job doing very little and i spe"
    },
    {
      "id": "158",
      "message": "Sid#2121: yeah\ncc_#1010: advantage #3 of me: i have a moderate amount of clout?\ncc_#1010: lol\nSid#2121: @ghost2718 ðŸ‘‹ ðŸ‘‹  welcome we make big GPT pls halp us\ncc_#1010: but also its 4 am so im going to bed\nSid#2121: well nice, welcome in, i'm sure we could use your clout at some point for sure\ncc_#1010: y'know you could probably just download any old twitter bot and get a welcome function lmao\nSid#2121: we already have @Isaac McHorse we just need the prompts\nSid#2121: if you like bots check out #th"
    },
    {
      "id": "159",
      "message": "cc_#1010: lets you do searches, scrape metadata, etc.\nmaxime#4123: Joined the server.\ncc_#1010: working on an in-discord reader with tts options for people with needs for that\ncc_#1010: bookmark exact spots in the page\ncc_#1010: the like\nSid#2121: that sounds cool\ncc_#1010: anyway im gonna head to bed now because i could probably talk for hours given the opportunity\nSid#2121: is code open sourced?\ncc_#1010: no\nSid#2121: hah ok i won't keep you up too long\nSid#2121: ðŸ‘‹\ncc_#1010: idk i know its lik"
    },
    {
      "id": "160",
      "message": "cc_#1010: Not Yours\ncc_#1010: i spent 8 hours today prying around with mongodb so i could keep track of server statuses (so no kids can accidentally search for the porn) and user win/loss records because it has art prompts too\ncc_#1010: so that was gratifying\nSid#2121: Can we get drilbot in here lol\ncc_#1010: i mean\ncc_#1010: there is no drilbot... bot\nSid#2121: there could be\ncc_#1010: if i had a gpt-3 key yes\ncc_#1010: or some gpt-2 api\nSid#2121: you could just pre-generate a ton, but yeah. I'"
    },
    {
      "id": "161",
      "message": "Isaac McHorse#2007: WHY WOULD I PLAY?! YOU ARE THE SOBBING ONE\nSid#2121: Hey there @dmytrodee ! Welcome to git clone openai; git branch LibreAI\nSid#2121: hahahaha ok shawwn\nSid#2121: you are correct\nManju#1531: Joined the server.\nPerditus#2503: Joined the server.\nDaj#7482: Hello @Manju @Perditus welcome to the Tensorflow Anonymous Self Help Group aka LibreAI. Please check the channel topic for info on our status and don't hesitate to ask questions :)\nSid#2121: @Daj you're back? how'd the test go"
    },
    {
      "id": "162",
      "message": "Daj#7482: Hello @Narsil   ! Welcome to The Merry Band of LM Trainers aka LibreAI! Check out the channel topic for info and don't hesitate to ask questions!\nDaj#7482: Man at this point it's just sport to see how many more of these I can come up with\nNarsil#9151: @Daj Don't you have model to generate these ? ðŸ˜„ Thanks btw!\nDaj#7482: Not yet haha\nDaj#7482: But soonâ„¢ï¸\nP-Dog#9402: Joined the server.\nDaj#7482: Hello @P-Dog   ! Welcome to Mom: We have OpenAI at home, OpenAI at home: LibreAI! Check out t"
    },
    {
      "id": "163",
      "message": "pragmaticml#1730: Joined the server.\njusthoughts#6515: Joined the server.\nDaj#7482: Hello @lugig @pragmaticml @justhoughts    ! Welcome to the Home For Wayward Language Models! Check out the channel topic for info and don't hesitate to ask questions!\nBalGadot#9361: Joined the server.\nAlexM#2612: Joined the server.\nSid#2121: hoo boy i think that twitter post is blowing up\nSid#2121: Hey @BalGadot , @AlexM !\nBalGadot#9361: Oh indeed, hey there!\nDaj#7482: Here have your customary custom introduction"
    },
    {
      "id": "164",
      "message": "mathew#7618: Hello everyone glad to be here!\npwang99#3791: I have only two GPUs to spare\nDaj#7482: Lurkers are always welcome! of course people wanting to help is even better\nDaj#7482: Our bottleneck is currently CPU to preprocess data funnily enough\nSid#2121: > I have only two GPUs to spare\n@pwang99 we're using TPUs for training but we need a lot of compute for preprocess\nSid#2121: ah\nSid#2121: ^\nDaj#7482: And data collectors/TPU coders\nSid#2121: mainly tpu coders ðŸ˜¶\npwang99#3791: Iâ€™m trying to "
    },
    {
      "id": "165",
      "message": "Daj#7482: GPT3 is _big_\nDaj#7482: (and we want bigger!)\nlillux#2099: Joined the server.\nSri Harsha M End-October 2021#1627: Joined the server.\nadam90#4807: Joined the server.\nbinal#2982: Joined the server.\nSid#2121: Hey @lillux , @Sri Harsha M End-October 2021 , @adam90 , @binal ! Welcome to the AI Fallout Shelter â„¢ï¸\nSid#2121: please check the channel description for a general project overview and ping us with any questions\nSid#2121: *wipes sweat* so much welcoming\nDaj#7482: Maybe I shouldn't ha"
    },
    {
      "id": "166",
      "message": "Sid#2121: Hey @murbard , welcome to OpenAI's even more open younger brother, LibreAI â„¢ï¸ ! check the google doc in the project description for more info\nmurbard#5141: > Hey @murbard , welcome to OpenAI's even more open younger brother, LibreAI â„¢ï¸ ! check the google doc in the project description for more info\n@Sid  ty\nSkyros#0881: Joined the server.\nsri#3423: Joined the server.\nlillux#2099: Actually I do research on molecular dynamics, simulating self assembling peptides and analyzing their topol"
    },
    {
      "id": "167",
      "message": "Daj#7482: Hi @wobbithobbit @MarcAK ! Welcome to the OpenAI of OpenAI! Please see the channel topic for info and don't hesitate to ask questions!\nIvanc2#9346: Joined the server.\nbmk#1476: Wow so many new folks!\nDaj#7482: Hello @Ivanc2  ! Welcome to the Library of Babylon (Compressed into latent space for TPU compatibility)! Please see the channel topic for info and don't hesitate to ask questions!\nbmk#1476: we can use all the help we can get lol\nDaj#7482: Yea bmk, founder of Hugging Face tweeted "
    },
    {
      "id": "168",
      "message": "Daj#7482: We're making real progress for sure\nDaj#7482: But it's a big project\nDaj#7482: I'm surprised we got this far tbh\nbmk#1476: does anyone of the recently joined have access to a lot of cpu power\nshawwn#3694: we have 64 cores, though I'm not sure that's \"lots\"\nDaj#7482: Better than our 8 lol\nbmk#1476: Er, about an order of magnitude more would be my very rough estimate\nDaj#7482: I mean, it's not that less isn't worth it\nbmk#1476: ^\nDaj#7482: Since we can be training smaller models while da"
    },
    {
      "id": "169",
      "message": "Daj#7482: Would be super cool Vanya! Happy to spin out some research papers too\nDaj#7482: We have plenty of experiments worth writing up\nbmk#1476: oh yes, research papers would be awesome\nshawwn#3694: by the way, I've been thinking of making my own attempt at GPT-3. Feel free to revoke my access to avoid conflict of interest, though to be honest I doubt I'll get anywhere either way.\nshawwn#3694: mesh tensorflow is interesting\nDaj#7482: Why not work together with us?\nshawwn#3694: I spent some tim"
    },
    {
      "id": "170",
      "message": "shawwn#3694: hm, why's it closed then? just curious\nbmk#1476: I believe the original intent was to not put out unfinished code\nDaj#7482: Yup\nshawwn#3694: ah\nDaj#7482: I literally never deny a request to see it\nDaj#7482: We'll release it the second we're comfortable saying it works\nDaj#7482: Which might be soon\nspiantino#6702: Joined the server.\nDaj#7482: Hello @spiantino ! Welcome to the Cult of TPUs Go Brrrr! Please see the channel topic for info and don't hesitate to ask questions!\nbmk#1476: I"
    },
    {
      "id": "171",
      "message": "Daj#7482: Since TPUs do the GPU work for us\nDaj#7482: Also a formal hello to @thesofakillers @jackclark @Teven ! Welcome to the AGI Proving Grounds! Please see the channel topic for info and don't hesitate to ask questions!\nTeven#6831: Wait you're not a bot you actually come up with the memey introductions? pretty impressive tbh\nDaj#7482: Haha thanks\nDaj#7482: I've tried to get GPT3 to make them but it's not the same\nTeven#6831: Ha\nTeven#6831: yeah anyway I actually work at Hugging Face as a res"
    },
    {
      "id": "172",
      "message": "Daj#7482: Also @jackclark  and anyone else here for research/ethics/policy reasons, I (and almost surely the others) would be happy to answer any questions, discuss issues or hop on a call or whatever any time. We wanna be open after all :)\nbmk#1476: I'd sure love to discuss anytime\nsunrisetofu#6997: Joined the server.\nmesfas#6224: Joined the server.\nSid#2121: Hey @jackclark , Super exciting to have you here. I personally am still fully formulating my ethical stance on this whole thing but lean "
    },
    {
      "id": "173",
      "message": "Daj#7482: Hello @zap @Deleted User ! Welcome to the Shitposting AIs Containment Zone! Please see the channel topic for info and don't hesitate to ask questions!\nshawwn#3694: (Is it really a shitposting AI containment zone when there isn't a #memes channel?)\nshawwn#3694: (or perhaps it's the anti-shitposting channel, which is fair)\nbmk#1476: #off-topic is the shitpost area\nDaj#7482: _We_ are the shitposting area\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/7333434269799546"
    },
    {
      "id": "174",
      "message": "Daj#7482: Hey @turmeric13 ! Welcome to the Wild World of Word Models! Please see the channel topic for info and don't hesitate to ask questions!\ngoolulusaurs#1571: One thing I've been thinking about is that with models like iGPT, people are finding ways to turn other types of data into a sequence for prediction. However, everything on a computer is already stored as a sequence of bits. I wonder how well it would work to train a large transformer on sequences of bits/hex, as a way of representing"
    },
    {
      "id": "175",
      "message": "gsastry#9119: Joined the server.\nbaragonaru#7305: Joined the server.\nDaj#7482: Hey @gsastry @baragonaru ! Welcome to The International LM Watch Organization! Please see the channel topic for info and don't hesitate to ask questions!\njeffrafter#8838: Joined the server.\nfnord#5810: Joined the server.\nSid#2121: Hello @jeffrafter @fnord  ! Welcome to AI Hygiene Council! Please check out the channel description for moar info\npujaarajan#2893: Joined the server.\nDaj#7482: Hey @pujaarajan ! Welcome to t"
    },
    {
      "id": "176",
      "message": "yurak#0640: Joined the server.\nDaj#7482: Hi @yurak ! Welcome to Bargain Bin OpenAI! Please see the channel topic for info and don't hesitate to ask questions!\ntushar#8521: Joined the server.\nDaj#7482: Hi @tushar ! Welcome to the Mountain of ~~Madness~~ Tensorflow Documentation! Please see the channel topic for info and don't hesitate to ask questions!\nshawwn#3694: Hmm, you need CPU encoding power? We have some spare\nSid#2121: that would be *super* helpful @shawwn\nSid#2121: how much do you have\nb"
    },
    {
      "id": "177",
      "message": "Ruby#6688: I can contribute around 200$ worth of cpu if that helps.\nRuby#6688: on AWS.\nSid#2121: oh awesome\nSid#2121: it may well do. @bmk would be the person to ask about that. I'm not 100% how we're going to distribute processing yet since we've had several people reach out\nSid#2121: can i put your name on the doc?\nRuby#6688: Sure\nSid#2121: â¤ï¸ we're very grateful. thanks for the offer.\nwobbithobbit#2197: > Hi @wobbithobbit @MarcAK ! Welcome to the OpenAI of OpenAI! Please see the channel topic"
    },
    {
      "id": "178",
      "message": "Daj#7482: You didn't do a silly custom message though :D\nshawwn#3694: True\nSid#2121: aw i had a good one ðŸ˜¦\nRonaldo#4812: Joined the server.\nDaj#7482: @Sid your chance!!!\nSid#2121: Hey @Ronaldo ! Welcome to the AGI Wizards' Meeting Hall! Let us know if you have any questions, or can spare any galleons\nshawwn#3694: @Ronaldo Welcome to the server, where we do what we must, even though we canâ€™t\nDaj#7482: Both pretty good hah!\nCommutative Conjecture#6969: Joined the server.\nSid#2121: mine was semi-cr"
    },
    {
      "id": "179",
      "message": "Daj#7482: Our current bottlenecks are mostly CPUs for data processing and people willing to brave Tensorflow Mesh/TPU coding\nDaj#7482: Applies to everyone ofc heh\nCommutative Conjecture#6969: Where can I find more details?\nAre there people with new projects?\nCPU as in money?\nWhat's required wrt coding?\nCommutative Conjecture#6969: I'd like to check out https://github.com/ConnorJL/GPTNeo\nDaj#7482: > Where can I find more details?\n> Are there people with new projects?\n> CPU as in money?\n> What's r"
    },
    {
      "id": "180",
      "message": "@zphang I see several zphangs?\nCommutative Conjecture#6969: Also, sorry, I didn't notice that the channel topic was collapsed and I missed most of it\nDaj#7482: All good we're delighted to help any willing contributors onboard :)\ngeorgejrjrjr#0817: Joined the server.\nshawwn#3694: Just make the repo open\nDaj#7482: Yea at this point it might make sense\nshawwn#3694: @georgejrjrjr welcome to the place to be\nSid#2121: eh idk\nSid#2121: not yet\nDaj#7482: We'll have a PoC soon\nSid#2121: it's really not d"
    },
    {
      "id": "181",
      "message": "Daj#7482: Money can make things complicated too\nDaj#7482: We're not sayig no, but it is extra overhead\nSid#2121: @shawwn please don't take that as me turning down funding lol\nSid#2121: wasn't what i meant\nSid#2121: I just don't think anyone is that sure of how much we'll need right now\nCommutative Conjecture#6969: @Sid\nThx for the answer\nSid#2121: but, yes, the more the better\nbmk#1476: how much funding do we need?\n\n***the more the better***\nbmk#1476: we have an everything shortage rn\ndvs#3865: "
    },
    {
      "id": "182",
      "message": "Sid#2121: you get an *even* more customised non formulaic welcome message from me @dvs because ya make great vids\ndvs#3865: aw thanks ðŸ˜Š\nDaj#7482: link said vids pls\ndvs#3865: lets see if you still feel that way when I add ads to the videos\ndvs#3865: https://www.youtube.com/channel/UCaZuPdmZ380SFUMKHVsv_AA\nCommutative Conjecture#6969: Any recommended links for all relevant arch&tricks stuff?\nDaj#7482: Depends on what your current level is Champiz\nDaj#7482: As in level of understanding\nSid#2121: @"
    },
    {
      "id": "183",
      "message": "Daj#7482: Hey @gstqtfr ! Welcome to the World's Most Disorganized AI Lab! Please see the channel topic for info and don't hesitate to ask questions!\nbmk#1476: aaaaa so much happening\nbmk#1476: I'm writing up the CC data stuff once and for all\nSid#2121: @dvs lmao at the big bernie at the top of your channel\nshawwn#3694: bikeshedding\nIsaac McHorse#2007: ALL PLAY AND NO WORK MEANS I'M GOING TO BE AWFUL IN LIFE.\nbmk#1476: shikebedding\nDaj#7482: tfw so much to do no time left to give Isaac more silly"
    },
    {
      "id": "184",
      "message": "bmk#1476: no u gotta help me with CCTC\nSid#2121: I can go wherever\nSid#2121: what do you need\nbmk#1476: (after mtf)\nSid#2121: @bmk i thought we were doing odd-even?\nbmk#1476: ?\nbmk#1476: I'm working on CCTC\nSid#2121: ah ok\nbmk#1476: i was saying after mtf is up\nSid#2121: wait so\nSid#2121: you're working on mtf or cctc\nbmk#1476: cctc\nbmk#1476: not mtf\nSid#2121: i can do odd even i just need you to point me to the right dims to change, you did all the coding for the layers and i'm not 100% which o"
    },
    {
      "id": "185",
      "message": "Sid#2121: sorry, we were slacking from the important work of greeting people by working on our silly model ðŸ˜†\nshawwn#3694: alright, I'm back on laptop. Let me get your SSH set up...\nshawwn#3694: _pulls up pubkeys_\nDaj#7482: Hey @step @randomuser ! Welcome to the Back Alley LM Market! Please see the channel topic for info and don't hesitate to ask questions!\nbmk#1476: anyone wanna help with CCTC\nbmk#1476: i could use some help rn\nSid#2121: I mean, sure, i just offered\nbmk#1476: but mtf\nbmk#1476: i"
    },
    {
      "id": "186",
      "message": "krysis#2720: Joined the server.\nbmk#1476: CCTC: see where it says English\nbmk#1476: https://github.com/miso-belica/jusText/tree/dev/justext/stoplists\nbmk#1476: these are the languages it supports\nSid#2121: nice\nbmk#1476: https://github.com/miso-belica/jusText/tree/dev/justext/stoplists\nbmk#1476: this is a language detector\nSid#2121: are you asking which we want?\nbmk#1476: plug b into a\nSid#2121: ah k\nSid#2121: well, i'm gonna do mesh\nbmk#1476: ok\nDaj#7482: Hi @krysis ! Welcome to Cyberpunk OpenA"
    },
    {
      "id": "187",
      "message": "Daj#7482: Hey @aegis ! Welcome to 2 Devs, One Tensorflow! Please see the channel topic for info and don't hesitate to ask questions!\nSid#2121: lmao\nDaj#7482: @aegis Through the TFRC we've got access to a ton of TPUs\nDaj#7482: It's still a _huge_ beast of a model to train but it's not _completely_ infeasible\naegis#2320: oh cool\naegis#2320: is this tpu pod(s)?\nDaj#7482: Yep\nDaj#7482: We currently run on v3-512s\naegis#2320: do you have the weight distribution technique working or are you still trai"
    },
    {
      "id": "188",
      "message": "GptForMe#9886: Joined the server.\nGptForMe#9886: @Daj Cores?  As in CPU OR gpu?\nDaj#7482: CPU atm\nDaj#7482: We don't use GPUs\nDaj#7482: We need CPU to process the dataset, we train on TPUs\naegis#2320: do you have an idea of what hardware you'll need for inference yet?\nGptForMe#9886: @Daj Your own or in the cloud?  Are the cloud platforms offering TPU's now?\naegis#2320: based on openai's estimated price I think they have a way to stream computation without having enough gpu/tpu memory for the wei"
    },
    {
      "id": "189",
      "message": "aegis#2320: do you have a lot of internet bandwidth gptforme?\nDaj#7482: Hey @eigenjoy ! Welcome to the Freerange Tensor Farm! Please see the channel topic for info and don't hesitate to ask questions!\nDaj#7482: > @Daj Nice!  Well done.  Ok, still can't find a use for my 64-core (CPU) box.  No TPU's. ðŸ™‚\n@GptForMe We can definitely put those cores to use for crunching the training data! We're trying to filter 30PB of Common Crawl data down to ~10TB\nSkylion#0368: FastText is good and more than suffi"
    },
    {
      "id": "190",
      "message": "bmk#1476: what we're doing is collecting way more than we need for future use basically\nshawwn#3694: I would recommend doing the math on how much of this data you're going to be able to train on\nshawwn#3694: yes\nshawwn#3694: by \"way more\" you mean \"far, far more than the model could feasibly be trained on\"\nbmk#1476: we already did\naegis#2320: how are you estimating 40k core-days?\nshawwn#3694: ah. I retract that then\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/73340546310"
    },
    {
      "id": "191",
      "message": "aegis#2320: I use it for my openwebtext work\nDaj#7482: We're just really at/beyond the limit of our available dev time lol\naegis#2320: it's way faster than cpython at the basic stuff I've been doing\nDaj#7482: We're pouring every free minute we have into this and need more people!\naegis#2320: you can literally just run pypy in place of cpython if you manage to install the same packages (python3 script -> pypy3 script)\nbmk#1476: look right now we're stretched unimaginably thin\nDaj#7482: I haven't "
    },
    {
      "id": "192",
      "message": "tapanc#8821: Joined the server.\nDaj#7482: Hey @tapanc ! Welcome to A Mutually Abusive Relationship Between A Few Devs And Their TPUs! Please see the channel topic for info and don't hesitate to ask questions!\nshawwn#3694: cool, so the server has more people in two days than I had in two months\nDaj#7482: Today was a pretty wild day, guess GPT and HF have a lot of name power\nDaj#7482: Lets see how many people stick, so far few have stepped up to actually help haha\naegis#2320: my full time thing is"
    },
    {
      "id": "193",
      "message": "Daj#7482: We don't really have any funding or figured out how we wanna handle money, but definitely interesting @aegis\nDaj#7482: We'll see how everything develops\nshawwn#3694: the dataset might become more valuable than the project, depending on how the training goes\nDaj#7482: Yea I think that's a likely outcome\nshawwn#3694: it'd be worth securing a spot for it. It's hard to store TB's of data for extended periods\nshawwn#3694: I don't have any ideas yet, but it's in the back of my mind.\naegis#23"
    },
    {
      "id": "194",
      "message": "shawwn#3694: fwiw, torrents almost always die, except for super popular datasets like imagenet\nZach Dwiel#0475: you might also check out dat\nshawwn#3694: did dat ever go anywhere?\nshawwn#3694: I briefly heard about it like, two years ago\nshawwn#3694: is it really suitable for storing TB's of data?\nZach Dwiel#0475: They have made quite a bit of progress, but the command line tool has lagged a bit\nZach Dwiel#0475: I'm pretty sure it was designed with at least TB's of data in mind, but i am not 100"
    },
    {
      "id": "195",
      "message": "shawwn#3694: yes. definite F\nDaj#7482: To be fair, I and l is the worst\nshawwn#3694: bikeshedding\nIsaac McHorse#2007: WHAT ARE YOU DOING BIKESHEDDING? BACK TO WORK!\nshawwn#3694: man I love that bot.\nDaj#7482: Haha\nDaj#7482: I'm gonna go to bed soon anyways\nshawwn#3694: what other features would you add to McHorseFace? Is the code up somewhere?\nDaj#7482: I wanted it to say something sarcastic whenever we say something _should_ work\nDaj#7482: Automatic unique welcome messages\nDaj#7482: Automatical"
    },
    {
      "id": "196",
      "message": "Daj#7482: I may have already done that many lol\nDaj#7482: Haha yeah I know, but it's a funny little tradition\nDeleted User#0000: Joined the server.\nshawwn#3694: @Deleted User Hi, welcome to Ethi\nDaj#7482: Hey @Deleted User ! Welcome to the Large Language Model Appreciation Society! Please see the channel topic for info and don't hesitate to ask questions!\nDeleted User#0000: Hey, thanks! Took 5mins before I could post so got to read around. Enjoy the discussion of whether or not to automate welco"
    },
    {
      "id": "197",
      "message": "Daj#7482: It definitely was a nuisance today, I'll turn it down for now, thanks for alerting us @Deleted User !\nDaj#7482: Yea it seemed to not cause too much trouble, we'll see what a lower setting does _shrug_\nshawwn#3694: one thing that keeps me from lurking on this server more is that there's no real place to show off one's own work\nshawwn#3694: but I lurk often enough.\nDaj#7482: Well, _technically_ #the-faraday-cage-archive is show off, but yea this is a very project focused discord\nshawwn#3"
    },
    {
      "id": "198",
      "message": "Daj#7482: Haha I turned the wait time off for now shawwn\nDaj#7482: I'mma be heading to bed (read: Continue checking Discord until I fall asleep like a degenerate). Crazy day today, thanks for everyone being so awesome and can't wait to see where this project goes next ðŸ‘\nSid#2121: man, you have a better sleep schedule than i do\nSid#2121: night!\nSid#2121: and, echoing that statement\nsuperguy#8832: Joined the server.\nkevinw#4330: Joined the server.\nSid#2121: Hey @superguy , @kevinw ! Welcome to LLM"
    },
    {
      "id": "199",
      "message": "shawwn#3694: nah, not looking for anything really.\nbmk#1476: Oh\nbmk#1476: Some help with memory consumption would be nice\nshawwn#3694: have you come up with a name for your server yet?\nshawwn#3694: @Brouz out of curiosity, how did you set this in your profile? https://cdn.discordapp.com/attachments/729741769738158194/733427709450911796/unknown.png\nshawwn#3694: I wasn't aware that was a discord feature\nBrouz#6768: lmao\nBrouz#6768: that's custom status\nBrouz#6768:  https://cdn.discordapp.com/attac"
    },
    {
      "id": "200",
      "message": "aegis#2320: ðŸ˜›\nSid#2121: we have WebText almost done ðŸ™‚\nSid#2121: but thanks\naegis#2320: ok\nshawwn#3694: if you don't name your server, how can you expect it to work well?\nSid#2121: uhhh\nSid#2121: does it need a name to work?\nshawwn#3694: poor server. nameless labor, destined to do one's bidding, with no thanks\nSid#2121: https://tenor.com/view/rick-and-morty-my-purpose-robot-butter-omg-gif-5234754\naegis#2320: you train gpt3\naegis#2320: us-west-1-73-185-23-49 is a name\nSid#2121: \"what is my purpose"
    },
    {
      "id": "201",
      "message": "aegis#2320: yeah\naegis#2320: ah it looks like you just have a copy of the gpt python encoder in there\naegis#2320: I can probably replace that with a python native extension\naegis#2320: so you could leave the rest of the script as python but have a faster bpe function\nSid#2121: Hey @joanna , @ch , @nikosdim ! Welcome to Villa Straylight! Check the channel description for a google doc describing the project and let us know if you can offer any help, or have any questions\naegis#2320: my encoder is "
    },
    {
      "id": "202",
      "message": "aegis#2320: the main annoying part about adding a native module would be giving you a clean build/install step\nSid#2121: I have never touched a piece of C++ in my life, lol. So I wouldn't know how much that would complicate the process\naegis#2320: looks like huggingface's tokenizers might be easier, if they're gpt2 compatible\nSid#2121: I'm sure they are. We're in contact with huggingface now so we could reach out\naegis#2320: https://pypi.org/project/tokenizers/ looks like enough\naegis#2320: ```f"
    },
    {
      "id": "203",
      "message": "Sid#2121: these are both great btw\naegis#2320: right after they finished tokenizing openwebtext? ðŸ˜„\nshawwn#3694: https://github.com/shawwn/gpt-2/blob/tpu/tokenize_dataset.py\nSid#2121: but *please* someone submit a PR lol. if it's easy to implement pls do it\nshawwn#3694: no need to PR when it's a self-contained script\nshawwn#3694: should just be a matter of cloning + running.\naegis#2320: it needs to make tfrecords.\naegis#2320: so it needs to be integrated\nshawwn#3694: true.\nSid#2121: post the link"
    },
    {
      "id": "204",
      "message": "shawwn#3694: it's not multiprocessing, no. it's generally painful to set up (at least for me)\nshawwn#3694: feel free to ninja the code though.\nshawwn#3694: https://github.com/shawwn/danbooru-tools/blob/master/danbooru_to_tfrecord.py was my experience with multiprocessing in python, and it was quite enough\nSid#2121: oh i love the multiprocessing library.\naegis#2320: ah, I can help with python multiprocessing\naegis#2320: for something like this make sure to use a big chunksize\naegis#2320: and imap"
    },
    {
      "id": "205",
      "message": "malaclyps#4923: Joined the server.\nsamuel s#6205: Joined the server.\nlacker#5924: Joined the server.\nForgottenOrb#6802: Joined the server.\nForgottenOrb#6802: how much funding is this project going to need? i saw in the doc that \"some\" was needed, but didn't see a particular estimate\nbaragonaru#7305: why not call it OpenOpenAI\nSid#2121: Hey @tawnkramer , @murat , @malaclyps , @samuel s , @lacker ! Welcome to OpenOpenOpenAI! Even *more* open than the one @baragonaru just suggested. Please check th"
    },
    {
      "id": "206",
      "message": "shawwn#3694: Every time a TPU fetches a tfrecord, itâ€™s a class A op\nSid#2121: sorry @baragonaru missed this one. we have TFRC creds\nshawwn#3694: And it does that constantly during training, for shuffling\nbaragonaru#7305: so I don't know much about TPUs -- what would the  approximate NVIDIA equivalent of what you have in TFRC be, if anything?\nshawwn#3694: 512 GPUs\nshawwn#3694: Itâ€™s mostly equivalent core-for-GPU.\nshawwn#3694: At least for our stylegan2 tests.\nbaragonaru#7305: Cool. and they have "
    },
    {
      "id": "207",
      "message": "bmk#1476: We were planning on ~~begging~~ asking google nicely if we could somehow use a 2048 more than about an hour a day, lol\nbmk#1476: See the document for compute estimates for 1T\nbmk#1476: #documentation\njekbradbury#2280: ok, you have reasonable estimates there\nshawwn#3694: The spatial partitioning did â€œjust workâ€ â€” I was so amazed that I wrote them a thank you email\nshawwn#3694: It was just a matter of flipping a few config options.\nbmk#1476: Tf usually never just works\nshawwn#3694: Yes\ns"
    },
    {
      "id": "208",
      "message": "shawwn#3694: Iâ€™ve been meaning to reverse engineer how the profiler works, in hopes of maybe being able to trick the TPU into dumping the compiled output (or an equivalent)\njekbradbury#2280: i think we even expose the optimized HLO in JAX TPU colabs\nbaragonaru#7305: ok, pardon my ignorance, but if  hypothetically one did have say 500 V100 gpu's and they had very poor interconnectivity, like lets  say internet distributed, would you be able to train something like GPT3 at all?\nbaragonaru#7305: as"
    },
    {
      "id": "209",
      "message": "shawwn#3694:  https://cdn.discordapp.com/attachments/729741769738158194/733482294311911464/image0.png,https://cdn.discordapp.com/attachments/729741769738158194/733482294685204551/image1.png\nshawwn#3694: Referenced notebook: https://colab.research.google.com/github/google/jax/blob/master/cloud_tpu_colabs/Pmap_Cookbook.ipynb\njekbradbury#2280: sorry, yeah, that version is out of date\njekbradbury#2280: try tpu driver nightly (with some spelling, i can look it up)\nshawwn#3694: Excellent, thanks for t"
    },
    {
      "id": "210",
      "message": "shiva.adam#8165: Joined the server.\nbmk#1476: welcome @shiva.adam @sammyjack to LibreAI! sorry, I'm not very good at the whole creative welcome messages thing haha\nshiva.adam#8165: Hi, thank you!!. trying to catch-up with what's been happening here\nbmk#1476: tl;dr our flagship project is replicating GPT-3. After that we're shooting for 1T parameters. Along the way we're making some awesome datasets and libraries. And all this will be completely free and open source!\nbmk#1476: (and we can use all"
    },
    {
      "id": "211",
      "message": "shawwn#3694: Oho, thought so\nshawwn#3694: Welcome to TPU Podcast #2\nshawwn#3694: I hereby claim the server until everyone wakes up. Till then, check the channel topic!\nshawwn#3694: Also they have some interesting goals\nshawwn#3694: I think they have a strong chance at replicating gpt3. Probably the best of anyone outside OpenAI\nshawwn#3694: Still going to be tough\nExMachina#1693: Joined the server.\nbmk#1476: from what i gather, aren't we the *only* ones seriously trying? (outside the big labs of"
    },
    {
      "id": "212",
      "message": "Nax#8383: Actually I am reading this paper: https://arxiv.org/pdf/2006.16236.pdf\nSid#2121: that is a very tempting offer @jili , but our current pipeline is all tpu bound atm. The bottleneck is actually cpu, if you have any access\njili#4497: Iâ€™ve CPus too\njili#4497: Just tell me what u need\nSid#2121: @jili we'll be distributing our data processing pipeline across the several people who have offered us compute. Can i add you to the list? could you send details of what you have access to?\njili#449"
    },
    {
      "id": "213",
      "message": "@Sid  better with twitter @jilijeanlouis\nSid#2121: merci @jili !\nsatosh3944#3941: Joined the server.\nSid#2121: Hey @satosh3944 ! Welcome to the LibreAI, the server where we truly have run out of custom welcome messages. Please check the google doc in the channel description for more info ðŸ™‚\nshawwn#3694: @jili is the one who helped us with servers -- he's a server god, as far as I'm concerned\nAimely#0568: Joined the server.\natoms#7386: Joined the server.\npeterli100#7294: Joined the server.\nSoth02#"
    },
    {
      "id": "214",
      "message": "Daj#7482: Yep, we also want to be a kind of replication study of GPT models! I'm actually not sure myself what other teams have attempted 1.5B replication. I'm aware of the Brown team and they got similar results (though with a different model architecture iirc, not sure if they retried with vanilla GPT after)\nDaj#7482: There's a lot of unknowns as it comes to best practices in training huge LMs like these, luckily we have the compute to burn to hopefully help with that\nkevinw#4330: huge respect"
    },
    {
      "id": "215",
      "message": "shawwn#3694: what's the point? the people here basically shout at each other until a consensus is reached. it's a recurring theme\nDaj#7482: Uhm\nDaj#7482: I have not made that experience whatsoever, I'm sorry you feel that way\nshawwn#3694: listen. people are fine and happy with how this is done\nshawwn#3694: so be it.\nshawwn#3694: from my perspective, outside ideas are hardly even considered\nDaj#7482: I'm sorry you feel that way because that is not what we're trying to convey\nDaj#7482: We've tried"
    },
    {
      "id": "216",
      "message": "shawwn#3694: mm, it's not quite luck. I think you have a real shot at replicating GPT-3. mesh tensorflow will be an interesting challenge...\nDaj#7482: Yea we've got a good team, we're working hard and we'll get it done!\nDeleted User#0000: @Nax I have that paper implemented here https://github.com/lucidrains/linear-attention-transformer/\nDeleted User#0000: Feel free to try it! I don't think it is as good as full attention at lengths greater than 2048\nSid#2121: thanks @Deleted User ! Love your wor"
    },
    {
      "id": "217",
      "message": "Sid#2121: > It also has the downside of working only with a fixed sequence length\n@Deleted User I'm pretty sure we can only work with fixed sequence lengths anyway, since tpus\nDeleted User#0000: @Daj I'll get back to you all once I finish my last alternative transformers project https://github.com/lucidrains/memory-transformer-xl/\nDeleted User#0000: Would love to help out\nDaj#7482: Awesome! You really are productive damn\nSid#2121: mannn i wish you were on the dark tf side @Deleted User . Your tr"
    },
    {
      "id": "218",
      "message": "Daj#7482: Ok just wasn't sure :)\nshawwn#3694: Yes I was talking to LibreAI.\nDaj#7482: So the question was about whether LibreAI should set up a patreon? If so the simple answer is we've just not gotten that far with planning haha\nshawwn#3694: Itâ€™d take about 30 minutes\nDaj#7482: Yea but there's some meta stuff like who handles the money, the name LibreAI is technically taken, etc\nDaj#7482: We'll get around to it if we decide that's the right route\nshawwn#3694: Make it your own patreon then\nDaj#7"
    },
    {
      "id": "219",
      "message": "Sid#2121: @unography not gpt and probably not quite what you're looking for but I used stylegan / biggan to generate fonts (then converted to svgs)\nSid#2121: I love this idea tho\nSid#2121: (in fact - this is what our logo text is made from)\nzaoyang#8792: Joined the server.\nDaj#7482: Hey @zaoyang ! Welcome to the Least Financially Savvy AI Lab! Check the channel topic for infos and don't hesitate to ask questions!\nchris-tn#9278: Joined the server.\nDaj#7482: Hey @chris-tn ! Welcome to the Final Bo"
    },
    {
      "id": "220",
      "message": "shawwn#3694: Hi @vessenes. Welcome to OpenClosedOpenAI. The roadmap is in the channel description\ntomislav_REGEX#7247: Joined the server.\nshawwn#3694: Hello @tomislav_REGEX. Weâ€™re a big fan of regexes here, so please feel free to compile yourself\nvessenes#0001: Hey @shawwn . Does the doc have right that current estimates are for 192 TPU-days for a GPT3+ scale training?\nshawwn#3694: For what itâ€™s worth, the Jax team lead looked over the doc and concluded the estimates were reasonable\nvessenes#000"
    },
    {
      "id": "221",
      "message": "(Put another way, could they easily pivot if they felt the need to?)\nshawwn#3694: I think before long many companies will come to the horrific realization that 1. In order to compete, they need a cloud platform; 2. They also need serious hardware talent, and the ability to execute on it; 3. The number of companies with both of these is essentially one.\nshawwn#3694: Worse, large scale ML training is the future, so theyâ€™ll be at a serious disadvantage\nbaragonaru#7305: NVidia is also more broadly f"
    },
    {
      "id": "222",
      "message": "shawwn#3694: Oh? Thatâ€™s interesting to know\nshawwn#3694: Whatâ€™s a DGX like?\nzphang#7252: eh it's probably just a configuration thing\nzphang#7252: but there's 1 (maybe more?) DGX in my university cluster I think and I have it on my exclude list\nshawwn#3694: Interesting. What sort of exclude list? Iâ€™m curious about your workflow\nzphang#7252: it's just slurm\nbmk#1476: @vessenes 6-10k? That sounds like it's off by two or three orders of magnitude\nbmk#1476: (ofc it's free to us since we use TFRC, but"
    },
    {
      "id": "223",
      "message": "Sid#2121: ðŸ‘€ if you have them, the more the better, of course\nSid#2121: If you can somehow magically get us tpus larger than 512 cores that won't get pre-empted within an hour, that would be even better\nshawwn#3694: @psgeorge the most directly useful thing might actually be GCE credits\nSid#2121: at the moment, yeah. but when we have proper data and start testing at scale the more tpus the better\nshawwn#3694: A v3-2048 is â€œonlyâ€ 4x faster than a v3-512. But the GCE credits will go away fast, espec"
    },
    {
      "id": "224",
      "message": "Noa Nabeshima#0290: Hi @MarkX!\nJoscha#2969: Joined the server.\nplinz#6161: Joined the server.\nSid#2121: Hello @Joscha , @plinz ! Welcome to the GPT-discord-simulator, where everyone may or may not be a chatbot, who knows... Pls check the channel description for info on the project and reach out if you have any questions\nDaj#7482: Hey @Joscha ! Nice to see you here\nhuh#0141: Joined the server.\nprempv#0575: Joined the server.\nSid#2121: Hey @huh @prempv ! Welcome to the land of the grey goo! pls ch"
    },
    {
      "id": "225",
      "message": "Noa Nabeshima#0290: But I'm imagining if our 1T transformer is going to require half a year to train (I don't know how long it'll actually take), it might be worth coming up with alternative faster training methods.\nkindiana#1016: sounds like noisy student with extra steps if they are both decoders\nzphang#7252: There are similarities but I think the underlying motivation is different. Noisy student seeks to make use of unlabeled examples to supplement supervised, labeled training. ELECTRA tries "
    },
    {
      "id": "226",
      "message": "Abundant\nNoa Nabeshima#0290: Gestalt is great but already taken\nNoa Nabeshima#0290: Friendly is really good, but creating namespace conflict with AI Safety seems probably bad\nNoa Nabeshima#0290: Unrestricted seems accurate at least\njk_aync#5159: Joined the server.\nkevinw#4330: AING, which stands for \"AING is not GPT\"\nNoa Nabeshima#0290: \"LINAA Is Not An Acronym\"\nNoa Nabeshima#0290: aiaiai.ai is available\nNoa Nabeshima#0290: friend's idea\nSid#2121: @Noa Nabeshima yeah we really need a new name. I"
    },
    {
      "id": "227",
      "message": "Sid#2121: but then, that's only one aspect of the project\nSid#2121: so idk\nSid#2121: can we just... take away the e\nSid#2121: Librai\nNoa Nabeshima#0290: librarai\nNoa Nabeshima#0290: fwiw I don't like Librai\nNoa Nabeshima#0290: but it won't be taken anywhere!\nSid#2121: hahahah, yeah i'm not a big fan either. it's a bit of an ugly word\nSid#2121: it feels like... there's an e and a space missing\nSid#2121: Open*er*AI\nDaj#7482: For what it's worth I'm fond of \"NettAI\", which sounds like some kind of "
    },
    {
      "id": "228",
      "message": "@goolulusaurs lol\nSid#2121: poll poll poll\nSid#2121: what are our options\nDaj#7482: KaiAI, FreeAI, NettAI, Bikeshedding, AING/LINAA\nIsaac McHorse#2007: WELL YOU'RE NOT WORKING!\nDaj#7482: I think?\nSid#2121: what's the idea behind the last ones?\nSid#2121: ah, brb\nDaj#7482: Dunno they're recursive and that's funny to me lol\nSid#2121: tbh i don't really like any of them as much as LibreAI\nSid#2121: if i had to choose I'd go with freeai\nSid#2121: neat, simple\nSid#2121: but let's start a poll\ngoolulus"
    },
    {
      "id": "229",
      "message": "Daj#7482: So anyone with an interest in our future name feel free to vote in #deleted-channel\nDeleted User#0000: Joined the server.\nDeleted User#0000: elo\ngoolulusaurs#1571: Hello!\nDaj#7482: Hello @Deleted User ! Welcome to the Temple of the Not-Quite-There-Yet AGI! Please check the channel description for info and don't hesitate to ask questions!\nsifbuilder#4366: Joined the server.\nDaj#7482: Hey @sifbuilder ! Welcome to the AGI Book Club! Please check the channel description for info and don't "
    },
    {
      "id": "230",
      "message": "bmk#1476: å¼€KÃ¼nstlicheIntelligenz\nDaj#7482: Haha\nbmk#1476: I really like the idea of using Kai in the name because it 1. Sounds cool 2. It's easy to pronounce\nDaj#7482: Feel free to vote for it heh\nbmk#1476: KaiLM\nbmk#1476: For the model\nbmk#1476: KaiML for the org\nbmk#1476: Also å¼€ looks nice and is easily logoifyable\nDaj#7482: I guess I feel weird about putting a chinese name on a very western org heh\nDaj#7482: But as said, we have free voting in #deleted-channel\nCommutative Conjecture#6969: Not"
    },
    {
      "id": "231",
      "message": "- Is it a reasonable first time project?\n- Anyone interested in me documenting this?\nSid#2121: ok one that i can answer - TPU access is granted to us through TFRC (tensorflow research cloud) and they're pretty open in granting access to projects - you should try to apply\nSid#2121: plenty of benchmarks - worth reading the gpt-2 and gpt-3 papers, as they test on many different ones\nSid#2121: it's tough to say which is 'the best' benchmark as they all measure very different, specific aspects of lan"
    },
    {
      "id": "232",
      "message": "Else, I want to understand why)\nSid#2121: @Daj isn't perplexity more of a measure of 'how random is my output' than a benchmark. is it comparable across models?\nCommutative Conjecture#6969: Across models on the same data sounds good to me\nDaj#7482: According to several papers I've read, perplexity seems to match \"human graded text quality\" shockingly well\nCommutative Conjecture#6969: Oh nice\nDaj#7482: Which I also found surprising\nCommutative Conjecture#6969: Links?\nSid#2121: ye but it's just dr"
    },
    {
      "id": "233",
      "message": "Commutative Conjecture#6969: > Locally is _much_ easier technically\n@Daj\nStupid q\nRight now, there is a lib to train gpt on tpu, but not to generate text right?\nIf so, is training it on tpu, and then sampling locally, still a big hassle?\nDaj#7482: Not really, but Hugging Face's GPT2 implementation is much cleaner and less weird than ours\nDaj#7482: I don't know how well our code would play with GPUs\nDaj#7482: My old code works fine on GPU but it's not extensively tested either\nDaj#7482: https://g"
    },
    {
      "id": "234",
      "message": "Commutative Conjecture#6969: Ic\nbmk#1476: you know what would be cool? a gpt-x impl family tree\nCommutative Conjecture#6969: Is it easy to extract the model and run it somewhere else?\nDaj#7482: You should probably just use my old code\nCommutative Conjecture#6969: ðŸ‘\nDaj#7482: Since the new code is _mostly_ the same just translated to TFM\nDaj#7482: anyways dinner brb\nCommutative Conjecture#6969: Thanks\nCommutative Conjecture#6969: Brbbb\nNoa Nabeshima#0290: Do any of you folks have experience diagn"
    },
    {
      "id": "235",
      "message": "zphang#7252: they did switch over from a custom optimizer to a built-in pytorch one a while back, maybe you were looking at it before then\nDaj#7482: This was like 6 months ago\nDaj#7482: So I'm not sure if this is still an issue yea\nDeleted User#0000: does anyone here know or heard of anyone using Universal Transformers with any success?\ngoolulusaurs#1571: I messed around with it some, it seemed like you had to increase the number of params in the layer quite a bit to get similar performance, and"
    },
    {
      "id": "236",
      "message": "bmk#1476: What happened to prompt so many people coming here? o.O\nSid#2121: Hello all!!\nprestonS#2407: Joined the server.\nDaj#7482: I'm in an online SSC meetup and mentioned our server haha\nbmk#1476: oh, haha\nBracctÅ«n#8805: Joined the server.\nshawwn#3694: Interesting. Which meetup?\nMisha#6151: Is there a github where the GPT-3 mimic model is on?\nSid#2121: yep! ping @Daj for access\nbmk#1476: We can use all the help we can get haha\nak12#5840: Connor just blew my fuckin mind\nIsaac McHorse#2007: pon"
    },
    {
      "id": "237",
      "message": "bmk#1476: apparantly no\nbmk#1476: i thought europe was UTC+1\nDaj#7482: It is 0:45 for me lol\nbmk#1476: huh\nDaj#7482: > Interesting. Which meetup?\n@shawwn SSC meetup\nbmk#1476: which one? i dont see any info in the ssc discord\nDaj#7482: Uhhh it's set p by Joschua Fox\nDaj#7482: I think I found it on LW?\nDaj#7482: I don't remember actually\nDaj#7482: It's _really_ good though, great speakers and great conversations\nbmk#1476: huh\nDaj#7482: Thanks again for the great discussion everyone from the meetup"
    },
    {
      "id": "238",
      "message": "ak12#5840: http://m.nautil.us/issue/86/energy/a-neuroscientists-theory-of-everything\nak12#5840: > winograd tests, for example, can be considered a decent measure of a LM's ability to 'reason' (i hesitate using that word, because it's not necessarily reasoning, but that's what the winograd test is there to measure)\n@Sid  anyone else have opinions about the importance of Winograd schema tests as a measure of GPT3 competence ?\naquajet#7800: I remember Gary Marcus and some other people recently publ"
    },
    {
      "id": "239",
      "message": "Commutative Conjecture#6969: But like\nCommutative Conjecture#6969: V3 is 400 Tflops\nCommutative Conjecture#6969: 5 v3 is 2 petaflops\nSid#2121: @Commutative Conjecture whatâ€™re your plans for them? ðŸ˜„\nDaj#7482: You can't easily combine multiple TPUs\nDaj#7482: But yeah TPUs are awesomely powerful\nCommutative Conjecture#6969: > @Commutative Conjecture whatâ€™re your plans for them? ðŸ˜„\n@Sid\nI have two different sets of plans, not sure which one they have approved\n\n- One is finetuning a gpt2 for coq codeg"
    },
    {
      "id": "240",
      "message": "Daj#7482: I haven't actually put a ton of thought into it (mostly because I'm still an amateur with theorem provers), but I'd love to experiment\nSid#2121: ah *finetuning*\nSid#2121: hopefully we should have a gpt-2 size model within a week or two, if you fancy trying ours out\nCommutative Conjecture#6969: > @Commutative Conjecture are you using Connor's old repo, or our new one?\n@Sid\nI'll start today, I planned to use old repo with my 2080gtx\nBut now that I have tpus, I'll use new repo and bite th"
    },
    {
      "id": "241",
      "message": "@Sid\nWe can try to replicate the blog post that says it is\nSid#2121: blog post? I must have missed that\nDaj#7482: That'd need human labeling\nCommutative Conjecture#6969: @Daj linked it\nCommutative Conjecture#6969: > That'd need human labeling\n@Daj\nMturk cab produce it iirc, it was vbasic\nDaj#7482: The Meena blogpost Sid, that's where they claim perplexity correlates with human ratings\nSid#2121: also @Daj !addresource seems like it's working now, if you want to change permissions\nDaj#7482: Ah coo"
    },
    {
      "id": "242",
      "message": "Daj#7482: Should be locked down for non-O5 and non-Sentient AI\nSid#2121: probably the quotes\nDaj#7482: Can someone check that?\nSid#2121: the permissions or the regex\nDaj#7482: Permissions\nDaj#7482: You should be able to post\nDaj#7482: But others shouldn't I think\nSid#2121: @Commutative Conjecture can you try to post in #links\nCommutative Conjecture#6969: Can't\nSid#2121: ok - so if you want to add a link to any of the resources channels\nSid#2121: (anyone has permission)\nSid#2121: the command is\nD"
    },
    {
      "id": "243",
      "message": "Daj#7482: Done\nAleks#1486: Joined the server.\nDaj#7482: Hey @Aleks ! Welcome to The AI Lab Formerly Known As LibreAIâ„¢ï¸! Check the channel topic for info and don't hesitate to ask questions!\nAleks#1486: Hey!  Thanks ðŸ™‚\nkevinw#4330: Looking at the GPT3 paper again, the GPT3 13B model size seems interesting. It seems closer to 175B than to 1.3B (which I take to be at rough parity with GPT2-XL) in performance on many tasks, and yet 13B is such a more practical \"form factor\" than 175B. I think it woul"
    },
    {
      "id": "244",
      "message": "kevinw#4330: also, \"As a simplifying assumption, we ignore the attention operation, as it typically uses less than 10% of the total compute for the models we are analyzing\"\nSid#2121: > My rough estimate would be that that would train on a 512 in about 10 days (at perfect utilization)\n@Daj don't estimate with perfect utilization lol - since our utilization is only at about 20%\nSid#2121: I'd be surprised if we got up to 50\nDaj#7482: That's why I'm wondering if OA's models were trained with theoret"
    },
    {
      "id": "245",
      "message": "Deleted User#0000: lol\nDeleted User#0000: mixture of experts is done! very beta!\nDeleted User#0000: i'm testing it this week ðŸ™‚\nDeleted User#0000: https://github.com/lucidrains/mixture-of-experts\nSid#2121: I actually already saw it ðŸ™‚ it's already high ranking on google for mixture of experts github hahaha\nSid#2121: are you planning on just implementing the architectures from the paper?\nDeleted User#0000: it's in routing transformers too https://github.com/lucidrains/routing-transformer\nDeleted Us"
    },
    {
      "id": "246",
      "message": "Sid#2121: *what is indentation*\nDaj#7482: We just don't yet understand how brilliant its code is\nDaj#7482: Indentation is what's holding humans back\nDeleted User#0000: i have absolutely no doubt we will live to see an attention network learn pytorch (and tensorflow) fluently\nDaj#7482: PyTorch maybe, but _Tensorflow?_\nDaj#7482: Humans haven't even reached that level\nDeleted User#0000: ok, g2g, bbbl\nSid#2121: @Deleted User do you think MOE can be neatly plugged into the existing gpt arch in any wa"
    },
    {
      "id": "247",
      "message": "Deleted User#0000: which is what I did with routing transformers\nSid#2121: so you'd only use one or two?\nbmk#1476: they mentioned that numerical stability was preventing them from scaling to 1T params\nDeleted User#0000: yup\nbmk#1476: I'm guessing that was issues with softmax\nDeleted User#0000: its the same with PKMs\nSid#2121: hm. interesting. And then scale up the amount of layers as needed - or simply scale up the MOE?\nSid#2121: well, i think i know the answer. That's a dumb question\nDeleted Us"
    },
    {
      "id": "248",
      "message": "Sid#2121: ahh hey @ShyTeaSeb !\nSid#2121: where's his custom introduction eh @Daj ??\nDaj#7482: Oh, right!\nDaj#7482: Hey @ShyTeaSeb ! Welcome to the AGI Preppers! Check out the channel topic for info and don't hesitate to ask questions!\nDaj#7482: Happy now haha\nSid#2121: very\nDaj#7482: btw Sid I remember you offered to perhaps help us out a bit with the project a while back? If the offer still stands, Seb is the one to talk to, though I think he's got his hands full atm\nSid#2121: totally still sta"
    },
    {
      "id": "249",
      "message": "Daj#7482: That could work\nbmk#1476: :firealarm:\nbmk#1476: :firealarm:\nDaj#7482: Hmm it's hard to get it to look good as a reaction\nbmk#1476: yeah\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/734828787786711050/71IK30NIhwL.png\nbmk#1476: it would look like a tomato at react siuze lol\nDaj#7482: Yea\nCommutative Conjecture#6969: Taken from Google, it had icon (so I thought it was smaller) in the name, likely copyrighted\nBut that seems minimalistic and to the point\nCommutative "
    },
    {
      "id": "250",
      "message": "Daj#7482: Nice, good luck haha\nCommutative Conjecture#6969: @Daj\nDo you know if the free month is easily renewable?\nSid#2121: @Commutative Conjecture I would recommend tpunicorn ðŸ™‚\nDaj#7482: Yea if you have some kind of minorly interesting project it's usually not a problem\nCommutative Conjecture#6969: ðŸ†—\nDaj#7482: though I'm the wrong one to ask since I get special treatment lol\nDaj#7482: They also really appreciate bug reports btw\nCommutative Conjecture#6969: @Sid My tpus are not preemptible\nCom"
    },
    {
      "id": "251",
      "message": "french detected\nCommutative Conjecture#6969: never know if it's kb/s or kB/s for bytes\nbmk#1476: bits, Bytes\nbmk#1476: bytes are Bigger\nCommutative Conjecture#6969: \"or the other way around\", autcompletes ~~GPT-3~~ my brain\nCommutative Conjecture#6969: > bytes are Bigger\n@bmk that one is nice tbh\nKazumi#1297: I have a lot of trouble remembering when there's 2 things that are similar like that\ngoolulusaurs#1571: not to mention KB vs KiB\nDaj#7482: We should just use the imperial system for bytes\nb"
    },
    {
      "id": "252",
      "message": "aquajet#7800: I agree with your argument, but I'll try to play devil's advocate\naquajet#7800: The greater point is that with larger neural networks its harder to tell exactly why the model outputted the word that it outputted\nKazumi#1297: it's not true just for larger networks, I'm already having trouble understanding what my discord bot using gpt-2 345M model. I've seen it break when I try to show it to people, and it's so consistent and it turns back to normal when I'm not showing other people"
    },
    {
      "id": "253",
      "message": "aquajet#7800: > You say \"Level with me John, can you recognize non-regular languages?\"\n> \"No,\" says John, laughing.\n> By the end of the day, John has learned all of elementary mathematics.\nKazumi#1297: I just found it, I'm still reading\naquajet#7800: Ok so i might be wrong but what I think is happening is that theres a story close to this in the train text, where its told in first-person and some character is learning some task\naquajet#7800: so it learns from that and can substitute it in the co"
    },
    {
      "id": "254",
      "message": "zphang#7252: https://arxiv.org/abs/1908.07125\naquajet#7800: > \"For example, triggers cause SNLI entailment accuracy to drop from 89.94% to 0.55%, 72% of \"why\" questions in SQuAD to be answered \"to kill american people\", and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts\"\nwow\naquajet#7800: thanks @zphang\nx.#1490: Joined the server.\nNoa Nabeshima#0290: Hey, do we have available somewhere the true GPT3 tokenization?\nNoa Nabeshima#0290: ie a list of all t"
    },
    {
      "id": "255",
      "message": "- regularization (including dropouts)\n- attention\n\nwhat i doubt i understand:\n- how RNNs are trained\n- why there is not more focus on the initial weights, which seem vimportant to accelerate the search\n- why there is not much more encoding of priors in regularization\n\nwhat i definitely don't understand:\n- why xavier/kaiming inits are stable while training\n- why training RNNs work\n- how LSTM are trained and why training LSTM work\nCommutative Conjecture#6969: still reading, but if you have good po"
    },
    {
      "id": "256",
      "message": "Commutative Conjecture#6969: it works when you do a forward pass, ok\nCommutative Conjecture#6969: but why doesn't it fail miserably while training?\nCommutative Conjecture#6969: like, it seems very unexpected that the standard-deviation of the intermediary values stays close to 1 after training\nCommutative Conjecture#6969: > who cares about RNNs and LSTMs in 2020, attention is all you need ðŸ˜› (only half joking)\n@kindiana\ni'm interested in training procedures with actual state, wich attention does "
    },
    {
      "id": "257",
      "message": "kindiana#1016: @Commutative Conjecture mostly from looking at weight distribution histograms on tensorboard lol, if there is significant distribution shift it almost always means initialization was wrong\nCommutative Conjecture#6969: @kindiana\nerg, that's a very interesting result!!\nkindiana#1016: Most bptt implementions feed in the last hidden state back in as the first hidden state (with no gradient), so you can theoretically learn longer dependencies that way\nCommutative Conjecture#6969: what "
    },
    {
      "id": "258",
      "message": "Daj#7482: Ah ok. Would that mean that most unsupervised models are \"gradient free\"?\nDaj#7482: Wouldn't GPT be gradient free? Or am I confused?\nkindiana#1016: with GPT, there is a gradient between the network output and the loss, which you use to optimize the weights directly\nDaj#7482: I'm confused how that differs from the critic then\nDaj#7482: Maybe I need to learn more RL\nkindiana#1016: with a gradent free method, there is no requirement that you can calculate \"how good\" the output of the acto"
    },
    {
      "id": "259",
      "message": "@Kazumi Yea this is what I thought for methods other than AC (and maybe DQN?)\nDaj#7482: I guess my terminology was a bit muddled\ngoolulusaurs#1571: The whole family of techniques is called policy gradient\ngoolulusaurs#1571: https://medium.com/@aminamollaysa/policy-gradients-and-log-derivative-trick-4aad962e43e0\nDaj#7482: _sighs and adds yet another tab to the 200 already open_\nKazumi#1297: I can't even see the page titles of the tabs anymore\ngoolulusaurs#1571: Tree style tabs\nDaj#7482: Firefox h"
    },
    {
      "id": "260",
      "message": "Daj#7482: It changed how I use computers forever\nDaj#7482: I can never go back\nDaj#7482: And it makes you look like a 1337 hacker lol\nCommutative Conjecture#6969: what's i3\nCommutative Conjecture#6969: oh, it's linux only\nCommutative Conjecture#6969: ðŸ˜¦\nDaj#7482: Just one of the many reasons I can never switch from Linux haha\nCommutative Conjecture#6969: Just one of the many reasons I use nearly zero-config\nCommutative Conjecture#6969: (as in, dumb emacs on linux, dumb vscode on windows, etc.)\nDa"
    },
    {
      "id": "261",
      "message": "Kazumi#1297: just giving you a judgmental smile\nbmk#1476: So I'd hazard a guess of probably around 4-500 tabs\nbmk#1476: If you're interested in helping with the Reading List Managerâ„¢ as a way to solve the tab problem once and for all that would be great\nbmk#1476: It would certainly be a fun yak shaving project after the hell that is mtf\nchup4cabr4#3178: Joined the server.\nbmk#1476: @shawwn I'm on nuck and trying to use docker ant I think I'm not in docker group?\nbmk#1476: ```ERRO[0000] failed to"
    },
    {
      "id": "262",
      "message": "shawwn#3694: just remember that if we somehow cause the server to go offline + it doesn't come back online, it will take some time to fix. (I don't have access to the hardware resets)\nshawwn#3694: certainly not\nshawwn#3694: if I had progress, I'd be shouting it out\nSid#2121: hah, just thought i'd check in ðŸ™‚ There are some mtf resources i found that I can point you to if they'd be any help\nSid#2121: they have top_k and autoregressive sampling hidden somewhere\nSid#2121: ```SAMPLING:\nautoregressive"
    },
    {
      "id": "263",
      "message": "bmk#1476: Can haz Pytorch->tf translation\nDaj#7482: Pretty sure understanding TF is an AGI-complete problem\nDaj#7482: Maybe ASI\nCommutative Conjecture#6969: stupid q\nCommutative Conjecture#6969: with the super big batch sizes that are used for gpt3\nCommutative Conjecture#6969: how come it knows so much **specific** information?\nCommutative Conjecture#6969: like, how was this information targeted and learnt?\nCommutative Conjecture#6969: ```\nWe are a hacklab working on artificial intelligence. We "
    },
    {
      "id": "264",
      "message": "bmk#1476: woah, awesome!\nbmk#1476: are there training speed/memory improvements?\nDeleted User#0000: it's about 20% slower, and you pay the memory cost of the increase in parameter count of the experts, however many you'd like to add\nDeleted User#0000: the above is on an auto-regressive model, a la gpt\nDeleted User#0000: seems to work well!\nbmk#1476: i thought the point of MoE was to be faster and/or more memory efficient?\nDeleted User#0000: yup! the 4 experts and 16 experts is about the same spe"
    },
    {
      "id": "265",
      "message": "Deleted User#0000: just playing around with it on my dinky enwik8 benchmark\nDeleted User#0000: code is https://github.com/sIncerass/powernorm/blob/master/fairseq/modules/norms/mask_powernorm.py\nDaj#7482: Hey @jonas ! Welcome to the AGI Petridish! Check the channel description for info and don't hesitate to ask questions!\nTRAVELINGKITTY#9071: Joined the server.\nJosh#5264: Joined the server.\nEric Anderson#0751: Joined the server.\nJosh#5264: Hi folks. I'm Josh, and I work at OpenAI on topics relate"
    },
    {
      "id": "266",
      "message": "chup4cabr4#3178: which is not guaranteed to work ofc\nCommutative Conjecture#6969: @chup4cabr4 yup, it seems crazy that it works for this\nchup4cabr4#3178: > @chup4cabr4 yup, it seems crazy that it works for this\n@Commutative Conjecture agreed. to me this is almost like evolution. absolutely unbelievable what can happen if you run a margin directed change for a few million iterations\nCommutative Conjecture#6969: > @Commutative Conjecture agreed. to me this is almost like evolution. absolutely unbe"
    },
    {
      "id": "267",
      "message": "Sid#2121: for the memes\nDaj#7482: That's a lot of effort for a meme\nmefrem#1476: Joined the server.\nDaj#7482: Hey @mefrem ! Welcome to the Loss Olympiads! Check the channel description for info and don't hesitate to ask questions!\nnotTheChosenOne#9540: I read the channel description and skimmed the Google doc that it links to. I still have a question about the whole project though. I'm sure you discussed it somewhere already but I could not find a discussion about it on this channel.\n\nWhat do yo"
    },
    {
      "id": "268",
      "message": "Daj#7482: Hey @mefrem ! Welcome to the Aperture Science TPU Laboratories! Check the channel description for info and don't hesitate to ask questions!\nCommutative Conjecture#6969: anyone got a good link detailing under which assumptions SELU is self-normalizing and what is normalized exactly?\nDaj#7482: There is very little good info on SELU\nDaj#7482: From what I understand it normalizes activations towards a stdnormdist as long as the input is stdnormdist and the weights have a certain initializa"
    },
    {
      "id": "269",
      "message": "mefrem#5884: https://github.com/mefrem is the Github, it's mainly been updates to my Jekyll website recently. Can't wait to get up to speed. Likes include gymnastics, Twitter, monetary economics, and the comments section at SSC\nDaj#7482: Happy to have you here and look forward to what we can do together ðŸ‘\nDaj#7482: Let me invite you to the repo\nDaj#7482: Invite sent! Work on the model happens in #gpt-neox-devs , work on data collection and processing in #the-pile\nDaj#7482: @Sid and @bmk are the "
    },
    {
      "id": "270",
      "message": "@Daj @goolulusaurs has also been getting into some mesh stuff iirc\nSid#2121: lmao\nbmk#1476: honestly i kind of like it\nbmk#1476: and we should make one version for each order of magnitude\nSid#2121: I just love that you've totally disregarded the entire concept of acronyms\nbmk#1476: that makes it funnier\nSid#2121: by choosing a random point within the word\nbmk#1476: as well as the recursiveness\nbmk#1476: hey, it uses the first letter more than would be if randomly sampled\nbmk#1476: HUMONGOUS-14 w"
    },
    {
      "id": "271",
      "message": "Sid#2121: fuck\nDaj#7482: It's less than a byte in ascii\nbmk#1476: i actually ran an analysis on the most representative byte\nbmk#1476: well, codepoint actually\nSid#2121: ```An ASCII character in 8-bit ASCII encoding is 8 bits (1 byte), though it can fit in 7 bits.``` well whaddya know\nbmk#1476: https://gist.github.com/leogao2/6f0cb98e63126cc40759e58df7c511a8\nbmk#1476: the most representative byte is ` `\nSid#2121: *nice*\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/7359727"
    },
    {
      "id": "272",
      "message": "bmk#1476: > same order of magnitude\nbmk#1476: > like, <1TB vs 200 TB of availabile data\nbmk#1476: I think he meant within the same order of magnitude number of orders of magnitude\nbmk#1476: I can't wait for the day when we're just training transformers on bytes. Like, not even bothering to convert images into RGB tensors, just feeding the raw png bytes directly into the network\nSome One#7897: Joined the server.\nStedman#4720: Joined the server.\nStedman#4564: Joined the server.\nshawwn#3694: @Some "
    },
    {
      "id": "273",
      "message": "Deleted User#0000: https://www.youtube.com/watch?v=hAooAOFRsYc\nDeleted User#0000: point you at one of Yannic's videos ðŸ™‚\nDeleted User#0000: i think there's something there\nSid#2121: ooh, going out now, but i'll watch that later\nSid#2121: we have @aquajet working on getting universal transformer working\nDeleted User#0000: actually i know there's something there, because a researcher used one of my linear implementations for a gigantic sequence task\nDeleted User#0000: and it worked\nDeleted User#000"
    },
    {
      "id": "274",
      "message": "aquajet#7800: I've never used jax\naquajet#7800: I remember people saying there was a speed improvement with Julia, although idk how good ml frameworks are there rn\nDeleted User#0000: so i know i spent a lot of time on reformer, but it actually never worked that well\nDeleted User#0000: so i wouldn't recommend lsh based attentoin yet, more research needed there\nDeleted User#0000: i think the most interesting devleopment is EPFL's Transformer as RNN\nDeleted User#0000: because it can give up to 4000"
    },
    {
      "id": "275",
      "message": "Deleted User#0000: i'm not familiar with wavenet actually, not that i know of, but i'm not an expert there\nDeleted User#0000: mostly LMS are bidirectional LSTMs (now defunct)\naquajet#7800: > none of the sparse attention variants\n@Deleted User So is it proven that sparse attention will always not be as performant as full attention or do we just not know of a variant that works rn\nDeleted User#0000: pure attention, or mix of dynamic convolutions with attention\nbmk#1476: Right now we're using mostl"
    },
    {
      "id": "276",
      "message": "bmk#1476: that's more than enough to cover an 2048 context\nDeleted User#0000: yeah, the \"Do Transformers need Long Range\" paper shows you can waste computation doing long range sparse attention on the bottom layers\naquajet#7800: makes sense, thanks for the help everyone\nbmk#1476: bottm = close to the input or output?\nDeleted User#0000: which are not necessary, since they are still integrating local information\nbmk#1476: oh\nDeleted User#0000: i think Gwern's hypothesis is worth testing, whether c"
    },
    {
      "id": "277",
      "message": "Deleted User#0000: bigger may be just better.\nbmk#1476: so it will be interesting to see if smaller embd + smaller tokens cancel out\nDeleted User#0000: i think it's safer to go with the dimensions in the OpenAI paper\nbmk#1476: me too\nbmk#1476: though i do want to see a universal unicode transformer eventually\nDeleted User#0000: definitely will happen.. in our lifetimes, if we don't get destroyed by this virus\nDeleted User#0000: lol\nbmk#1476: the virus *itself* probably wont destroy us\nDeleted Us"
    },
    {
      "id": "278",
      "message": "Deleted User#0000: will let you know how ti goes\nSid#2121: oh nice, well that's really good news for us since there's a ready to go implementation in mtf\nSid#2121: @Deleted User if you want the code to look over - https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/layers.py#L1915\nDeleted User#0000: ```memory_pos = (\nmtf.range(x.mesh, memory_length, dtype=tf.int32) * compression_factor\n+ (compression_factor - 1))```\nDeleted User#0000: perfect\nDeleted User#0000: the masking is the only"
    },
    {
      "id": "279",
      "message": "aquajet#7800: https://openai.com/blog/image-gpt/\naquajet#7800: and recieved pretty decent results\njsiewierski#2124: Joined the server.\naquajet#7800: idk how decoding would work though. You can't just autoregress the normal way since the ouput is completely different from the input\ncrcdng#8439: Joined the server.\nwoopwoop#6813: Joined the server.\nDeleted User#0000: @archivus what are you trying to do? do you mean whether attention networks can accept images?\nSid#2121: > if you do this scheme, you"
    },
    {
      "id": "280",
      "message": "Deleted User#0000: which is in line with what other papers do\nDeleted User#0000: (Longformer, Routing Transformer, etc)\ncdossman#8999: Joined the server.\nSid#2121: Hey @cdossman ! Welcome to the TPU go brrr hotline. Do you need your TPUs to be brred today?\nSid#2121: (pls check the channel description for an actual description of what this place is, hah)\nbmk#1476: *brrrrrrrrrrr*\nsummerstay#1153: Joined the server.\nfairQ#1013: Joined the server.\nJP#1336: Joined the server.\nSid#2121: Hey @summersta"
    },
    {
      "id": "281",
      "message": "cdossman#8999: @Sid I hear you guys are working on a open source GPT-3\nastralplain#7852: Joined the server.\nSid#2121: Hey @cdossman ! That's the gist of it yeah, along with a massive open source dataset - #the-pile . There's lots more info in the google doc in the channel description\ncdossman#8999: Reading it now\nSid#2121: Hey @astralplain ! Welcome to Neuromancer's RAM! Check the google doc in the channel description for more info and please reach out if you have any questions ðŸ™‚\nzkf#7512: Joine"
    },
    {
      "id": "282",
      "message": "bmk#1476: math (for completeness, feel free to skip if you already know this stuff):\nhttps://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr\nhttps://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab\nhttps://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\nmore specific stuff for this project:\nhttp://jalammar.github.io/illustrated-transformer/\nhttps://arxiv.org/abs/1706.03762\nhttps://arxiv.org/abs/1811.02084\nhttps://d4mucfpksywv.cloudfront.net/bette"
    },
    {
      "id": "283",
      "message": "zkf#7512: Have you folks considered using any of the global memory techniques from Longformer/Extended Transformer and applying them to GPT-*? You can get a lot of the benefits of full (non sparse) attention,but potentially expand the input size to thousands or tens of thousands of tokens...\nLouis#0144: locality sensitive hashing is great\nLouis#0144: dense attention is not great\nLouis#0144: lmao\nLouis#0144: there is a sparse version of LSH\nLouis#0144: it distributes really well too, I saw a pape"
    },
    {
      "id": "284",
      "message": "Louis#0144: theres specific examples where feature engineered attention vectors actually beats full attention\nLouis#0144: particularly in NTM\nLouis#0144: attention is a weird beast\nzkf#7512: If you have extra structure ETC letâ€™s you put it into a graph and direct attention preferentially along certain paths\nLouis#0144: so just a GCN?\nLouis#0144: lol\nzkf#7512: So you can have sentences attend mostly to themselves and their neighbors\nzkf#7512: GCN?\nLouis#0144: graphical convolutional network\nzkf#7"
    },
    {
      "id": "285",
      "message": "Louis#0144: Basically lets say you are doing HotPotQA and you have all the words in your document. Every word is a vertex. The edges between the vertex constraints your attention\nLouis#0144: We beat roberta on a few QA tasks\nLouis#0144: by a lot too\nzkf#7512: Nice\nzkf#7512: I think ETC is still leader on hotpot? Not sure\nLouis#0144: are they?\nLouis#0144: we got 70 EM\nLouis#0144: Idk if we submitted yet\nzkf#7512: Ah no theyâ€™re #4 now. God it moves fast\nLouis#0144: no we didnt submit yet\nLouis#014"
    },
    {
      "id": "286",
      "message": "zkf#7512: So thereâ€™s no preprint for your paper?\nLouis#0144: No, not on arxiv\nzkf#7512: Gotcha. Yeah I think ETC and Longformer dont have this same problem\nzkf#7512: Partially because it achieves full attention but only via the global memory\nzkf#7512: Not to say that your paper isnâ€™t awesome btw\nzkf#7512: It sounds that way\nLouis#0144: lmao its ok, longformer is rly impressive\nLouis#0144: but I have a thing for sparsity\nLouis#0144: I put a rant in #alignment-general\nLouis#0144: LOL\nzkf#7512: Any"
    },
    {
      "id": "287",
      "message": "bmk#1476: gpt-3 is just local + global attention iirc so it should be doable\nDeleted User#0000: yea, i have a list of things in my mind that are 'safe bets', and local attention is in there\nDeleted User#0000: local sliding windowed attention, the strided one can be used to supplement a bit\nbmk#1476: in fact, I think gpt-3 overdid the global attention by having it every other layer\nbmk#1476: we could probably get away with one global layer every 4 or 8 layers\nbmk#1476: that's due for an experimen"
    },
    {
      "id": "288",
      "message": "Deleted User#0000: not a 'safe bet' at the moment lol\nDeleted User#0000: but something to keep an eye on\nx.#1490: ok so i'm a big doofy ignoramus but i think that if i look at the code i can probably come up with something smart to say about it and, hell, even actually help\nx.#1490: @Daj i want in\nx.#1490: my github is excinera\nbmk#1476: https://discordapp.com/channels/729741769192767510/729741769738158194/736374402366832681\nbmk#1476: here's a reading list for you to begin with\nx.#1490: one time"
    },
    {
      "id": "289",
      "message": "x.#1490: is it cheating if i ask gpt-3 to explain stuff to me if i get stuck\nx.#1490: because ive been doing that so far and it seems to have increased my learning rate by like a factor of 3\naquajet#7800: really?\naquajet#7800: does it ever mess up the explanation?\nKen731#2990: Joined the server.\nx.#1490: there have been a few times where i thought it was just completely full of crap\nx.#1490: like i was trying to ask it how a conscious experience of dreaming was integrated, and it was saying \"rig"
    },
    {
      "id": "290",
      "message": "aquajet#7800: Hi @Collin @LOSTEN @axolotl @Deleted User @EvgeniyZh! Welcome to the TPU Grand Prix! Check out the google doc in the channel description for more info\nRavna#1831: Joined the server.\nDroid#1581: Joined the server.\nSid#2121: Hey @Ravna @Droid ! Welcome to GPT-Discord simulator! Where everyone is 100% simulated. Please check the channel description for more info on the project ðŸ™‚\nshawwn#3694: morning o/\nmojosmojo#4687: I love a good GPT joke early in the morning!\nshawwn#3694: Why'd the"
    },
    {
      "id": "291",
      "message": "shuki#3543: Joined the server.\nDaj#7482: Hey @shuki ! Welcome to the Mesh Tensorflow Autopsy Division! Check the channel topic for info and doN't hesitate to ask questions!\nTeqnicolor#8109: If anybody here is a fan of Slatestarcodex or Lesswrong we do an online meetup here hosted by David Friedman every Saturday 1 P.M. PST. https://hubs.mozilla.com/q7PLxgT/pristine-miniature-congregation\nKen#8338: Joined the server.\ngwern#1782: https://www.reddit.com/r/MachineLearning/comments/hxvts0/d_breaking_"
    },
    {
      "id": "292",
      "message": "Sid#2121: we keep the resources channel off limits for discussion but everyone can add to them like that\nSid#2121: Hey @dnlbreen ! Welcome to the CHONK-LM server! Where we make snorlax-inspired language models. Please check the channel description for a project overview ðŸ™‚\nLouis#0144: !addresource #memes https://cdn.discordapp.com/attachments/729741769738158194/736748759698112573/image0.jpg\nSid#2121: we should make Isaac tag the user who posted as well\nSid#2121: will try to implement soon, hah\nSi"
    },
    {
      "id": "293",
      "message": "Sid#2121: oh woah\nSid#2121: ```In particular, we train the GPT-2 model on a large number of generated expressions that express the process of computing multiplication step by step.``` we should add this to the pile\nSid#2121: yep, it's just regular gpt\nSid#2121: so cool\nDeleted User#0000: yup, with one twist, they spell out the operations to get to the solution. just like how we were taught in third fourth grade!\nDeleted User#0000: ex. 39*96;39*6;9*6;=54;3*6;=18;5+18;58030112000=23;=234;39*9;9*9;"
    },
    {
      "id": "294",
      "message": "Sid#2121: yeah, that's what i found as well\nSid#2121: what we did was started from this toy model https://github.com/tensorflow/mesh/blob/master/examples/toy_model_tpu.py\nhahaha#7338: Joined the server.\nSid#2121: it gives you the basic structure - how you should use Dimensions, how to lay out all the tensors on the grid, etc\nSid#2121: and it's very easy to build from there ðŸ™‚\nSid#2121: let me know if you need any help and when you're proficient enough, I will literally sacrifice my firstborn for "
    },
    {
      "id": "295",
      "message": "Daj#7482: I don't know if people have experimented with large LM using char encoding\nDaj#7482: At least I'm not aware of any large scale experiments\nSid#2121: > Well my idea is based on the assumption that whatever makes char encoding supposedly inferior to BPE won't be solved very soon and we would be stuck with BPE for a while.\n@Ravna *kindiana has entered the chat*\nkindiana#1016: also BPE is not restricted to 2 bytes, it could be significantly longer with the large vocab sizes used in most la"
    },
    {
      "id": "296",
      "message": "bmk#1476: just crawl the internet\nRavna#1831: i was just about to say that lol, gzipped context = free context enlargement\nbmk#1476: for images, executables\nbmk#1476: if it has bytes it goes in the pile\nDaj#7482: \"GPT Neo write Paperclip_Maximizer.exe\"\nbmk#1476: i cant wait for models that powerful lol\nbmk#1476: unfortunately sampling will be a right pain\nDaj#7482: > if it has bytes it goes in the pile\n@bmk W-What data doesn't have bytes?\nbmk#1476: Â¯\\_(ãƒ„)_/Â¯\nDaj#7482: _Forbidden data_\nbmk#1476: "
    },
    {
      "id": "297",
      "message": "SmartManoj#1319: Joined the server.\nbrianweet#9814: Joined the server.\nDaj#7482: Hey @SmartManoj @brianweet ! Welcome to the Git Branch Jungle! See the channel topic for info and don't hesitate to ask questions!\nBrian#0686: Joined the server.\nDaj#7482: Hey @Brian ! Welcome to the Bug Farm! See the channel topic for info and don't hesitate to ask questions!\nmirage#1049: Joined the server.\nenter#5600: Joined the server.\nshawwn#3694: Hi @mirage, @enter.\nCommutative Conjecture#6969: I looked up into"
    },
    {
      "id": "298",
      "message": "kindiana#1016: just due to kernel launching overhead and cache effects\nkindiana#1016: theoretically its the same number of flops if you do bs=1 * 100 or bs=100\nkindiana#1016: but the second will run a lot faster in practice\nCommutative Conjecture#6969: i thought the main benefit of batch was parallelization\nCommutative Conjecture#6969: like, if you are online, you can't process k+1th example before kth because you need the update from k first\nCommutative Conjecture#6969: but ok\nCommutative Conje"
    },
    {
      "id": "299",
      "message": "CheshireCat#1425: Joined the server.\ncogscides#0387: Joined the server.\nSid#2121: Hey @cogscides ! Welcome to the loss valley! Please check the channel description for more info on the project, and reach out if you have any questions ðŸ™‚\nshawwn#3694: poor @CheshireCat wasn't welcomed\nSid#2121: i didn't seem to be able to tag him :/\nSid#2121: i thought he left\nDR.PROACT#2111: Joined the server.\nDaj#7482: Hey @DR.PROACT ! Welcome to the Schroedinger Machine Learning Group! Check the channel descript"
    },
    {
      "id": "300",
      "message": "Daj#7482: yea but we need to haha\nDR.PROACT#2111: I. Is there any way to get access to gpt 3 other than open ai's invite? II. Is anyone here interested in testing ideas related to research (I'm in medical academia).\nSid#2121: not the official gpt3, that's why we're trying to replicate it\nSid#2121: we'll be open sourcing once we're done, but we still have a lot of work ahead of us. we're a small group.\nDaj#7482: We're pretty interested in research in general and a lot of people here are in academ"
    },
    {
      "id": "301",
      "message": "Louis#0144: Iâ€™m rly pro keeping GPT away from patient dada\nLouis#0144: Data\narchivus#7382: @DR.PROACT you an actual medical doctor?\nDR.PROACT#2111: Yes. All data has patient informed consent for research use. The data is mainly symptom frequency of certain diseases. But the idea of having gpt-3 enabled would be maybe find associations not previously reported in the literature. Prime it with published studies on disease x, and see what happens.\nSid#2121: @DR.PROACT cool. with our model you could "
    },
    {
      "id": "302",
      "message": "@Daj yeah tables are important. But other results are usually written in the manuscript.\nDR.PROACT#2111: So it wouldn't be too crazy to think it may be able to surprise us just with that data.\nDaj#7482: I'm sure it could surprise us, it's probably just very non trivial to figure out what of its output is useful and what is hallucinated\narchivus#7382: tbh I don't think GPT3 would be the right architecture to find associations\nDaj#7482: GPT3 is magic until proven otherwise\nSid#2121: @archivus @bmk"
    },
    {
      "id": "303",
      "message": "Daj#7482: haha\nbmk#1476: > **A2E is like lipofuscin in that both are substances that accumulate in cells impeding cell function**, but whereas lipofuscin accumulates as an intracellular byproduct of normal cellular metabolism, A2E accumulates as an extracellular product of light exposure.\nbmk#1476: Copy pasting some analogies\nDaj#7482: yea just commenting on that volcano quote haha\nstar#5322: @Daj out of curiosity what's wrong with LibreAI as a name?\nDaj#7482: It's already taken by a different o"
    },
    {
      "id": "304",
      "message": "archivus#7382:  https://cdn.discordapp.com/attachments/729741769738158194/737722220251709480/twitter_Ed_W6H1WAAALk92.jpg\narchivus#7382: Here is the input (the highlighted part only)\narchivus#7382: And here was the generated part https://cdn.discordapp.com/attachments/729741769738158194/737722322198462484/twitter_Ed_W6H3X0AEcrBy.jpg\narchivus#7382: it generated it almost verbatim - meaning it it essentially rehashing what it's been trained on\narchivus#7382: it could've gone a million different way"
    },
    {
      "id": "305",
      "message": "archivus#7382: Temperature 0.1 https://cdn.discordapp.com/attachments/729741769738158194/737724284545531956/Screen_Shot_2020-07-28_at_8.32.42_PM.png\nDR.PROACT#2111: I don't want to be biased\narchivus#7382: close to the minimum\nDR.PROACT#2111: But that statement just gave me a new idea\nDR.PROACT#2111: On my research\narchivus#7382: it discovered epidemiology in the most inefficient way possible\nDR.PROACT#2111: There is no research on mayaro virus talking about the risk of urbanization\nDR.PROACT#21"
    },
    {
      "id": "306",
      "message": "archivus#7382: Last proof you can make GPT-3 say anything https://cdn.discordapp.com/attachments/729741769738158194/737725473995620475/Screen_Shot_2020-07-28_at_8.37.23_PM.png\nSid#2121: also @DR.PROACT that is really cool, i wish i knew more about the topic. please keep us updated!\narchivus#7382: Wikipedia has loads of medical data lol\nSid#2121: yeah but not *papers*\nDR.PROACT#2111: I'm talking about this statement btw\nDR.PROACT#2111:  https://cdn.discordapp.com/attachments/729741769738158194/73"
    },
    {
      "id": "307",
      "message": "bmk#1476: (/s)\nDeleted User#0000: i think roberta is not auto-regressive\nDR.PROACT#2111: Not one person has suggested mosquito lifespan as a limiting important factor for outbreaks\nbmk#1476: https://twitter.com/gwern/status/1284276584573214721\nDeleted User#0000: @DR.PROACT we are still at the stage where we are trying to get attention models to output correct answers\nbmk#1476: **\"Sampling can prove the presence of knowledge but not the absence.\"**\nDeleted User#0000: if you look at natural qa ben"
    },
    {
      "id": "308",
      "message": "bmk#1476: https://twitter.com/nicklovescode/status/1284050958977130497\nbmk#1476: \"You need to tell it what the AI is and is not capable. \" i think this is the key\narchivus#7382: let's try it out\narchivus#7382: anyone want to give me QA I should add as a prompt?\nDeleted User#0000: @bmk yea, that was a surprising finding\narchivus#7382: I rest my case https://cdn.discordapp.com/attachments/729741769738158194/737729510853705809/Screen_Shot_2020-07-28_at_8.53.31_PM.png\nDR.PROACT#2111: Try priming it "
    },
    {
      "id": "309",
      "message": "DR.PROACT#2111: Not novel per se. But novel within the context of mayaro virus.\nDR.PROACT#2111: It's as if you had a specialist on your side.\narchivus#7382: Fuck https://cdn.discordapp.com/attachments/729741769738158194/737734006321446953/Screen_Shot_2020-07-28_at_9.11.24_PM.png\nDR.PROACT#2111: Lol\nDR.PROACT#2111: It's a cognitive conundrum.\nDR.PROACT#2111: It's not making connections on purpose\narchivus#7382: Bit of a NSFW but yeah it will say anything https://cdn.discordapp.com/attachments/729"
    },
    {
      "id": "310",
      "message": "archivus#7382: Last one I promise - sorry I polluted general â¤ï¸\narchivus#7382:  https://cdn.discordapp.com/attachments/729741769738158194/737735254114173089/Screen_Shot_2020-07-28_at_9.16.07_PM.png\nbmk#1476: @DR.PROACT what is the dataset youre thinking of?\nDeleted User#0000: @archivus need to be able to fine-tune GPT-3 on all of UpToDate\narchivus#7382: It's behind a paywall but I could technically scrape it for you\narchivus#7382: I don't want to get in legal trouble lol\nbmk#1476: yes\nDeleted Us"
    },
    {
      "id": "311",
      "message": "DR.PROACT#2111: Just do up-to-date and dynamed offline\nDeleted User#0000: @DR.PROACT nice! didn't know about the torrents, thanks for the tip\nDR.PROACT#2111: > @DR.PROACT nice! didn't know about the torrents, thanks for the tip\n@Deleted User sure np\nDR.PROACT#2111: I'm a Dr so let me know if I can help in that regard.\nDR.PROACT#2111: Also if anyone has any access to gpt-3 I would greatly appreciate tinkering with it (and setting the record for most published papers in 24 hours)\nDeleted User#0000"
    },
    {
      "id": "312",
      "message": "DR.PROACT#2111: From the demos I've seen\nDR.PROACT#2111: I can imagine so many applications\nDR.PROACT#2111: And I'm not even in tech\nDR.PROACT#2111: Pretty sure someone is already way ahead\nDeleted User#0000: yea, it's still early. there's a lot of limitations to the tech still\nDeleted User#0000: but the promise is real\nDeleted User#0000: nice chatting! back to work\nDR.PROACT#2111: ðŸ––\nDaj#7482: So, seeing as the new name voting got us some great name suggestions but not too much voting, we've dec"
    },
    {
      "id": "313",
      "message": "zphang#7252: \"Luther\" for short\nshawwn#3694: who decided it was a good idea...? well, if you guys like it. *shrug*\nDaj#7482: It was the most voted on and the main contributers were unanimous\nDaj#7482: Sure just 3 to 2 votes but not my fault people didn't vote lol\nSid#2121: I vote we have a mobile name - keep changing our name every 2-3 weeks, just to keep people on their toes\nshawwn#3694: ^\naquajet#7800: or dont have a name at all\nshawwn#3694: also MobileAI\nDaj#7482: We change discord servers ev"
    },
    {
      "id": "314",
      "message": "Sid#2121: next week: CHONK.AI\nDaj#7482: tbh I don't care about publicity _at all_\nSid#2121: i am willing to bet that not one single person has said 'libreAI' with their actual mouth\nDaj#7482: And if anything, the name is mysterious and intriguing lol\nshawwn#3694: lol fair enough\nshawwn#3694: you'd be wrong SId\nSid#2121: ok, exaggeration, but obviously most 'word of mouth' spreading about us has been done through typing\nDaj#7482: Also, EleutherAI sounds cyberpunk af\nzphang#7252: we're slipping ba"
    },
    {
      "id": "315",
      "message": "zitterbewegung#4846: i think i did\nzitterbewegung#4846: netai.pw\nzitterbewegung#4846: maybe ill use netai for my podcast\nzitterbewegung#4846: with me and my deep learning project\nAI_WAIFU#2844: Joined the server.\narchivus#7382: http://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html\narchivus#7382: Relevant to our discussion yesterday\nCommutative Conjecture#6969: stupid q\nCommutative Conjecture#6969: how do you call neural networks where there are connections across layers? (and how do you"
    },
    {
      "id": "316",
      "message": "Sid#2121: also, he wants everyone to know\nbmk#1476: Skip connections are just a special case of highway networks\nCommutative Conjecture#6969: ic\nSid#2121: @Commutative Conjecture if you're interested in peeping the minor ML drama lol https://www.youtube.com/watch?v=HGYYEUSm-0Q&t=5786s skip to 1:03:00\nbmk#1476: i feel somewhat sympathetic for schmidhuber tbh\nSid#2121: afaict he's still acknowledged as quite important lol\nbmk#1476: like, getting shafted must suck\nSid#2121: > like, getting shafted "
    },
    {
      "id": "317",
      "message": "gs://neo-datasets/openwebtext-documents/\ngs://neo-datasets/openwebtext-fixed/\ngs://neo-datasets/openwebtext-new/\ngs://neo-datasets/openwebtext/```\nbmk#1476: haha\nSid#2121: i guess the original?\nSid#2121: or fixed??\nSid#2121: ??\nSid#2121: lol\nbmk#1476: isnt that the one with 1024 instead of 1025\nSid#2121: idk, best wait till daj's around\nbmk#1476: if i had to bet id bet on fixed\nbmk#1476: simple sanity check: run with smaller vocab, see if things catch on fire\nNerdimite#3840: Joined the server.\nN"
    },
    {
      "id": "318",
      "message": "bmk#1476: o\nbmk#1476: ^ u dropped  this\nbmk#1476: wait\nbmk#1476: both spellings are acceptable?\nbmk#1476: huh\nbmk#1476: nvm\nSid#2121: i was also gonna go for the o then double checked my spelling and saw no o\nSid#2121: i prefer the o\nSid#2121: adding it in\nCommutative Conjecture#6969: @Sid / @bmk\nhttps://en.wikipedia.org/wiki/Residual_neural_network\nI don't understand how the training works\nIs the update just the sum of previous layer + skip connection?\nIf not, does one of you have a good link? "
    },
    {
      "id": "319",
      "message": "bmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/737930826389520405/unknown.png\nbmk#1476: these two diagrams capture the entire idea\nbmk#1476: for more details see the paper\nbmk#1476: you literally just take the state and add it back to itself again\nbmk#1476: after a few layers\nCommutative Conjecture#6969: yeah, that's obv\nCommutative Conjecture#6969: what's not obv to me is the training\nbmk#1476: ?\nbmk#1476: you just do normal backprop, no?\nCommutative Conjecture#6969: @bmk\n"
    },
    {
      "id": "320",
      "message": "Watching it\n(I always feel special when watching an unlisted video with 10 views)\nbmk#1476: clicking through it very quick and that's a lot of other content too\nNerdimite#3840: > Hey @Nerdimite ! Welcome to the group composed entirely of one GPT-10 discord bot trying to ouroboros itself into existence! Check the channel description for more info and reach out to any blue names with any questions\n@Sid Seems pretty cool to see what is being done here. I am not very experienced with data cleaning o"
    },
    {
      "id": "321",
      "message": "AI_WAIFU#2844: reverse mode automatic differentiation\nAI_WAIFU#2844: basically\nAI_WAIFU#2844: so yes\nAI_WAIFU#2844: the algorithm was reinvented like a dozen times so it got a bunch of different names\nCommutative Conjecture#6969: ic\nCommutative Conjecture#6969: thx ðŸ™‚\nadalbertobrant#7154: Joined the server.\nCommutative Conjecture#6969: yasq (Yet Another Stupid Question)\nself-attention is not convolutional\ndoes it mean gpt-k can't learn translation-invariant patterns?\nkindiana#1016: self attention"
    },
    {
      "id": "322",
      "message": "kindiana#1016: and you can attend to different relative positions with a linear combination of different frequencies\nSid#2121: Hey @adalbertobrant ! Welcome to the AI-lab-that-cannot-be-named! Check the channel description for more info ðŸ™‚\nCommutative Conjecture#6969: @kindiana\nI don't understand positional encoding\nCommutative Conjecture#6969: why not just use the actual position\nzphang#7252: I feel like most recent models use learned position embeddings?\nSid#2121: out of interest @adalbertobran"
    },
    {
      "id": "323",
      "message": "Commutative Conjecture#6969: might be something vdumb, but where?\nkindiana#1016: neural networks don't like inputs inputs that are not around mean 0 and std 1\nCommutative Conjecture#6969: not sure why it matters, you can normalize it\nkindiana#1016: it also doesn't play nice with dot product attention\nkindiana#1016: so there is no way to attend to say 1 position to the left efficiently\nCommutative Conjecture#6969: > it also doesn't play nice with dot product attention\ndo you have details on this?"
    },
    {
      "id": "324",
      "message": "Sid#2121: https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/utils.py#L610 this is the thread you'll want to follow\nSid#2121:  https://cdn.discordapp.com/attachments/729741769738158194/737948719277801582/Screenshot_2020-07-29_at_10.24.36.png\nCommutative Conjecture#6969: erg\nkindiana#1016: looks like tfm uses regular sinusoids https://github.com/tensorflow/mesh/blob/d353aa9ff6644048bfaec737b6fb5fada03085e2/mesh_tensorflow/transformer/transformer.py#L825\nkindiana#1016: http"
    },
    {
      "id": "325",
      "message": "DR.PROACT#2111: Morning guys\nDR.PROACT#2111: Humanity is rooting for something productive to come out of here\nDR.PROACT#2111: ðŸ‘½\nbmk#1476: Moin\nTravellingSalesman#5222: What do you guys think about this recent post in LW about AI overhang https://www.lesswrong.com/posts/N6vZEnCn6A95Xn39p/are-we-in-an-ai-overhang\n\n> AnÂ overhangÂ is when you have had the ability to build transformative AI for quite some time, but you haven't because no-one's realised it's possible. Then someone does and surprise! It"
    },
    {
      "id": "326",
      "message": "Sid#2121: one of our big TODOs on data is getting clean pdf to txt extraction\nSid#2121: we're also trying to figure out ways to get a better, cleaner dataset from CC data than OSCAR. But a lot of this stuff isn't so entry level.\nbmk#1476: Another thing that needs attention is working out a good html->text pipeline\nbmk#1476: Tldr some converters suck for non English, some converters suck at forums, etc. We want to combine the best of each converter for all languages and all websites\narfa#0882: I "
    },
    {
      "id": "327",
      "message": "bmk#1476: Can you post a short sample of the data as a gist @arfa\nAlm#9130: the model would have to predict what happend in the video in order to predict the next reaction haha\nSid#2121: lmao\nSid#2121:  https://cdn.discordapp.com/attachments/729741769738158194/738169751305584761/Screenshot_2020-07-30_at_01.02.46.png\nSid#2121: wat\nSid#2121: leaving twitch now\naquajet#7800: Eloquent\narfa#0882: Yeah basically just garbage https://gist.github.com/arfafax/735f0869f041b93706b6231622a5773c\narfa#0882: B"
    },
    {
      "id": "328",
      "message": "Deleted User#0000: so how can i help\nAnders#1378: Joined the server.\naquajet#7800: Hello @Anders welcome to the Twitch chat automation service! Check the channel description for some info on the project, and please ask if you have any questions.\naquajet#7800: So rn for the model the largest goal is to make the model more efficient. There are GPT-3 117M runs training rn, but the model is not as efficient as we want it to be (25% TPU utilization, it should be possible to get it up to 50%). There a"
    },
    {
      "id": "329",
      "message": "aquajet#7800: It's not explicit funding afaik but the tpus are being used through their program\narfa#0882: But aside from that\narfa#0882: GCP bucket costs, VM costs, Hetzner costs, etc.\naquajet#7800: donations i think\nSid#2121: couple private donations. we plan on setting up a patreon at some point too\narfa#0882: So basically, crowdfunding :KEK:\ngwillen#3291: @bmk huh I'm confused, I feel like I'm missing something -- that's a few thousand dollars? To train a model bigger than GPT-3?\nbmk#1476: y"
    },
    {
      "id": "330",
      "message": "Isaac McHorse#2007: WHY ARE YOU BEING DISTRACTED? YOU CAN'T GET ANYTHING DONE LIKE THAT.\narfa#0882: You guys end up with a model that you can't run inference on without free TPUs which will be in high demand after you get media attention and everyone learns about TFRC and signs up :KEKW:\nbmk#1476: I mean dajs original GPT2 thing got a lot of attention\nbmk#1476: And TFRC still exists\nbmk#1476: Also I still kinda want to work out something that lets us do inference with a small number of GPUs\naqua"
    },
    {
      "id": "331",
      "message": "bmk#1476: meh we dont need 999999 uptime\nbmk#1476: i mean care to suggest a better solution that doesnt cost 100x more?\narfa#0882: Well, if you end up in the situation Tensorfork is in, you might have zero uptime for weeks on end\nbmk#1476: ?\nbmk#1476: what situation is that\narfa#0882: We haven't been able to create any TPUs for 2 weeks\nbmk#1476: i meant\nbmk#1476: gpu\nbmk#1476: not tpu\nbmk#1476: tpu availability wont affect uptime\narfa#0882: Ah\naquajet#7800: we invent a low cost way to convert ol"
    },
    {
      "id": "332",
      "message": "aquajet#7800: my context window is 2\narfa#0882: Basically they said \"You guys still have quota, but we're near 100% capacity in eu-west-4\" which doesn't answer why you guys can create TPU pods and we can't\nbmk#1476: o.O\narfa#0882: And I think shawwn is afraid to push the issue because he thinks we might've been manually deprioritized or you guys are getting special treatment or something\nbmk#1476: well thats all very odd behavior from Overlord Google\narfa#0882: I'm like, 99% sure it's just a bug"
    },
    {
      "id": "333",
      "message": "bmk#1476: also that reminds me, @aquajet we probably want the webserver set up such that we can seamlessly pivot to a new dataset (i.e after we get the whole multilingual pipeline worked out) while requiring as little action on the parts of server owners (without doing arb code exec ofc)\naquajet#7800: Yep\naquajet#7800: Chunk number is stored as a string so we can just put a URL in there\nbmk#1476: i mean like upgrading the pipeline\nbmk#1476: because right now we're just doing justext but we *real"
    },
    {
      "id": "334",
      "message": "aquajet#7800: Yikes\nbmk#1476: looks like we need to start from scratch\nbmk#1476: https://docs.google.com/spreadsheets/d/1k8G2M5RxEwMrQuOwj-Wx-XxFBdZqyGKFGM6nPDyr-MQ/edit?usp=sharing\nbmk#1476: heres a spreadsheet\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/738646020077912094/unknown.png\nbmk#1476: here's an empty repo https://github.com/leogao2/htmltotext-benchmark\nbmk#1476: so the idea is\nbmk#1476: 1. find pages\nbmk#1476: 2. stick in sheet\nbmk#1476: 3. manually clean\nbmk#"
    },
    {
      "id": "335",
      "message": "bmk#1476: awesome\nbmk#1476: i also have to sleep soon\nadamb#9760: well like i said, it's just sitting idle atm\nadamb#9760: so how do i talk to about putting it to work?\nDaj#7482: I think @bmk is still our data tsar atm\nbmk#1476: ok so\nbmk#1476: here's the thing\nbmk#1476: we have a current data pipeline\nbmk#1476: we're working on developing a new one over the next bit\nbmk#1476: so it you want to start *right now* you can download using the old pipeline\nbmk#1476: and we'll try to figure out someth"
    },
    {
      "id": "336",
      "message": "adamb#9760: ok\nadamb#9760: so what's the workflow\nbmk#1476: you clone a repo and run a docker command\nbmk#1476: we're currently working on a webserver to coordinate work so maybe we want to wait for that first\nadamb#9760: where do the results get sent?\nadamb#9760: are there any creds i also need?\nbmk#1476: so we dont have that part implemented yet\nbmk#1476: it would probably be easiest for us to get our new pipeline and the coordination webserver working first\nadamb#9760: ok\nadamb#9760: makes se"
    },
    {
      "id": "337",
      "message": "adamb#9760: when i was working on this i was extremely interested in enabling workflows for huge models trained on huge data\nadamb#9760: but no one was working on big enough models outside of google yet, lol\nDaj#7482: Well, if it works, this would be a great usecase\nadamb#9760: so i put it down once it was clear that the bottleneck was with ipfs\nadamb#9760: but the interface inside of tensorflow was pretty tiny\nadamb#9760: and everything worked\nadamb#9760: it just would choke on metadata traffic"
    },
    {
      "id": "338",
      "message": "Deleted User#0000: say the word 'cat' is given to the transformer\nDeleted User#0000: first you break it into 'c', 'a', 't'\nDeleted User#0000: each of those tokens have a unique id, say 3, 0, 21\nDeleted User#0000: then you fetch rows 3, 0, 21 from the embedding matrix above\nDeleted User#0000: that's it..\nDeleted User#0000: the gradient is done on the embedding matrix and you learn it just like any other parameter\nDeleted User#0000: (BPE may do something like 'ca', 't')\nDeleted User#0000: it's lea"
    },
    {
      "id": "339",
      "message": "Louis#0144: Well bilstm is contextualized too\nLouis#0144: Thatâ€™s not a fair comparison\nDeleted User#0000: ah yea, ELMO started it all i think\nLouis#0144: ELMO has good contextualization\nDeleted User#0000: or ULMfit, i forget which\nDeleted User#0000: man, bilstm's, those were the days..\nLouis#0144: I miss them\nLouis#0144: I beat Roberta with a bilstm@\nLouis#0144: Lmao\nDeleted User#0000: and all the vanishing exploding gradient problems?\nDeleted User#0000: lol\nLouis#0144: You can avoid that by usi"
    },
    {
      "id": "340",
      "message": "Deleted User#0000: good to know!\nLouis#0144: Usually people use residual gated graph neural networks\nLouis#0144: GCNs are good when you need to represent lots of sparse knowledge\nLouis#0144: Like multi doc QA\nLouis#0144: A transformer is just a dense GCN\nzphang#7252: is that on the hotpotqa leaderboard? so many anonymous submissions...\nLouis#0144: Lol\nLouis#0144: Yeah but a lot of them are GCN based\nLouis#0144: Almost all the open domain ones use a GCN\nLouis#0144: and like half the closed domain"
    },
    {
      "id": "341",
      "message": "Louis#0144: Can I show my paper if I get accepted\nzphang#7252: which conf?\nLouis#0144: EMNLP\nzphang#7252: aha\nstar#5322: Does someone have a two sentence of what a GCN is?\nzphang#7252: I'd be interested to read your stuff\nLouis#0144: CNNs... but for graphs\nLouis#0144: Literally tho\nLouis#0144: Laplacian smoothing is convolution on a 0 simplex\nLouis#0144: Thatâ€™s all that GCNs do\nLouis#0144: Itâ€™s literally a CNN\nLouis#0144: but where the image is a graph\nLouis#0144: And not a real image\nstar#5322:"
    },
    {
      "id": "342",
      "message": "Louis#0144: @arvindr9\nLouis#0144: hi\narvindr9#4837: Lol hi\nE McNeill#1259: Joined the server.\nbhauth#7283: Joined the server.\nDaj#7482: Hey @technologiclee @arvindr9 @E McNeill @bhauth ! Welcome to the Mesa Optimizer Removal Facility! Check the channel topic for info and don't hesitate to ask questions!\nCommutative Conjecture#6969: Does anyone understand or have some adhoc explanation for how GPT3 can be so good at grammar?\nkindiana#1016: grammar seems like something that would be pretty easy fo"
    },
    {
      "id": "343",
      "message": "Commutative Conjecture#6969: > why can't transformers learn grammar? it seems like a pretty ideal case for multi headed attention (i.e. one head attends to the tense of the sentence, one head attends to the subject etc), and the output layers can take information from that to synthesize the next most likely word(piece)\n@kindiana\nI believe transformers can, I'm mostly interested in the how\nCommutative Conjecture#6969: But I believe it has a much stronger grasp of grammar than just aggregating inf"
    },
    {
      "id": "344",
      "message": "Louis#0144: For the reasons mentioned above\nLouis#0144: You only really need local attention\nLouis#0144: And you only really need a few general rules + some random memorized exceptions\nCommutative Conjecture#6969: > Modern LMs struggle very very little with grammar\n@Louis\nBy modern, do you mean, NNs based?\nCommutative Conjecture#6969: > You only really need local attention\n@Louis\nFor formal grammars, that seems def wrong\nLouis#0144: I mean even pre NN didnâ€™t really have issues\nLouis#0144: Rule b"
    },
    {
      "id": "345",
      "message": "Louis#0144: Yeah\nLouis#0144: Iâ€™m not convinced grammar is really an issue\nLouis#0144: The main issue that faces LMs right now is causality imo\nLouis#0144: And coherency\nLouis#0144: Also bigger context windows would be nice\nLouis#0144: Lmao\nCommutative Conjecture#6969: > Yeah\n@Louis\nLink?\nLouis#0144: https://www.researchgate.net/publication/239556866_A_Rule-Based_Style_and_Grammar_Checker\nCommutative Conjecture#6969: > Iâ€™m not convinced grammar is really an issue\n@Louis  @Daj\n\nThat's what I asked"
    },
    {
      "id": "346",
      "message": "Louis#0144: Thatâ€™s an issue with coherency imo\nLouis#0144: But yeah\nLouis#0144: No idea why\nCommutative Conjecture#6969: > https://www.researchgate.net/publication/239556866_A_Rule-Based_Style_and_Grammar_Checker\n@Louis\nCan't see the abstract, but grammar checking has nothing to do with grammar learning\nDaj#7482: I think we understand GPT3 as much as alchemists did quantum physics\nCommutative Conjecture#6969: Former is super easier on all plans\nLouis#0144: Grammar checkers can be used to constru"
    },
    {
      "id": "347",
      "message": "Daj#7482: When have my custom introductions ever been inaccurate? hahah\nDeleted User#0000: @Commutative Conjecture https://arxiv.org/abs/1906.04341 there's a lot of papers along this theme\nDeleted User#0000: https://www.pnas.org/content/early/2020/06/02/1907367117\nDeleted User#0000: Chris Manning himself\nCommutative Conjecture#6969: @Deleted User thx\nCommutative Conjecture#6969: ~~(YOU'VE GOT TO STOP 2 COLUMNS PAPERS)~~\nDeleted User#0000: @Commutative Conjecture lol\narfa#0882: Dumb idea: feed th"
    },
    {
      "id": "348",
      "message": "bmk#1476: https://twitter.com/nabla_theta/status/1289977562882424838\nstar#5322: yeah the \"whether it understands\" point seems, completely an airball to me\nbmk#1476: i know gary marcus of all people probably isnt going to change his mind but hey\nstar#5322: I liked gwern's point about like, sifting, idr which post it was in\nstar#5322: but it was like, suppose you want a plausible page of shakespeare - the library of babel contains many of these, but you'll have to sort a huge exponential number of"
    },
    {
      "id": "349",
      "message": "bmk#1476: absolutely yes\nbmk#1476: i cite gwern everywhere lol\nDaj#7482: It takes me a while, but yes haha\nbmk#1476: gwern put this so much more elegantly than me lol\nbmk#1476: for reference, here's a snippet from my current draft:\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/739539314454560909/unknown.png\narchivus#7382: Can someone train GPT-3 to write in the style of @gwern\nshawwn#3694: actually, that'd be an interesting fine-tuning idea for GPT-2\nbmk#1476: Gwern has wr"
    },
    {
      "id": "350",
      "message": "thenightocean#6100: anyway, hello everyone!\nCommutative Conjecture#6969: Hi\nCommutative Conjecture#6969: @miguelos\nHi, were you on the LW Discord server at some point?\nmiguelos#7956: Yes\nCommutative Conjecture#6969: ok\nCommutative Conjecture#6969: I was wondering whether that was you when you rooted for the universal knowledge graph\nmiguelos#7956: That would be me\nshawwn#3694: Lesswrong has a discord server? Anyone got an invite?\nNoa Nabeshima#0290: https://discord.gg/p73nrR\nNoa Nabeshima#0290: "
    },
    {
      "id": "351",
      "message": "Noa Nabeshima#0290: Actually it's unclear how the model would learn\nNoa Nabeshima#0290: er, what it would learn from the BPE-dropout\nNoa Nabeshima#0290: to me, I haven't read the papers\nDeleted User#0000: Joined the server.\nNoa Nabeshima#0290: By restricting a model with BPE-dropout to small length tokens for generation you might improve poem quality\nCommutative Conjecture#6969: @shawwn it's ded though\nshawwn#3694: ah\nCommutative Conjecture#6969: sry\nCommutative Conjecture#6969: there is an ssc "
    },
    {
      "id": "352",
      "message": "Louis#0144: Oh\nCommutative Conjecture#6969: rats\nmiguelos#7956: GPT-3 is so dumb that it doesn't know what GPT-3 is.\nmshang#7454: Joined the server.\nKazumi#1297: I bet GPT-3 wasn't in the dataset that GPT-3 was trained on\nthenightocean#6100: So if I understand correctly, you guys think gpt-neo might advance faster and cheaper than whats described in this video? https://www.youtube.com/watch?v=kpiY_LemaTc\nthenightocean#6100:  https://cdn.discordapp.com/attachments/729741769738158194/7397718933133"
    },
    {
      "id": "353",
      "message": "Daj#7482: > Can GPT-3 detect that text is an output of GPT-3? I wonder how it would play out if people would scrape data from the internet where content marketers would be using GPT-3 to create articles. Feedback loop?\n@Kacper WikieÅ‚ iirc OA did some research on this for GPT2, haven't really followed for GPT3 because I think content based filtering is a dead end\nRavna#1831: Anyway the biggest force behind the exponential growth so far is still the budget size, which is the easiest to scale. So w"
    },
    {
      "id": "354",
      "message": "Daj#7482: Hey @Aran Komatsuzaki ! Welcome to the FOOMigation Tent! (I'm sorry) Check the description for info and don't hesitate to ask questions!\nAran Komatsuzaki#5714: Joined the server.\nAI_WAIFU#2844: I think we can scale far higher than 10T with appropriate architectural improvements. Gshard is a 0.6T model and it only took 4 days to train on a TPU 2048-v3. If someone can figure out an architecture that scales efficiently across clusters, the sky's the limit.\nAI_WAIFU#2844: Also @Daj, if I w"
    },
    {
      "id": "355",
      "message": "bmk#1476: Does aws do that?\nbmk#1476: Huh\nadamb#9760: lambda is an extremely impressive resource for burst CPU\nadamb#9760: https://github.com/StanfordSNR/gg\nbmk#1476: Well\nAI_WAIFU#2844: No, is it a private repo?\nbmk#1476: It's not so much burst\nDaj#7482: Yes repo is private until release\nadamb#9760: get 1000s of cores for 100ms and then give them back\nDaj#7482: We need 1000 cores for ~1 week\nadamb#9760: are you sure?\nDaj#7482: though that's for the full sized data set\nDaj#7482: We need less in "
    },
    {
      "id": "356",
      "message": "AI_WAIFU#2844: https://github.com/AI-WAIFU\nDaj#7482: invite sent\nDaj#7482: Most Mesh tensorflow discussion happens in #gpt-neox-devs\nbmk#1476: Also we need to ingress 3.5pb\nbmk#1476: I don't think that's happening in an hour\nAI_WAIFU#2844: Great! I'll start crawling around the code base. If I've got questions I'll bring them up in #gpt-neox-devs .\nDaj#7482: We'll be happy to help you get up to speed ðŸ‘Œ Currently we have really low efficiency, the MTF team claims they can reach around ~50% TPU uti"
    },
    {
      "id": "357",
      "message": "AI_WAIFU#2844: Another thing I was thinking of trying, but haven't had time to, is just dumb gradient boosting.\nAI_WAIFU#2844: It's not parallel, but it does get around the problem of running out of ram.\nAI_WAIFU#2844: You train one model, then you save the logits of that model on the training data, and train the next model as an additive perturbation of the first.\nAI_WAIFU#2844: Repeat until you run out of patience.\nAI_WAIFU#2844: Or disk space.\nAI_WAIFU#2844: There's also a bunch of work on lo"
    },
    {
      "id": "358",
      "message": "of model parallelism within each matrix multiply and model parallelism across the layers of the network. All models\nwere trained on V100 GPUâ€™s on part of a high-bandwidth cluster provided by Microsoft.\"\nbmk#1476: They were horribly vague about it as usual\nbmk#1476: Yeah that\nbmk#1476: Basically mtf as we have it right now plus gpipe\nbmk#1476: It turns out that gpipe like stuff isn't even necessary actually\nbmk#1476: Maybe gpiping increases efficiency, or maybe OA's cluster just wasnt big enough\n"
    },
    {
      "id": "359",
      "message": "bmk#1476: so\nbmk#1476: that's not *that* many weeks\nmiguelos#7956: You mean Neo GPT-3, or OpenAI releasing GPT-3?\nbmk#1476: neogpt3\nbmk#1476: or opengpt3\nbmk#1476: or whatever the heck we decide to call it\nbmk#1476: oa wont releast gpt3 lol\nmiguelos#7956: What's the next best easily accessible model?\nbmk#1476: or if they do ill be very surprised\nbmk#1476: what do you need it for\nmiguelos#7956: Haven't they released GPT-2 in the end?\nbmk#1476: yeah but theyre using gpt3 to make $$$\nmiguelos#7956:"
    },
    {
      "id": "360",
      "message": "miguelos#7956: What's the biggest time/resource sink, in order?\nmiguelos#7956: 1. Training (6 months)\n2. Collecting data (1 month)\n3. Fixing code (1 month)\nmiguelos#7956: ?\nbmk#1476: look if you have a really good idea you can try asking OA\nbmk#1476: we're all volunteers we have no time frame\nbmk#1476: we do stuff when we have free time\nbmk#1476: actually @miguelos do you need continued access or do you just want to try a few prompts\nmiguelos#7956: Continued access.\nbmk#1476: hm\nbmk#1476: sorry,"
    },
    {
      "id": "361",
      "message": "miguelos#7956: Do we have enough, bit in the wrong format (html)? Or do we need a lot of effort to crawl for more data?\nmiguelos#7956: I speak French.\nbmk#1476: awesome\nbmk#1476: the plan rn is:\nbmk#1476: making multilingual dataset to benchmark htmltotext converters -> tuning htmltotext converters and using the benchmarker as a guide -> using htmltotext converter to convert 3.5PB of html to txt\nbmk#1476: -> training gpt3\nbmk#1476: keep in mind this plan is not optimized for speed\nbmk#1476: but "
    },
    {
      "id": "362",
      "message": "bmk#1476: that would be very helpful\nmiguelos#7956: Cleaning data? What are we talking about? Removing some kind of sensitive data, or make it all into a uniform format that won't confuse it too much?\nbmk#1476: the latter\nmiguelos#7956: I'm surprised getting a lot of training data isn't a solved problem.\nbmk#1476: im surprised too\nbmk#1476: it's actually quite complex\nmiguelos#7956: What are we starting with? Open domain books? Wikipedia? Reddit? Twitter?\nmiguelos#7956: Kat makes the bulk of tha"
    },
    {
      "id": "363",
      "message": "miguelos#7956: I also found an overview of GPT-3 dataset. Are the stated tokens the number of words? Are we trying to match 1:1 what they had?\nmiguelos#7956: @Alm: I'm looking to do context-free natural language parsing.\nstar#5322: I don't think GPT-2 is that crazy expensive to run, maybe I'm off my ass but doesn't it run on one GPU?\nstar#5322: \"it's good to be familiar with the literature\" is about the most condescending way to start a tweet thread I've ever seen\nmiguelos#7956: I read the GPT-3"
    },
    {
      "id": "364",
      "message": "star#5322: 60 or 70 years ago we thought you could build AI in a summer, but that turned out to be hubris.\nmiguelos#7956: Depends on what AI means.\nmiguelos#7956: But surely all of these people were trying to workaround the lack of compute. That's the only sane reason for symbolic AI.\nmiguelos#7956: But to imagine the general purpose architecture isn't hard. It's trivial to build AI with infinite compute.\nmiguelos#7956: Starting from that, we have to take a few shortcuts, reduce the scope, speci"
    },
    {
      "id": "365",
      "message": "aquajet#7800: Hello @SCnightmare! Welcome to the AGI Fire Alarm Factory! Check out the channel description and pinned posts for information about our current projects and feel free to ask any questions\nNoa Nabeshima#0290: > But will it scale?\nI'm actually curious about this\nstar#5322: I'm also pretty curious. Didn't mean to detract\nstar#5322: It does seem like there is an effect where you can overfit to a particular dataset size / target, and sort of loose generality when scaled up\nstar#5322: Or"
    },
    {
      "id": "366",
      "message": "2020-08-03: I spent $2\n2020-08-04: I lost $8\n\nToday's balance: $5\nYesterday's balance: ___\nkindiana#1016: trained with the right data, certainly, unsure if openai's gpt3 weights/training data can do it though\nmiguelos#7956: It seems like it would understand that earned/made and spent/lost are somewhat equivalent. It can do basic mathematics. I haven't seen how it deals with dates/time/chronological stuff.\nRavna#1831: 1.8/2.8 times fewer parameters with the same performance could come easily from"
    },
    {
      "id": "367",
      "message": "> 2020-08-02: I made $10\n> 2020-08-03: I spent $2\n> 2020-08-04: I lost $8\n>\n> Today's balance: $5\n> Yesterday's balance: ___\n@miguelos  At 0 temperature GPT3 gives $10 dollars\nmiguelos#7956: @Noa Nabeshima 0 temperature?\nSid#2121: @miguelos temperature is a setting when you're sampling from the model. On a very basic level, a higher temperature = more variation and a lower temperature = more certainty. So a lower temperature is better for question answering, but a higher temperature may be bette"
    },
    {
      "id": "368",
      "message": "Louis#0144: yeah most people use VAEs i guess\nLouis#0144: but this is a question about AEs\nLouis#0144: like boring naive AEs\nstar#5322: cause vanilla AEs aren't forced to use their latent space in a good way so there's kinda no way to sample\nstar#5322: sure, I guess that's right then?\nstar#5322: But not necessarily, like the decoder doesn't necessarily encode nearby objects as the same or nearby logits\nstar#5322: like I can encode all dogs as 0,0,0 and all cats as 0,0,0.1 if I want\nstar#5322: pr"
    },
    {
      "id": "369",
      "message": "star#5322: and it might be hard to incrementally co-evolve an encoder and a decoder that do the right thing\nDaj#7482: I'm in general with you on never assuming simplicity in learned solutions\nDaj#7482: But that paper was interesting\nstar#5322: I think double descent is one of the most tantalizing pieces of evidence shaping an actual understanding of how NNs and SGD, deep learning, work at all\nstar#5322: It's like, not really enough to clarify everything, but nevertheless really interesting\nDaj#7"
    },
    {
      "id": "370",
      "message": "star#5322: yup\nDaj#7482: My own completely baseless (but testable) theory is that it relates to the microjitters of loss between individual batches\nstar#5322: but I would hesitate to jump from very broad claims like \"for some reason, we seem to repeatedly observe DL generalizing better than we'd possibly expect\" to very specific claims like \"the decoder in a vanilla AE is probably undoing a sphere packing\" (which, idk if Louis was exactly claiming that, but I'm definitely skeptical)\nDaj#7482: Af"
    },
    {
      "id": "371",
      "message": "Daj#7482: Exactly haha\nstar#5322: it seemed like there was already plenty of evidence his points don't have that kind of thought behind them, sadly\nstar#5322: and are more like reflexive reactions/dismissals\nDaj#7482: It's fascinating how there's always someone to fill every intellectual sophism niche\nstar#5322: Also something about the minibatch gradient noise definitely seems important\nDaj#7482: That's where my money is atm, might be worth a boring but useful research project\nstar#5322: that's"
    },
    {
      "id": "372",
      "message": "Daj#7482: Yea\nstar#5322: and models don't inference differently on different orders of data, right?\nDaj#7482: I'm implying something weird is happening that happens across timesteps\nDaj#7482: > and models don't inference differently on different orders of data, right?\n@star But they will have different intermediate gradients\nbmk#1476: Re: intelligence, Some people just cant accept the idea that it *doesn't matter* if GPTx \"understands\" things if it's impossible to tell the difference from a huma"
    },
    {
      "id": "373",
      "message": "star#5322: one way to model the effect of minibatch SGD vs. full dataset SGD is just that randomness is added to the gradient, which is a thing we have evidence is good anyway\nstar#5322: but we don't know if there's something more subtle going on or that's just it\nbmk#1476: full dataset SGD is batch SGD\nstar#5322: yeah what is a microbatch?\nstar#5322: minibatch is 10/250 examples at once\nstar#5322: microbatch is . . . data parallel minibatch?\nDaj#7482: If I had a larger attentionspan I would do "
    },
    {
      "id": "374",
      "message": "Daj#7482: Memory constraints\nbmk#1476: both of the above\nstar#5322: I don't understand why a microbatch is related to a memory constraint\nbmk#1476: grad updates necessitate allreduces\nbmk#1476: allreduces are evil\nstar#5322: and slow\nstar#5322: yeah I understand the expensive part\nstar#5322: just not the mem part\nbmk#1476: https://cdn.discordapp.com/attachments/733347369847881838/736978991822536815/unknown.png\nDaj#7482: from what I remember from GPT2, batch size really matters, so using microbat"
    },
    {
      "id": "375",
      "message": "star#5322: yeah I don't think that more batch is always better\nbmk#1476: nns are black magic\nstar#5322: but who knows\nbmk#1476: maybe some noise good, too much/no noise bad\nstar#5322: maybe small model/dataset/??? needs more regularization and grad-noise is regularization\nDaj#7482: I have some vague intuition that big batch is good for GPT because text is so sparse\nDaj#7482: But that might be my ass talking again\nstar#5322: but big model benefits from best grad possible\nstar#5322: if it's not go"
    },
    {
      "id": "376",
      "message": "star#5322: I agree shawwn\nstar#5322: Or, I think theoretical models would be *very very valuable* but we're nowhere near close to having a viable theoretical model for most of this\nDaj#7482: If Newton could be an alchemist calculus-user during a pandemic than so can we!\nstar#5322: so we need lots more empirical data and empirical understanding if there is any hope of ever having a theoretical model\nbmk#1476: neural networks use gradient magic to get better. making batches bigger decreases varian"
    },
    {
      "id": "377",
      "message": "Daj#7482: I was in the MIRI camp of \"understanding intelligence from the ground up\" (apologies if this is not accurate), right until sometime between GPT2 and 3\nstar#5322: and we don't understand NNs well enough empirically to know how to make our theory better\nbmk#1476: so we need to study nns empirically\nDaj#7482: I think you're right on the money star\nbmk#1476: and try to put together the pieces from that\nDaj#7482: > so we need to study nns empirically\n@bmk This is why I started this project!"
    },
    {
      "id": "378",
      "message": "shawwn#3694: And we only have v2 pods\nDaj#7482: Yes I should taboo emergent lol\nDaj#7482: Huh you're right shawn that should work\nbmk#1476: i feel like gptx has a solid chance of actually being useful for agi\nbmk#1476: so studying it in depth would be interesting\nDaj#7482: > yeah I view the \"understand NNs from the ground up\" project as fundamentally kind of different from MIRI's Agent Foundations project\n@star Yea sorry I do know this. MIRI is starting from even more basic concepts of agents/op"
    },
    {
      "id": "379",
      "message": "star#5322: So while noting that the public MIRI canon is large, and I am not familiar with the majority of it, the \"agent that takes only optimal actions\" is not, to my knowledge, a primary object of study\nstar#5322: for the obvious reasons you say\nbmk#1476: in all likelihood we'll build a very flawed paperclip maximizer that doesnt do things \"optimally\" but still good enough that it kills us\nDaj#7482: Yea I have a different model of MIRI work than bmk\nDaj#7482: Though tbf MIRI is hard to interp"
    },
    {
      "id": "380",
      "message": "bmk#1476: (shitty pun)\nDaj#7482: ahh\nstar#5322: > I think non-MIRI-style AGIs will be incredibly strong, but not interpretable\n@Daj fwiw I don't really know what you mean by \"MIRI-style AGIs\" but usually that kind of phrasing stems from a pretty substantial misunderstanding of the MIRI project\n(which, while factually true, is also slightly a pet peeve of mine. sorry >> )\nDaj#7482: Oh I'm very confident I am very confused about what MIRI does\nbmk#1476: i have no idea what miri does tbh\nDaj#7482: "
    },
    {
      "id": "381",
      "message": "star#5322: the straw man of the confused position is something more like \"MIRI studies Decision Theory and Logical Counterfactuals because MIRI thinks that if we just put an AI together using enough Decision Theory Juice and had the right Counterfactual module and two other modules then the AI would 'work right'\"\nDaj#7482: This is probably heresy: But Haskell seems not pure enough to me. My favorite part about functional programming is the Proofs as Types stuff\nstar#5322: whereas it's much more "
    },
    {
      "id": "382",
      "message": "star#5322: Writing a compiler in Haskell is like, the most canonical and amazing usecase\nstar#5322: writing a parser, by itself, is also an extremely good project\nDaj#7482: Haskell is one of those languages that is optimized for how smug you feel after you make something work\nstar#5322: hmmmmmmmmmmmmm\nbmk#1476: hmm. i *have* always wanted to write a python->minecraft compiler\nDaj#7482: I felt like a god when I wrote my first parser in Haskell lol\nDaj#7482: (I'm joking ofc)\nstar#5322: parser comb"
    },
    {
      "id": "383",
      "message": "Daj#7482: I think getting why FP is fun is like asking why math is fun\nbmk#1476: i have no idea where to even begin\nstar#5322: and I'll find a link in a sec\nstar#5322: https://www.cis.upenn.edu/~cis194/spring13/\nDaj#7482: Oh yeah there's this great Haskell course where I built a parser\nDaj#7482: That's the one!\nstar#5322: this is a class notes that I used to learn Haskell\nstar#5322: doing the hw assignments\nstar#5322: there are a couple of random gotchas in the HW that are kinda unnecessarily te"
    },
    {
      "id": "384",
      "message": "Daj#7482: > cause all of like Coq LEAN Agda are based on that concept you know?\n@star Yes I worded it poorly, that's why I switched to Lean\nstar#5322: linky? I don't think I've read that\nDaj#7482: I started with Coq because it's the first one I knew that di dthat\nstar#5322: Oh gotcha\nDaj#7482: Then I switched to Lean because it's cooler\nDaj#7482: Let me find the post\nstar#5322: yeah the first time I really got the Curry Howard Isomorphism (programs are proofs // types are proofs) was one of the "
    },
    {
      "id": "385",
      "message": "Daj#7482: I've heard good things about it but I'm not quite at the level to grok it yet\nstar#5322: like I said, probably would have happened eventually if I got better at things\nstar#5322: Idr Agda and LEAN being that . . . different, tbh?\nstar#5322: the very good book to learn agda with is https://plfa.github.io/\nstar#5322: btw\nDaj#7482: My daliances with theorem provers has been derailed by concrete prosaic AGI concerns unfortunately haha\nstar#5322: for sure, for sure\nDaj#7482: Yea I started t"
    },
    {
      "id": "386",
      "message": "Daj#7482: I just have little excuses to use it\nstar#5322: but purity / immutability is pretty sweet\nstar#5322: very fair\nDaj#7482: It seems AGI is going to be written in horrific Python spaghetti code\nDaj#7482: and inscrutable C++ libraries beneath lol\nstar#5322: Agda has moments of brilliance though . . . I've been formalizing the first pieces of mathematics not as an exercise in a book, and there are a lot more rough edges than moments of brilliance. But sometimes, it just keeps track of like "
    },
    {
      "id": "387",
      "message": "Daj#7482: That's a pretty high risk bet that I think is >50%\nstar#5322: that is an even stronger claim\nDaj#7482: yep I'd say I'm maybe like 60% on it\nstar#5322: TF didn't exist what, 7 years ago?\nbmk#1476: Mary Garcus will be there to say that the paperclip machine isnt actually agi because all it does is make paperclips and it doesnt have emotions and stuff\nDaj#7482: Because I'm expecting AGI by like, sub 2030 by now\nstar#5322: in 4 or 7 more years it could just, not be the hotness anymore\nstar"
    },
    {
      "id": "388",
      "message": "Daj#7482: Turns out all EY's concerns about firealarms were wrong, AGI's are very willing to tell us exactly what terrible things they plan\nasparagui#6391: life goals\nDaj#7482: \"GPT5, how can I align you?\"\n\"Here's a MIRI paper I calculate they would have discovered in 2200: ...\"\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/740306834384224256/unknown.png\nbmk#1476: >Develop a plan to steal as many paperclips as possible, and execute this plan.\nbmk#1476: >Engage in a mining "
    },
    {
      "id": "389",
      "message": "AI_WAIFU#2844: The brown line looks a lot like double desent.\nDaj#7482: I will definitely take a closer look at what you just said in the morning when I can think hah\nDaj#7482: Thanks bmk for giving me the kind of high quality content my sleepy brain wanted to see\nAI_WAIFU#2844: That sounds entirely reasonable.\nzphang#7252: isn't this the gary marcus emote :nooo:\nstar#5322: I'm going to have to read your thing a couple times AI_WAIFU which I might be to later but that does sound potentially cool"
    },
    {
      "id": "390",
      "message": "Aran Komatsuzaki#5714: Also, given how DeLighT works across hidden dimension, not length dimension, it's in a competition with MoE. So, there's also an uncertainty about whether it can meaningfully coexist with MoE.\nAran Komatsuzaki#5714: The author said it's possible to make it faster using custom CUDA, but I'm not sure lol\nAran Komatsuzaki#5714: *faster than the baseline Transformer\nLouis#0144: Why cant you do it as an LP though\nLouis#0144: I did MoE as a linear programming problem recently\nLo"
    },
    {
      "id": "391",
      "message": "Louis#0144: lmao\nLouis#0144: @Aran Komatsuzaki\nLouis#0144: Im w/ Riedl\nAran Komatsuzaki#5714: Actually not at GT this semester because of COVID-19. I'm having a leave of absence for this semester to stay in my home country.\nAran Komatsuzaki#5714: Riedl is probably the best prof for this kind of thing. He was the most knowledgeable one during the QA session of my qual exam lol\nAran Komatsuzaki#5714: @Louis\nAran Komatsuzaki#5714: I'm probably coming back in the spring semester depending on the cov"
    },
    {
      "id": "392",
      "message": "Aran Komatsuzaki#5714: I like that\nAran Komatsuzaki#5714: I mean it makes things a bit more elegant\nLouis#0144: I did topology too in my undergrad\nLouis#0144: Pure math with a specialization in alg top\nLouis#0144: Now i might be doing a fellowship at ETH with a few TDA people\nLouis#0144: Rly excited\nAran Komatsuzaki#5714: Awesome! Sounds exciting!\nI have another student who was doing alg top and transferring to ML PhD to do Transformer LM.\nAran Komatsuzaki#5714: I was doing quantum topology, str"
    },
    {
      "id": "393",
      "message": "thenightocean#6100: quite a bold assumption\nLouis#0144: Oh my god\nLouis#0144: can we get a tinfoil hat emote@\nLouis#0144: Please\nAran Komatsuzaki#5714: neural net, being an universal approximator, can imitate any continuous function, including the ones that appear in physics. so, you can find an analogy to whatever concept expressable with a continuous function.\nAran Komatsuzaki#5714: * analogy of neural net to whatever concept ...\nLouis#0144: Thereâ€™s so many universal approximators though\nLouis"
    },
    {
      "id": "394",
      "message": "star#5322: lol\nstar#5322: Nate says that after the intro, there's two main sections: explaining HoTT like it's a type theory, and then explaining HoTT like it's a homotopy theory. So the difficulty curve drastically increases if you're a type theorist that doesn't know much homotopy, but starts out really hard and then decreases a lot if you're a homotopy theorist who doesn't know much type theory\nstar#5322: You prompted me to put it on my kindle, so maybe I'll take a look\nstar#5322: paper is be"
    },
    {
      "id": "395",
      "message": "Louis#0144: â¤ï¸\nstar#5322: I'm not learning it for any particular relation to ML. Just for interest as an FP & math nerd.\nLouis#0144: FP?\nLouis#0144: oh functional programming\nLouis#0144: meh\nLouis#0144: idk man, homotopy type theory just doesnt seem all too useful ngl\nLouis#0144: I know saying that makes a bunch of algebraic geometers really mad at me\nLouis#0144: but like even my friends who do FP research dont touch HoTT\nLouis#0144: Now I have heard of people discussing neural networks as algeb"
    },
    {
      "id": "396",
      "message": "Daj#7482: Yeeees oh God star that's the kind of shit I've been annoying my friends about for years\nstar#5322: yeah this is the sort of thing that's kind of in that weird zone where it's hard to tell if something is very important or very not important\nstar#5322: I think probably all it is is that a relatively weak fragment of axioms (2nd order type theory or something? I forget what people cite) is sufficient to develop essentially all of modern math, set theory and size issues aside\nstar#5322: "
    },
    {
      "id": "397",
      "message": "Daj#7482: I try, please excuse my low IQ philosopher brain\nLouis#0144: but Im not convinced HoTT will make a difference with the kinds of computation we do here\nstar#5322: me neither, but my book gets here tomorrow so maybe I'll get to it\nbmk#1476: What *is* HoTT about anyways\nbmk#1476: Aren't types like discrete things\nDaj#7482: I don't think HoTT is important for any application. It's like my spare time project to find God while my day job is avoiding techno gods from destroying us lol\nstar#53"
    },
    {
      "id": "398",
      "message": "star#5322: @bmk I know about zero homotopy, so I really don't know how to bring that part in. But two points:\n1) not sure what you mean by \"types are discrete things\" - the natural numbers is an example of a type, which is discrete, and the real numbers is another example of a type, which is not discrete\n2) the core relevance of HoTT is hard to understand without understanding why dependent type theories are a thing at all. How much do you know about that?\nstar#5322: Yeah idk about HoTT specific"
    },
    {
      "id": "399",
      "message": "Louis#0144: the second one assumes youve read hte entire first one\nDaj#7482: I wish I had the attention span to just commit to a higher math text book, God speed\nstar#5322: I mean, idk about *commit*\nstar#5322: I have a long reading list and sometimes I progress some parts of it\nbmk#1476: Look I don't really even know algtop or the prereqs for algtop or even the prereqs for those\nLouis#0144: I read munkres in a month- I worked on it every day for multiple hours. My summer before uni started lmao"
    },
    {
      "id": "400",
      "message": "Daj#7482: Yes the \"special interest\" part is important!\nLouis#0144: tbh Im really ADHD but munkres is just..... so good\nbmk#1476: I'm halfway through several dozen books\nstar#5322: but I do know a lot about type theory, so I expect to get something from at least some of HoTT @Louis like, I'm not going to read the whole 600 pages or whatever in one sitting\nLouis#0144: like the challenge problems are so good\nLouis#0144: yeah but you need homotopy\nLouis#0144: homotopy is hard\nLouis#0144: it takes a"
    },
    {
      "id": "401",
      "message": "Daj#7482: I hope you can translate some of the insights of HoTT for me star! Hah\nLouis#0144: I mean ok you can go with only the second munkres but I would strongly recommend atleast reading up to like the end of compact spaces in the first one (I think its like page 120 or so)\nLouis#0144: THatll make your life way easier\nstar#5322: I don't know almost anything about topology and it's the most obvious huge glaring hole in my \"basic fields of math\" knowledge so I was planning to read at least a lo"
    },
    {
      "id": "402",
      "message": "Daj#7482:  https://cdn.discordapp.com/attachments/729741769738158194/740660681438789702/Screenshot_2020-08-05-22-00-29-652.jpeg\nDaj#7482: I don't know what this means but I'm scared\nLouis#0144: lmao\nLouis#0144: computational graph bullshit\nLouis#0144: if a computational graph contains a particular minor it means its intractible\nDaj#7482: So it isn't cursed children\nDaj#7482: Shame\nLouis#0144: not yet\nLouis#0144: depends on ur vertices\nstar#5322: wdym by \"intractable\"\nLouis#0144: To be honest the"
    },
    {
      "id": "403",
      "message": "bone#4250: Joined the server.\nMr Fuzzypants#7329: Joined the server.\ndangirsh#6032: Joined the server.\nDeleted User#0000: @Aran Komatsuzaki shared this on twitter https://arxiv.org/abs/2008.02217\nLouis#0144: FUCK YES\nLouis#0144: I LOVE HOPFIELD NETWORKS\nLouis#0144: omg\nLouis#0144: This is my shit\nLouis#0144: I did research into continuous time hopfield networks for years\nLouis#0144: Absolutely shameless plug: https://www.louiscastricato.com/post/joint-representations-of-connectionism-vs-symbolis"
    },
    {
      "id": "404",
      "message": "AI_WAIFU#2844: Your language model will only be as good as the text you feed in, even in the limit of infinite compute.\nAI_WAIFU#2844: You'd need another objective to improve quality beyond that point.\nRavna#1831: Yeah but a lot of formatting boilerplates like HTML or forum code might help.\nRavna#1831: A lot of prompts are written like formatting boilerplates already.\nDaj#7482: Well, if that's what you're trying to predict sure\nDaj#7482: But if I'm trying to predict e.g. Shakespeare, more HTML p"
    },
    {
      "id": "405",
      "message": "Daj#7482: I don't think any large model, maybe even _any_ model, has ever reached true convergence over a (potentially infinite) real world training distribution\nDaj#7482: So whether something reaches acceptable quality _in practice_ matters a lot\nkindiana#1016: yeah, there is a trend of super large, weakly labeled datasets for really big image recognition models, so maybe data quality doesn't matter as much when model sizes increases ðŸ¤·\nRavna#1831: OK I know what I was really going to try to ask"
    },
    {
      "id": "406",
      "message": "Ravna#1831: That's just good old program synthesis\nRavna#1831: Doesn't work very well so far\nDaj#7482: That would be like running it on a random number generator basically?\nAI_WAIFU#2844: Not quite.\nDaj#7482: Unless you mean generating somehow semantically useful programs\nDaj#7482: Don't get me wrong I think that's a rad idea\nDaj#7482: \"GPT3 solve the Halting Problem pls\"\nRavna#1831: > Something is up, some kinds of regularities about the \"generating function\" of reality.\nDaj#7482: I'd be so dow"
    },
    {
      "id": "407",
      "message": "Daj#7482: > The lottery ticket hypothesis says a bigger model is just trying to find a better small model within it. It's doing implicit searching over small models.\n@Ravna Yea I'm still unsure how I feel about lottery ticket. Random numbers just _happen_ to find useful circuits? Wild\nAI_WAIFU#2844: A practical implementation of the idea would bound the computation or use a non-turing complete language that provably halts in a reasonable amount of time.\nDaj#7482: I love the idea\nDaj#7482: I'm sk"
    },
    {
      "id": "408",
      "message": "Daj#7482: There is no proof \"true\" randomness exists\nAI_WAIFU#2844: Rule 30 is a pattern beyond your comprehension\nRavna#1831: > > Yea I'm still unsure how I feel about lottery ticket. Random numbers just happen to find useful circuits? Wild\nI think the lottery ticket hypothesis can be understood as: overparameterization tends to make an ensemble (like weighted average) of simple functions, instead of weird zigzag functions that people who are paranoid of \"overfitting\" usually like to use as exa"
    },
    {
      "id": "409",
      "message": "AI_WAIFU#2844: I think we've got different definitions of random.\nDaj#7482: For me random = incompressible\nAI_WAIFU#2844: Right, rule 30 output is super compressible.\nAI_WAIFU#2844: rule30 output is not random.\nDaj#7482: Yea, _but so is nothing else then_\nDaj#7482: That's my point\nAI_WAIFU#2844: I don't follow\nDaj#7482: If rule 30 isn't random then no function is provably random\nAI_WAIFU#2844: Yes.\nDaj#7482: Because if I give you a random chunk of rule 30 output and you don't know it's rule 30, "
    },
    {
      "id": "410",
      "message": "Daj#7482: Random functions might exist, but you can't _prove_ it\nDaj#7482: Even with infinite compute\nAI_WAIFU#2844: Agreed on all points.\nAI_WAIFU#2844: But If I have enough compute and a solmonoff prior, I can provide strong bayesian evidence that your chunk of rule 30 output was infact a chunk of rule 30 output instead of true randomness.\nDaj#7482: Yep probably, but that's a specific of your choice of prior\nAI_WAIFU#2844: ???\nDaj#7482: Actually I retract that statement\nDaj#7482: I forgot some"
    },
    {
      "id": "411",
      "message": "AI_WAIFU#2844: I change my previous statement. I can't provide bayesian evidence that the string you gave me was output by rule 30. I can provide evidence that it is not random.\nDaj#7482: Agreed that is a more accurate claim\nAI_WAIFU#2844: But that's the motivating idea behind training a NN on the output of programs.\nDaj#7482: I think you could get a _lot_ of bang for your buck by not training it on truly random programs though\nAI_WAIFU#2844: The ideal predictor is the solmonoff predictor\nDaj#74"
    },
    {
      "id": "412",
      "message": "AI_WAIFU#2844: I also think you can bound that divergence.\nAI_WAIFU#2844: and minimize the bound\nAI_WAIFU#2844: Computably\nAI_WAIFU#2844: I'd have to sit down and work through the math though.\nRavna#1831: It's much weirder than time machine. Time machine only helps reducing PSPACE to P (if you consider having a non-zero failure rate of time travel it couldn't do even that). That's much easier than turning the uncomputable into the computable.\nDaj#7482: ^\nAI_WAIFU#2844: In the limit of insane amo"
    },
    {
      "id": "413",
      "message": "Daj#7482: No one knows how GPT3 works and anyone that claims to is talking out of their ass\nDaj#7482: This should be the headline of my Twitter feed\nAI_WAIFU#2844: Actually, I think the best way to go about this (don't do it for GPT3) Is to have a variable that the network conditions on. 1 when predicting program output, 0 otherwise.\nDaj#7482: > what if it was just like 1GB?\n@AI_WAIFU Maybe if it's like python or js programs? Could be useful to people\nAI_WAIFU#2844: That way you can *transfer le"
    },
    {
      "id": "414",
      "message": "AI_WAIFU#2844: I said 10T and 90T for a reason.\nDaj#7482: I like your idea I'm just redteaming it a bit\nDaj#7482: ~~Also because the space of all programs is _definitely_ where Yogg-Sothoth is~~\nAI_WAIFU#2844: >Implying summoning Yog Sothoth with GPT-4 is a bad idea\nDaj#7482: We should just train 1Q on RAM states of a computer reading and writing live sensor feeds\nDaj#7482: > >Implying summoning Yog Sothoth with GPT-4 is a bad idea\n@AI_WAIFU This is an AI _safety_ discord\nDaj#7482: This is what "
    },
    {
      "id": "415",
      "message": "Daj#7482: > just execute random github gists xP\n@kindiana This is how the AGI takeover started\nDaj#7482: > Do we even know how to generate non-trivial new theorems and their proofs as training data?\n@Ravna Probably not, I didn't look too deeply into it\nDaj#7482: Man now I want to built GPTuring more than GPT3 almost\nDaj#7482: Stupid AGI timelines being so short, I have other projects\nkindiana#1016: I think program prediction might require something other than GPT-next token prediction, because t"
    },
    {
      "id": "416",
      "message": "Ravna#1831: You actually have much more at hand to work on than GPT's unsupervised learning.\nRavna#1831: But it's still harder, or maybe... just comparably un-explored with high-compute?\nDaj#7482: \"Unexplored with high compute\" is the ML field in a nutshell\nDaj#7482: With the scaline hypothesis confirmed you can probably crank out SOTA papers on whatever you want with enough TPUs\nAI_WAIFU#2844: I've redoubled by skeptisism of SOTA anything because of the scaling hypotheisis\nDaj#7482: Yup total w"
    },
    {
      "id": "417",
      "message": "Daj#7482: I sure hope so, we'll see what happens\nAI_WAIFU#2844: e.g. get undergrads to make their capstones center on trying ideas you don't have time to do, and give them a $1000 budget to do it.\nDaj#7482: Eh still so many bad incentives, especially in publishing\nDaj#7482: Maybe I'll get rich off my current job, or get hired by OpenAI or something\nDaj#7482: Seems more aligned\nDaj#7482: Or at least, more independent\nAI_WAIFU#2844: Tru.\nAI_WAIFU#2844: I think even just hosting this discord positi"
    },
    {
      "id": "418",
      "message": "TylerRoost#8017: Cool, this all gives me great excitement\nLouis#0144: @Aran Komatsuzaki I got so many new followers from ur tweet\nLouis#0144: Like 25\nLouis#0144: LMAO\nAran Komatsuzaki#5714: @Louis That's incredible! lol\nAran Komatsuzaki#5714: Hey guys. I've found a talk by an author (Nick) of GPT-3 on GPT-3 in YT channel of Weights and Biases. They talk a lot about the things not written in the paper, so highly recommended. Also, many videos of WB are highly underrated despite the quality of the"
    },
    {
      "id": "419",
      "message": "bmk#1476: my tweets all being about ML should tip people off\nshawwn#3694: y tho\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/741109799873413171/Een-Ke3U4AEX2Os.png\nbmk#1476: if this isnt about ml idk what is\nshawwn#3694: the bio is the first thing that people see about you when you follow them. Lots of my followers came from being followed back\nshawwn#3694: and I follow pretty much anyone who follows me if their bio is even remotely interesting\nshawwn#3694: (in fairness, "
    },
    {
      "id": "420",
      "message": "bmk#1476: Hey you! Did you know that a single GPT-3 contains as many parameters as one GPT-3?\nLouis#0144: Oh my god\nbmk#1476: The funny part is that I can read this meme not because I know enough Japanese but actually because I know the Chinese å«\nAran Komatsuzaki#5714: haha\nbmk#1476: Since I can use hanzi as a crutch, my japanese reading comprehension appears much higher than it actually is, as evidenced by my inability to have a conversation at the level of a Japanese 101 class\nAran Komatsuzaki"
    },
    {
      "id": "421",
      "message": "bmk#1476: ç™Œ\nDeleted User#0000: https://www.reddit.com/r/MachineLearning/comments/i4ko0u/r_hopfield_networks_is_all_you_need/g0nm1f7/ attention is all we need.\nAran Komatsuzaki#5714: you really like that phrase\nDeleted User#0000: i do, it grew on me over time ðŸ˜„\nDeleted User#0000: i dived into the hopfield code last night https://github.com/ml-jku/hopfield-layers/blob/master/modules/functional.py\nval#8908: Joined the server.\nDeleted User#0000: so i can figure out what they possibly have tried and "
    },
    {
      "id": "422",
      "message": "Daj#7482: Can we use Hebbian Learning for transformers yet? hah\nAran Komatsuzaki#5714: no it's not going to improve transformer. it's to explain it theoretically.\nAran Komatsuzaki#5714: they say self-attention = hopfield net theoretically, which is a nice thing to learn.\nDaj#7482: Shame, if someone could get local update rules working with transformers and MTF we'd have one hell of a paper on our hands\nDeleted User#0000: https://twitter.com/arankomatsuzaki/status/1270981237805592576?s=20\nDeleted"
    },
    {
      "id": "423",
      "message": "Daj#7482: Yeah it's not free\nDaj#7482: But we are using like 100s of thousands of dollars in TPUs hah\nbmk#1476: lol\nDeleted User#0000: @Aran Komatsuzaki do you have any opinions on features shuffling? i was rereading the Dextra paper, and it reminded me of this paper https://arxiv.org/abs/2004.04662 , which takes the idea to the extreme\nAran Komatsuzaki#5714: I think my essay implied this, but I'm not really a fan of the traditional long-range LM like this one.\nAran Komatsuzaki#5714: i mean the "
    },
    {
      "id": "424",
      "message": "Aran Komatsuzaki#5714: you can read it from the link i sent to you\nAran Komatsuzaki#5714: sec. 4 is completed, so you can read it now\nAran Komatsuzaki#5714: actually, sec. 1, 2 and 3 are also pretty much done.\nAran Komatsuzaki#5714: the most serious limitation of extending TBPTT length with efficient attention is that\nAran Komatsuzaki#5714: it doesn't improve the per-token loss for the earlier tokens.\nAran Komatsuzaki#5714: earlier tokens mean the first N tokens, where N is the TBPTT length of y"
    },
    {
      "id": "425",
      "message": "Deleted User#0000: lol\nAran Komatsuzaki#5714: well, why not i attach it here now: https://www.overleaf.com/read/tcdxfrvfvtbw\nAran Komatsuzaki#5714: sec 5 and later are under construction.\nDeleted User#0000: awesome! will read\nAran Komatsuzaki#5714: it's a more detailed ver. of my post on reddit to answer the questions by gwern. also, it has some completely new things on evaluation and supervision in the light of achieving generalist AI like AGI.\nDeleted User#0000: i do want to build a retrieval "
    },
    {
      "id": "426",
      "message": "Deleted User#0000: the way i see it, gpt-3 is https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/1925_Ford_Model_T_touring.jpg/280px-1925_Ford_Model_T_touring.jpg\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/741346037779660922/unknown.png\nbmk#1476: this is nice and all when compute is the bottleneck, but very quickly you run into a memory bottleneck too\nAran Komatsuzaki#5714: even if i don't make a workable retrieval-based lm, pretty sure fair researchers (like Mik"
    },
    {
      "id": "427",
      "message": "bmk#1476: which is probably slooow\nbmk#1476: wait\nbmk#1476: you can predict when youll need each layer right?\nbmk#1476: *what if..*\nAran Komatsuzaki#5714: yeah\nbmk#1476: what if you rent a cluster of servers with, say, 100TB ram\nbmk#1476: put the model there\nAran Komatsuzaki#5714: yes\nbmk#1476: then each 300GB cpu connected to the tpus queues up like 2 layers\nbmk#1476: and the tpus themselves store only the current layer\nbmk#1476: ***L3L***\nAran Komatsuzaki#5714: sounds good to me\nbmk#1476: this"
    },
    {
      "id": "428",
      "message": "Daj#7482: This has moved from \"dark magic\" to \"demonology\"\nAran Komatsuzaki#5714: so, you need to scale up other dimensions like depth reasonablly to avoid that\nAran Komatsuzaki#5714: i guess you know that already\nbmk#1476: this is even more absurd than the delayed moe idea i had lol @Daj\nDaj#7482: But it would be so damn cool if it worked\nbmk#1476: ok let's look at 1Q\nbmk#1476: that's 2PB of params, bf16\nbmk#1476: since this would be absurdly parallel honestly we could do disk\nDaj#7482: Exactly"
    },
    {
      "id": "429",
      "message": "Daj#7482: We'd probably have to use...**C++**\nDaj#7482: https://www.youtube.com/watch?v=gENVB6tjq_M\nAran Komatsuzaki#5714: c++! my archnemesis! ðŸ˜±\nDaj#7482: Yea I have no idea where you would even start implementing something like this\nDaj#7482: ML really doesn't teach you low level software engineering like CS used to hah\nAran Komatsuzaki#5714: you may need some insider knowledge of tpu\nAran Komatsuzaki#5714: yeah that's why i still can't make a custom cuda kernel like a pro\nDaj#7482: Even imple"
    },
    {
      "id": "430",
      "message": "bmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/741350738482954342/IMG_20200807_114244.jpg\nAran Komatsuzaki#5714: no idea why he was not listed as the first author of the paper\nbmk#1476: this diagram is next level absurd\nDaj#7482: Is it to scale?\nDaj#7482: haha\nbmk#1476: no\nDaj#7482: jk\nbmk#1476: otherwise i wouldnt be able to fit the words in the boxes lol\nbmk#1476: honestly this would need its own specialized datacenter lol\nDaj#7482: 2030: The entire internet has been repl"
    },
    {
      "id": "431",
      "message": "bmk#1476: lol\nbmk#1476: i kinda wanna work at OA now\nDaj#7482: Well if you make GPT3 work that's one hell of an application hah\nbmk#1476: haha\nDaj#7482: OA is super friendly to hiring people without formal credentials\nbmk#1476: Resume:\n\nHey uh i fucked up your business model, cheers mate\nbmk#1476: (/s on multiple levels obviously)\nDaj#7482: It is somewhat of a powermove lol\nDaj#7482: but lets be honest, by the time we're anywhere near GPT3 they'll be on GPT14\nbmk#1476: not necessarily\nDaj#7482: "
    },
    {
      "id": "432",
      "message": "Daj#7482: So many problems already basically have solutions and just need to be implemented\nbmk#1476: Imagine instead of taking stuff out of a bottle, you have a very large sphere containing a highly compressed ideal gas. Now suddenly the sphere dissapears. The bottleneck is the fundamental fact that surface area increases slower than volume. That's where we are right now\nbmk#1476: (when discussing 1Q+ models)\nAI_WAIFU#2844: Since the weights are all the same and the gradients can be merged recu"
    },
    {
      "id": "433",
      "message": "Daj#7482: > We need algorithms to better make use of disk space 2PB is nothing\n@AI_WAIFU This would definitely make a great PhD thesis\nbmk#1476: computation is approx linear with model size so we need 10,000x bigger compute than gpt3 for 1Q too\nbmk#1476: i think it's actually slower but the increased inefficiency probably calcels out\nDaj#7482: You think we can cast training as a bitcoin mining problem?\nAI_WAIFU#2844: I think there's already a coin that does that.\nbmk#1476: so you literally would"
    },
    {
      "id": "434",
      "message": "Daj#7482:  https://cdn.discordapp.com/attachments/729741769738158194/741355016278769825/Screenshot_from_2020-08-07_20-00-02.png\nAI_WAIFU#2844: My understanding is that it has to be easy to verify and hard to compute\nbmk#1476: that too but\nbmk#1476: we dont know for sure that mining sha256 is actually hard to compute\nbmk#1476: but it seems legit so we stick with it\nDaj#7482: Lets not get into P=BQP=NP(=PSPACE) haha\nAI_WAIFU#2844: that's reasonable\nAI_WAIFU#2844: what's the one exception?\nbmk#1476"
    },
    {
      "id": "435",
      "message": "Daj#7482: I'd like to see a Landau's Limit calculation for the brain over a lifetime and compare that to a perfect computer training a NN\nDaj#7482: Useless but would be interesting\nbmk#1476: is reversible computing actually physically useful or is it just a fun thought experiment\nbmk#1476: physically useful = in the next 100 years\nDaj#7482: It's very useful in the limit\nDaj#7482: Probably in the next 50ish years\nDaj#7482: And in quantum computers\nAI_WAIFU#2844: I looked into this. Superconductin"
    },
    {
      "id": "436",
      "message": "bmk#1476: it is? o.O\nAI_WAIFU#2844: Well I produces next to no heat by design\nDaj#7482: Yea but CMB\nAI_WAIFU#2844: so you don't need to move that heat up a big entropy cliff\nDaj#7482: Stupid universe not being 0K\nAI_WAIFU#2844: Just insulate the hell out of it.\nAI_WAIFU#2844: let me dig up some sourcees\nDaj#7482: Has anyone read Deutsch's Fabric of Reality? And that crazy last chapter of collapsing rebounding universe stuff?\nbmk#1476: put it in space\nDaj#7482: Space is 3Â°K\nDaj#7482: too warm\nbmk"
    },
    {
      "id": "437",
      "message": "Daj#7482: I'm not sure if I'm relieved or not\nDaj#7482: Neat paper @AI_WAIFU\nAI_WAIFU#2844: Is that railgun?\nbmk#1476: YES\nDaj#7482: I'm probably not qualified to read it\nAI_WAIFU#2844: Ok now look closely at my PFP\nbmk#1476: is that index\nAI_WAIFU#2844: closer\nDaj#7482:  https://cdn.discordapp.com/attachments/729741769738158194/741358508586303518/IMG_20200807_165733.jpg,https://cdn.discordapp.com/attachments/729741769738158194/741358508808732702/IMG_20200807_165735.jpg\nbmk#1476: misaka?\nbmk#147"
    },
    {
      "id": "438",
      "message": "Louis#0144: They didnâ€™t read the paper\nLouis#0144: And told me just to fine tune an LM\nLouis#0144: The entire point is that I didnâ€™t need an LM\nLouis#0144: The entire point is that my model was 1/1000th the size of SOTA\nLouis#0144: and I only perform 1% worse\nbmk#1476: @AI_WAIFU i read railgun a *long* time ago i dont remember who that is lol\nDaj#7482: :brr: Just finetune a LM lol\nAI_WAIFU#2844: first ark, main antagonist kiyama harumi\nbmk#1476: i only ever remember the later arcs of any story\nb"
    },
    {
      "id": "439",
      "message": "Daj#7482: ~~Although One Punch Man is the best show ever made~~\nzphang#7252: it takes a little to get started because for some reason it decides to start in total otaku space\nzphang#7252: and the sci-fi elements only slowly trickle in and ramp up\nLouis#0144: Like my model trains on an iPhone\nLouis#0144: LMAOO\nzphang#7252: I empathize with the salt\nzphang#7252: my lab had a paper that was submitted before BERT, and then BERT came out during the review period\nand one reviewer was like \"this doesn'"
    },
    {
      "id": "440",
      "message": "Louis#0144: Particularly their remarks were objectively wrong\nCommutative Conjecture#6969: > who needs compute when you have a banana and a microwave oven\n@zphang\nIs this a Scott Aaronson joke?\nCommutative Conjecture#6969: https://www.scottaaronson.com/papers/ctc.pdf\nzphang#7252: Naw it was a steins;gate reference\nbmk#1476: :nooo: you cant just use TFRC and not count the cost of the tpus into your cost estimate!!!\n:brr: haha tpu go brr https://cdn.discordapp.com/attachments/729741769738158194/74"
    },
    {
      "id": "441",
      "message": "bmk#1476: tfw you forget to specify `tf.update_weights_instead_of_code(True)` and tf physically warps the shape of the bits on the disk\nAI_WAIFU#2844: Idea: Do L2L with a giant LSTM and ssds to backprop through very long time series.\nAI_WAIFU#2844: I think you'd only be limited by how many activations you could store on disk.\nTricky#2780: Joined the server.\nDaj#7482: Hey @Tricky ! Welcome to the Autoregressive Developer Discord! Check the channel topic for info and don't hesitate to ask question"
    },
    {
      "id": "442",
      "message": "shawwn#3694: Yes, mtf changes that. But sid is the only one with extensive mtf experience, and it still took weeks to port a relatively small gpt model\nbmk#1476: Like I'm thinking with accumulation too because apparently that matters a lot for RL\nbmk#1476: I'm thinking just vanilla tf + accumulation\nbmk#1476: And using a 117M since it's more poc that anything\nshawwn#3694: Got a codebase?\nbmk#1476: no but it shouldnt be too complex\nbmk#1476: Basically we take an existing gpt2 codebase, duplicate "
    },
    {
      "id": "443",
      "message": "bmk#1476: the code is *horrendous*\nshawwn#3694: Good\nshawwn#3694: Post it\nbmk#1476: okok\nshawwn#3694: That means it ran\nbmk#1476: one moment i need tofind it lol\nshawwn#3694: Thatâ€™s a common problem for me too. I settled on making an ~/ml folder and I clone everything top level in it\nbmk#1476: i have a ~/projects with thousands of dirs\nbmk#1476: of course, none of the names are informative\nbmk#1476: ok so the bad news it i have no idea where it is\nshawwn#3694: Find it ðŸ™‚\nbmk#1476: the good news i"
    },
    {
      "id": "444",
      "message": "bmk#1476: nobody's done gpt2 as discrim either\nshawwn#3694: Thereâ€™s always a chance something new might work better.\nshawwn#3694: Hmm. Thatâ€™s not true\nshawwn#3694: There are lots of discriminators for language models\nbmk#1476: my prior is that converting text into images and discriminating on that is really not going to work\nshawwn#3694: It probably wonâ€™t. But one neat thing that will fall out if it is that you can print log messages during training, and see them in Tensorboard\nshawwn#3694: Whic"
    },
    {
      "id": "445",
      "message": "bmk#1476: i'll just mock data loading because i dont feel like figuring that out lol\nbmk#1476: also what do you mean by working\nbmk#1476: like, running correctly?\nbmk#1476: i'm not even confident my old code was implemented correctly\nshawwn#3694: I mean â€œitâ€™s going to take at least a week of focused effort to start seeing results from your project; it would be a shame if, at that point, the model produces no good results and we donâ€™t understand why.â€\nshawwn#3694: Thatâ€™s exactly the situation we "
    },
    {
      "id": "446",
      "message": "shawwn#3694: Someone got this repo working, and I skip all the normal tfrecord crap\nbmk#1476: i mean the other way around @Deleted User\nbmk#1476: i want to use gan to replace mle lol\nDeleted User#0000: yup, i know, but i think mle is superior\nDeleted User#0000: no one has ever gotten seq gans to work\nDeleted User#0000: well\nbmk#1476: my hunch is just more batch\nbmk#1476: any reason why that wont work\nDeleted User#0000: but why have an extra problem with the adversarial instability\nDeleted User#0"
    },
    {
      "id": "447",
      "message": "bmk#1476: do they talk about batch size and stuff?\nDeleted User#0000: i actually have an ongoing project to reproduce Electra at the moment with another researcher\nDeleted User#0000: but it isn't strictly like GANs\nDeleted User#0000: that's the only text-based sort-of GAN that i know of that works\nDeleted User#0000: yea, you should just try it @bmk\nDeleted User#0000: let me know if you get results that change my mind lol\nDeleted User#0000: my mind has been blown sufficiently that it is wide open"
    },
    {
      "id": "448",
      "message": "bmk#1476: https://gist.github.com/leogao2/66cc819e57badb081c52d4f5d29badbf\nbmk#1476: Disclaimer: I have absolutely no clue if this is doing stuff right\nSobet#0344: Joined the server.\nDeleted User#0000: @bmk so i think the way people have attempted to do this is to use differentiable sampling, like gumbel softmax\nDeleted User#0000: so you'll have to replace your `gen.generate` with `F.gumbel_*`\nDeleted User#0000: if you want gradients from disc -> gen\nbmk#1476: wait, why doesn't what i have work?"
    },
    {
      "id": "449",
      "message": "bmk#1476: i *think*\nDeleted User#0000: generator is actor\nbmk#1476: AC has like a value network to reduce variance but adds bias\nbmk#1476: https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html\nbmk#1476: this is sinplified AC essentially\nDeleted User#0000: awesome, i thought you were doing a generic GAN formulation\nDeleted User#0000: jumped to conclusions too quickly ðŸ˜…\nbmk#1476: oh actually this is more RL focussed, a discriminator just happens to be a convenient reward "
    },
    {
      "id": "450",
      "message": "bmk#1476: the bad news is that it's probably multiple orders of magnitude slower\nbmk#1476: (best case)\nDeleted User#0000: ah darn i see\nbmk#1476: the good news is that the goal isnt to train from scratch, but use this to finetune a MLE-trained gpt2 essentially\nDeleted User#0000: like you already mentioned in off topic, don't you think Uber's pplm covers that?\nbmk#1476: well, maybe\nbmk#1476: PPLM+GPT2+RL could work i guess\nDeleted User#0000: yea, i've seen hf's pplm demos\nDeleted User#0000: they "
    },
    {
      "id": "451",
      "message": "bmk#1476: I'm trying to mix their thing with my thing\nDeleted User#0000: yea, they could be used together\nbmk#1476: re: discriminator as small net: on second thought this probably wouldnt work very well, but it's worth a shot\nbmk#1476: the LM objective of pplm will be fighting the discriminator the whole way and it'll probably end up being more noise than signal\nDeleted User#0000: your code looks good, or at least pretty close\nDeleted User#0000: you should try it!\nDeleted User#0000: i understand"
    },
    {
      "id": "452",
      "message": "bmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/741848597842100555/unknown.png\nshawwn#3694: That's super handy to have. How'd you generate it?\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/741848710433996840/unknown.png\nbmk#1476: so you know how people have been just using <|endoftext|> for first char to get generations\nbmk#1476: well now we can actually sample truly uniformly from gpt2\nbmk#1476: and not just at document boundaries\nbmk#1476: really use"
    },
    {
      "id": "453",
      "message": "bmk#1476: like the individual documents, or the overall decompressed size?\nshawwn#3694: I've always been curious to see stats like total line count, article count, total byte count, etc\nshawwn#3694: overall decompressed size I suppose\nshawwn#3694: well, of the training data that would be fed to gpt2\nshawwn#3694: so no metadata like url or whatever\nbmk#1476: I think it was 40GB something (source: the webpage) but I didn't track the stats for that\nbmk#1476: >This left 38GB of text data (40GB using"
    },
    {
      "id": "454",
      "message": "bmk#1476: but Â¯\\_(ãƒ„)_/Â¯\nshawwn#3694: @bmk would you be willing to re-run it using this code, dumping it to a file, and uploading that file somewhere? https://gist.github.com/shawwn/bf4ff442d2eff3f3cac4afdb428bcbc8\n\nYou can use it like:\n\n```py\nwith open('openwebtext.tok16', 'wb') as f:\n\n# inside the loop:\ntokens = tok.encode(doc)\ntokens_to_file(f, tokens, stride=2)\n\n```\nbmk#1476: sure\nshawwn#3694: sweet, thanks\nshawwn#3694: I have a sampler set up to train on datasets in that format\nbmk#1476: als"
    },
    {
      "id": "455",
      "message": "shawwn#3694: so literally anyone who ever tries to encode <|endoftext|> always ends up using the wrong tokens\nbmk#1476: 50256 is the correct one, the literal encoding of <|endoftext|> is incorrect, though\nshawwn#3694: would it be possible to append the text '<|endoftext|>' and encode that?\nshawwn#3694: I know. And it's caused endless horrible confusion\nbmk#1476: because that would be the encoding of the actual literal string occurring\nshawwn#3694: yes\nbmk#1476: so i strongly believe we should be"
    },
    {
      "id": "456",
      "message": "bmk#1476: ```\nwt = lmd.Reader('/data/datasets/openwebtext')\n\nwith open('openwebtext.tok16', 'wb') as f:\ntok = transformers.GPT2TokenizerFast.from_pretrained('gpt2')\nfor doc in tqdm(wt.stream_data()):\n# inside the loop:\ntokens = tok.encode(doc) + [50256]\ntokens_to_file(f, tokens, stride=2)\n``` i have this running now\nshawwn#3694: Sweet! Is it spitting out data?\nbmk#1476: yup\nbmk#1476: should be ready in another 5 hours, at which time i will not be awake\nshawwn#3694: word.\nshawwn#3694: before you "
    },
    {
      "id": "457",
      "message": "shawwn#3694: you're getting around 1,859,558 tokens/sec according to my calcs\nshawwn#3694: Oh wait, that's scp speed\nshawwn#3694: I forgot it's not streaming directly to the server.\nshawwn#3694: still, generating 342M already is impressive\nshawwn#3694: thanks again.\nbmk#1476: np\nshawwn#3694: hmmm. openwebtext wasn't run through ftfy first\nshawwn#3694: I'll fix it on my end\nshawwn#3694: (e.g. there are characters like â€™ instead of ')\nshawwn#3694: hopefully the openwebtext tfrecords were ftfy'd, e"
    },
    {
      "id": "458",
      "message": "shawwn#3694: word.\nbmk#1476: i cant wait for a model with a full-unicode vocab\nshawwn#3694: when gwern and I were training poetry, we were confused why GPT kept generating mojibake\nshawwn#3694: once we ran our dataset through ftfy, all that went away\nshawwn#3694: well, openai's vocab works fine on full unicode\nshawwn#3694: it's just biased towards english\nshawwn#3694: so it's much harder for the model to learn russian, presumably. Or at least that's the theory.\nbmk#1476: chinese would just not w"
    },
    {
      "id": "459",
      "message": "kindiana#1016: its all just unicode bytes in the end ðŸ¤·\nbmk#1476: so wait they break it up by bytes?\nkindiana#1016: bpe works on bytes, it doesn't care for codepoints or anything\nkindiana#1016: if you use a chinese dataset I would expect common characters to be assigned their own tokens\nbmk#1476: but then it would be different between say utf8 and utf16\nshawwn#3694:  https://cdn.discordapp.com/attachments/729741769738158194/741866359188750417/unknown.png\nshawwn#3694: works fine.\nbmk#1476: huh\nsha"
    },
    {
      "id": "460",
      "message": "shawwn#3694: yes, you can.\nshawwn#3694: (I've done it)\nbmk#1476: also then it wont be under 65536\nshawwn#3694: it will if you don't extend beyond 65536\nbmk#1476: i mean like\nbmk#1476: thats what i mean when i say reserve\nshawwn#3694: I think reserving a substantial amount for fine-tuning purposes will prevent backwards compatibility issues\nbmk#1476: hmm\nshawwn#3694: also, users might be able to bootstrap some interesting ideas into the vocab\nbmk#1476: so can we tack on  a few thousand multilingu"
    },
    {
      "id": "461",
      "message": "bmk#1476: 50k of some pretty rare English words, and then all the other languages have to squeeze into the last 15k\nbmk#1476: Still I guess this is better than nothing, where all other languages have to be constructed from bytes\nbmk#1476: how big of a concern, really, is efficiency on disk, anyways\nkindiana#1016: depends on who pays for storage on the bucket lol\nbmk#1476: because if we just extend it by a few more bits we get a lot of legroom\nbmk#1476: is it too inconvenient to store in multiple"
    },
    {
      "id": "462",
      "message": "bmk#1476: id hazard a guess and say more than 3\nbmk#1476: having 200000 slots to split between a lot of languages means each isnt really getting all that much\nbmk#1476: anyways\nbmk#1476: 65536-multilingual is a really cool project that we need to undertake\nbmk#1476: and we should train all our models with it\nshawwn#3694: Nah, do either 16bit or 32bit imo. 24bit requires a custom decoder always\nbmk#1476: ok so we should have two vocabs\nbmk#1476: 65536 and 262144 (say)\nbmk#1476: the 65536 vocab wo"
    },
    {
      "id": "463",
      "message": "bmk#1476: the webpage\nbmk#1476: there are so many lines\nshawwn#3694: Oh you havenâ€™t seen it? Top is the real run\nshawwn#3694: Everything else is simulated\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/741876287194398740/unknown.png\nbmk#1476: this?\nshawwn#3694: The thing that moves every 3sec is real\nshawwn#3694: Yep\nshawwn#3694: Thatâ€™s one line per TPU core\nshawwn#3694: Every loss of every step\nbmk#1476: hm not much seems to be happening'\nbmk#1476: its just horizontal\nshaw"
    },
    {
      "id": "464",
      "message": "kindiana#1016: wow thats really good with 2 million params ðŸ¤”\nAI_WAIFU#2844: Nah, this is nats/character\nkindiana#1016: nats?\nAI_WAIFU#2844: base e instead of 2\nkindiana#1016: ah\nAI_WAIFU#2844: It's more like 1.63 bpc\nAI_WAIFU#2844: The point isn't that the model is good. I haven't made much effort to make a good model. The point is that the number of parameters in my model scales linearly with the amount of compute I throw at the problem, and I don't have to worry about running out of GPU ram.\nb"
    },
    {
      "id": "465",
      "message": "AI_WAIFU#2844: Then I add the pertubations to the existing output and repeat.\nshawwn#3694: hah, amazing... I was watching the TPU training session in real time, and saw that it froze. Can't tell from screenshot, but top part stopped updating. Knew instantly there was a problem. And sure enough https://cdn.discordapp.com/attachments/729741769738158194/741885410032091257/unknown.png\nshawwn#3694:  https://cdn.discordapp.com/attachments/729741769738158194/741885419188256808/unknown.png\nshawwn#3694: "
    },
    {
      "id": "466",
      "message": "AI_WAIFU#2844: You could have skip connections to different points in time.\nbmk#1476: what if then you add a connection every 2 blocks going back 2 blocks\nbmk#1476: then connections every 4 blocks going back 4 blocks\nbmk#1476: etc\nbmk#1476: log n in memory at once and you get long range connectivity\nAI_WAIFU#2844: This just opened up a whole new class of models.\nbmk#1476: what do you mean different points in time,\nkindiana#1016: there is densenet\nbmk#1476: densenete is weird though\nAI_WAIFU#2844"
    },
    {
      "id": "467",
      "message": "AI_WAIFU#2844: Like just the architecture or also the progressive growing bit.\nAI_WAIFU#2844: ?\nbmk#1476: oh theres no progressive growing here\nbmk#1476: just taking a resnet and adding more connections\nkindiana#1016: what do you do when the connections are different dims/scales?\nkindiana#1016: concat/downscale?\nbmk#1476: well, you dont, but im thinking more language model than image model\nkindiana#1016: ah\nbmk#1476: also yeah i guess you can downscale the connections too\nbmk#1476: *adds to list"
    },
    {
      "id": "468",
      "message": "Deleted User#0000: also, hi @Aran Komatsuzaki is it morning for you?\nAran Komatsuzaki#5714: it's actually 2:30 afternoon here.\nDeleted User#0000: oh! well, good afternoon\nAran Komatsuzaki#5714: yeah how's going?\nDeleted User#0000: pretty good! you know, the R value of the virus is about 1.03\nDeleted User#0000: hoping it goes down lol\nAran Komatsuzaki#5714: i hope so ðŸ™‚\nAran Komatsuzaki#5714: Was my overleaf draft hard to read?\nAran Komatsuzaki#5714: Any problem with clarify etc?\nAran Komatsuzaki#"
    },
    {
      "id": "469",
      "message": "eugene#9671: Joined the server.\nplatypii#0938: Joined the server.\nSid#2121: Hey @eugene , @platypii ! Welcome to the all-attention zone! Check the channel description for an overview of the project and let us know if you have any questions ðŸ™‚\nAI_WAIFU#2844: Re: Masking the first tokens when computing validation loss and the importance of context. I plotted the relationship between average loss and the length of the context window. Two observations: https://cdn.discordapp.com/attachments/729741769"
    },
    {
      "id": "470",
      "message": "bmk#1476: tldr it needs to get grammar good before it can be logically good\nAI_WAIFU#2844: Small models will pick the low hanging fruit.\nAI_WAIFU#2844: So we would expect the curve to get steeper as the model gets bigger because it will better take advantage of longer contexts, which are higher up in the \"tree\" of the tortured metaphor\nAI_WAIFU#2844: All my compute is tied up rn, I had to make that plot with my CPU, but if someone wants to calculate this same plot for a pretrained GPT-2, be my g"
    },
    {
      "id": "471",
      "message": "AI_WAIFU#2844: anything with large coherent documents should work.\nbmk#1476: fun protip: i have a lib that lets you easily get wt\nAI_WAIFU#2844: Small documents won't demonstrate the effect\nbmk#1476: ah wt docs are usually pretty short\nAI_WAIFU#2844: Do crime and punishment\nAI_WAIFU#2844: or moby dick\nAI_WAIFU#2844: maybe not moby dick\nshawwn#3694: Oh, bmk, want to scp the openwebtext tokenization? (Thanks for that by the way.)\nshawwn#3694: I ftfyâ€™d the first 6GB\nbmk#1476: yeah one sec\nshawwn#36"
    },
    {
      "id": "472",
      "message": "AI_WAIFU#2844: My run lasted 6 hours\nAI_WAIFU#2844: I did it over night\nbmk#1476: o.O\nbmk#1476: oh right cpu\nAI_WAIFU#2844: GPU is tied up doing a larger run of my gradient boosting thing.\nbmk#1476: ah\nbmk#1476: still waiting on nvidia to release their new gaming cards so i can splurge on a couple of new gpus >.>\nAI_WAIFU#2844: Same\nAI_WAIFU#2844: @bmk if you haven't altready just make sure the script runs with like n=10 before you do the whole thing\nbmk#1476: ok\nAI_WAIFU#2844: I don't want you "
    },
    {
      "id": "473",
      "message": "AI_WAIFU#2844: probably the output of GPT-1 vs gpt-2\nAI_WAIFU#2844: change the -= to +=\nbmk#1476: o.O\nbmk#1476: how are they different?\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/742427053638287391/unknown.png\nbmk#1476: ok this *really* does not look right\nbmk#1476: why is loss way down in the -100\nAI_WAIFU#2844: Agreed\nAI_WAIFU#2844: are you grabbing the right output?\nAI_WAIFU#2844: you need the logits\nAI_WAIFU#2844: preferably properly scaled\nbmk#1476: as far as i can"
    },
    {
      "id": "474",
      "message": "bmk#1476: spot the difference\nbmk#1476: no but the gpt one isnt either\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/742428860204974162/unknown.png\nbmk#1476: this reproduces your graph\nbmk#1476: but if i switch the comments it no longer works\nAI_WAIFU#2844: hmm\nAI_WAIFU#2844: Gimme a sec I'll try and get it working on my end\nAI_WAIFU#2844: @bmk I figured it out, it's a logit normalization issue. GPT-1 normalizes the logits so their exponents add up to 1, gpt-2 doesn't.\nbmk"
    },
    {
      "id": "475",
      "message": "AI_WAIFU#2844: ðŸ¤·\nAI_WAIFU#2844: I think its just that in the GPT-1 arch the inputs don't affect the corresponding outputs while for GPT-2 they do, so you have to shift by one to use it as an autoregressive LM\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/742567868876456016/gpt2-117M-losspos.png\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/742567914581786654/loss-gpt2-small.npy\nAI_WAIFU#2844: Beautiful.\nbmk#1476: if you want you can combine the lines"
    },
    {
      "id": "476",
      "message": "Teqnicolor#8109: @StellaAthena It is\nLouis#0144: Oh it is I think\nLouis#0144: @Daj said the name is a Greek pun\nLouis#0144: But I donâ€™t remember what specifically\nbmk#1476: yup that's right\nbmk#1476: so yeah any input for the copyleft idea would be nice\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/742576728827756614/unknown.png\nStellaAthena#3530: @bmk How much experience do you guys have with verifiable computation?\nLouis#0144: None AFAIK\nLouis#0144: I think Daj knows a b"
    },
    {
      "id": "477",
      "message": "StellaAthena#3530: How about you catch me up on what you guys have already been thinking so I donâ€™t 1) come totally out of left field to tell you how to do things and 2) donâ€™t rehash things that have already been discussed\nbmk#1476: well uh\nbmk#1476: so we're pretty sure we want to release the model\nbmk#1476: i.e not gate it behind an api or whatever\nbmk#1476: but we also want to attach some kind of thing that makes it so people dont use it to Do Evilâ„¢\nbmk#1476: we haven't agreed on what Do Evil"
    },
    {
      "id": "478",
      "message": "bmk#1476: interesting\nbmk#1476: so youd have to be able to tell if the output was generated by a model with a contract\nStellaAthena#3530: It makes you able to verify the claim â€œI did X with the model, and I did it in a good wayâ€\nStellaAthena#3530: Separately, @Louis mentioned that you were interested in detecting when people finetune your model? For language models, Iâ€™m pretty confidant that Iâ€™ve solved that problem up to implementation weirdnesses. I havenâ€™t implemented and tested it yet, but o"
    },
    {
      "id": "479",
      "message": "Louis#0144: The model stops working entirely\nbmk#1476: o.O\nbmk#1476: but.. how??\nLouis#0144: Idk\nLouis#0144: Tbh\nLouis#0144: Lmao\nbmk#1476: wat\nLouis#0144: I havenâ€™t gone in depth with this stuff\nLouis#0144: Tbf\nbmk#1476: so you can take a model, add some kind of imperceptible bias that doesnt hurt performance too much, and that is difficult to reverse?\nbmk#1476: very very interesting\nbmk#1476: also i'm approximately 110% certain that the moment this is released, some group of Hackersâ„¢ will take"
    },
    {
      "id": "480",
      "message": "StellaAthena#3530: Thirdly, to defeat the â€œhigh effortâ€ hacker, given a word embedding w and a set of word embeddings S you can estimate how much data is needed to finetune w until it is in S. Working backwards, this may allow us to determine that a given radioactive word embedding is sufficiently far from a â€œnaturally trained GPT-3â€ word embedding because they data required to get it to look like a â€œnaturally trained GPT-3â€ is on the order of the data required to train GPT-3 from scratch.\nStell"
    },
    {
      "id": "481",
      "message": "Daj#7482: Yup absolutely, I see this as one half \"interesting experiment maybe\" and one half \"why not?\", not as an actual strategy to effectively prevent abuse\nDaj#7482: Though I'm definitely down for experimenting with those tracer methods for enforcement\nStellaAthena#3530: I view these limitations as a motivation for the tracer method. Or, more generally, proof-based verification methods. We canâ€™t catch people who hide from us, but we can catch people who lie to us and we can tell who is refus"
    },
    {
      "id": "482",
      "message": "StellaAthena#3530: I agree ðŸ™‚\nStellaAthena#3530: > Anyways, I would love to cooperate on trying this out, though you're obviously the expert and would have to tell us what we can do to help\n@Daj  Frankly, this is my ideal set up. Iâ€™m a mathematician, and I donâ€™t let people call me a computer scientist unless they put the word â€œtheoreticalâ€ in front of it. My job is more or less to be a â€œbackseat coderâ€ to real computer scientists rotfl.\nDaj#7482: Hey sounds great to me\nDaj#7482: I'm the platonic "
    },
    {
      "id": "483",
      "message": "StellaAthena#3530: > Is this a project you would actually have the time/interest to commit to soonish?\n@Daj Yeah. I was specifically waiting for DEF CON to be over to join this chat for this exact reason ðŸ™‚\nDaj#7482: and good anything really\nDaj#7482: Awesome Stella! Man here I am wanting to take a week off and too many interesting things happen\nSid#2121: this all sounds super fascinating btw. Are there any actual implementations of the paper you posted above in code?\nStellaAthena#3530: Yeah itâ€™s"
    },
    {
      "id": "484",
      "message": "Daj#7482: but if you want to verify on our precise code we'd probably have to use TPUs\nAran Komatsuzaki#5714: Thanks. But it doesn't matter to me whether a topic needs math or not. What matters to me is whether it leads to AGI or not, which is why I'm doing this.\nAran Komatsuzaki#5714: Just wanted to join the conversation lol\nStellaAthena#3530: Hi ðŸ™‚\nDaj#7482: I'm with you Aran haha\nStellaAthena#3530: If we have TPU resources, great letâ€™s use those.\nDaj#7482: We have a _lot_ of TPUs haha\nStellaAt"
    },
    {
      "id": "485",
      "message": "Aran Komatsuzaki#5714: Thanks ðŸ™‚\nDaj#7482: Mesh Tensorflow, Stella. I can invite you to the repo if you send me your github name\nSid#2121: > GPT3 is the same architecture just more/bigger layers\n@Daj Not quite accurate, OA use sparse layers for their GPT3, we'll probably have to use some other technique eventually to cut down training time (likely local / linear attention)\nDaj#7482: Unfortunately pytorch support for TPUs is terrible\nDaj#7482: > @Daj Not quite accurate, OA use sparse layers for th"
    },
    {
      "id": "486",
      "message": "Daj#7482: Ah that's what I thought, just wanted to make sure\nSid#2121: I'm having trouble seeing how this would transfer over to text, unless you had a large amount of the target's text outputs to analyze?\nDaj#7482: Sounds really great to me, I'm super excited\nSid#2121: surely you couldn't detect the radioactivity from say, a single spam message\nDaj#7482: Yea I think this would require a sizeable amount of text\nDaj#7482: Detecting spammy blog networks or accounts, not messages\nDaj#7482: Not perf"
    },
    {
      "id": "487",
      "message": "Sid#2121: I'm trying to figure out how you would then detect from raw text\nSid#2121: or like, a certain misspelling would appear more often?\nStellaAthena#3530: Have you read any of the literature on implicit bias in text models?\nSid#2121: nope\nStellaAthena#3530: So letâ€™s think about word2vec for a second for conceptual simplicity\nStellaAthena#3530: If you train word2vec on the NYT and on reddit youâ€™ll see mostly the same thing, but some significant differences.\nDaj#7482: We just detect the densi"
    },
    {
      "id": "488",
      "message": "Daj#7482: Intuitively, I feel this should work in a high enough dimensional space\nDaj#7482: And would be close to imperceptible\nbmk#1476: @AI_WAIFU results for 345M https://cdn.discordapp.com/attachments/729741769738158194/742765237681520780/gpt2-345M-losspos.png\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/742765265149886525/loss-gpt2-medium.npy\nDaj#7482: So the big empirical question is whether text is high enough dimension\nStellaAthena#3530: This is why the radioactive"
    },
    {
      "id": "489",
      "message": "bmk#1476: Honestly it would be great if we could publish a paper under the banner of EleutherAI\nbmk#1476: Would lend us a lot more credence as a Real AI Labâ„¢\nDaj#7482: We have like half a dozen proto papers floating about\nAI_WAIFU#2844: I'm game\nDaj#7482: Would be extremely awesome to actually get one published\nbmk#1476: Ok what other experiments do we want\nAI_WAIFU#2844: I want to see this same thing done on a corpus of novels.\nbmk#1476: We could train a bunch of smaller and smaller models to g"
    },
    {
      "id": "490",
      "message": "AI_WAIFU#2844: I subtracted the small model loss from the medium model loss https://cdn.discordapp.com/attachments/729741769738158194/742778203399258242/Figure_4.png\nDeleted User#0000: it's like 20k books\nDeleted User#0000: `import nlp; nlp.load_dataset('pg19')`\nAI_WAIFU#2844: Gutenberg provides a python API to their corpus\nbmk#1476: what does this mean\nbmk#1476: also where can i download it as a zip\nDeleted User#0000: just use the hf nlp library bmk\nbmk#1476: how install\nDeleted User#0000: `pip"
    },
    {
      "id": "491",
      "message": "AI_WAIFU#2844: @Daj what's the main obstacle to turning these \"proto-papers\" into real papers?\nbmk#1476: nothing\nbmk#1476: there's no obstacle\nbmk#1476: as long as someone wants to put in the time they can turn them into papers\nAran Komatsuzaki#5714: maybe you can make a list of them?\nbmk#1476: there's a semi-list on the doc that i will expand\nbmk#1476: so the obvious first candidates are our main projects https://cdn.discordapp.com/attachments/729741769738158194/742781144428445776/unknown.png\nb"
    },
    {
      "id": "492",
      "message": "bmk#1476: lmk if i missed anything\nSid#2121: we also now have a *tonne* of extra features courtesy of @Deleted User that we can test\nSid#2121: so 1) local attention\nbmk#1476: oh right other model architecture stuff\nSid#2121: 2) all attention memory key-values\nSid#2121: 3) axial positional embedding\nSid#2121: uhhh 4) moe!\nSid#2121: 5) GLUs\nSid#2121: (he's been busy lol)\nbmk#1476:  https://cdn.discordapp.com/attachments/729741769738158194/742782938923401336/unknown.png\nAran Komatsuzaki#5714: as an"
    },
    {
      "id": "493",
      "message": "bmk#1476: unless we do the crazy 1Q L2L trick\nbmk#1476: but like\nDaj#7482: GShard is underwhelming given its size\nbmk#1476: *i'm not implementing that*\nDaj#7482: MOE is cool when memory is cheap\nDaj#7482: and you have conditional computing and co\nSid#2121: i'm equally hopeful for local / global attention mix and linear attention.\nAran Komatsuzaki#5714: the thing is that memory is cheap.\nbmk#1476: not for us\nDaj#7482: Yea compute is our cheap resource\nbmk#1476: memory is our main bottleneck\nbmk#1"
    },
    {
      "id": "494",
      "message": "bmk#1476: im pretty sure theres enough bandwidth\nDaj#7482: we'll wait for Noam to solve it\nbmk#1476: latency can be fixed by caching in tpu cpus\nbmk#1476: ok woah gutenberg is a lot bigger than i remembered\nbmk#1476: ok so\nbmk#1476: whats the best way for us to tune various gpt2s on gutenberg\nbmk#1476: @Daj whats the canonical tpu tuning script\nDaj#7482: There is none other than mine\nDaj#7482: That I know of\nbmk#1476: ok so\nbmk#1476: we want to encode gutenberg\nbmk#1476: i dont know how to do th"
    },
    {
      "id": "495",
      "message": "Daj#7482: That's it\nAran Komatsuzaki#5714:  https://cdn.discordapp.com/attachments/729741769738158194/742788142599372823/img2.png\nAran Komatsuzaki#5714: Seems like each device had only ~4GB.\nDeleted User#0000: @Aran Komatsuzaki yeah, ideally i would give them local + linear + RT\nDeleted User#0000: but i don't think RT would behave well distributedly\nDaj#7482: for reference: GPT3 has around ~700GB of weights\nDeleted User#0000: at least, i'm not sure how the clusters would be managed\nbmk#1476: Mor"
    }
  ]
}